{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Overview\n",
        "This notebook implements an end-to-end research paper processing pipeline.\n",
        "\n",
        "- **Milestone 1**: Uses the Semantic Scholar API to search, retrieve metadata, and download research paper PDFs.\n",
        "- **Milestone 2**: Extracts text from PDFs, performs section-wise parsing, key-finding extraction, cross-paper comparison, and validates extracted content.\n",
        "\n",
        "The system converts unstructured scholarly PDFs into structured and analyzable textual data.\n"
      ],
      "metadata": {
        "id": "TJn4s6ETRLCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install semanticscholar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV9ZmSFtRh7B",
        "outputId": "c28cbbe7-4923-4e45-f65d-a3e3aecbeb6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting semanticscholar\n",
            "  Downloading semanticscholar-0.11.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.12/dist-packages (from semanticscholar) (9.1.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from semanticscholar) (0.28.1)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from semanticscholar) (1.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->semanticscholar) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->semanticscholar) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->semanticscholar) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->semanticscholar) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->semanticscholar) (0.16.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->semanticscholar) (4.15.0)\n",
            "Downloading semanticscholar-0.11.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: semanticscholar\n",
            "Successfully installed semanticscholar-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o1p2zDzpKi7n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "from semanticscholar import SemanticScholar\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = Path(\"data\")\n",
        "PDF_DIR = BASE_DIR / \"pdfs\"\n",
        "RAW_TEXT_DIR = BASE_DIR / \"extracted_text\"\n",
        "STRUCTURED_DIR = BASE_DIR / \"structured_text\"\n",
        "LOG_DIR = BASE_DIR / \"logs\"\n",
        "\n",
        "for d in [PDF_DIR, RAW_TEXT_DIR, STRUCTURED_DIR, LOG_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=LOG_DIR / \"pipeline.log\",\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "1MH3aokMRl_9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_semantic_scholar_client(api_key=\"wFKolR3bfa5XUZaFntmdo5AXd7kL506y1klYRd3y\"):\n",
        "    if api_key:\n",
        "        return SemanticScholar(api_key=\"wFKolR3bfa5XUZaFntmdo5AXd7kL506y1klYRd3y\")\n",
        "    return SemanticScholar()\n"
      ],
      "metadata": {
        "id": "fU-OAstIRs4O"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sch = get_semantic_scholar_client()  # add api_key=\"YOUR_KEY\" if needed\n"
      ],
      "metadata": {
        "id": "CT1yVRV5RzvE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_papers(query, limit=10):\n",
        "    results = sch.search_paper(\n",
        "        query=query,\n",
        "        limit=limit,\n",
        "        fields=[\"title\", \"authors\", \"year\", \"citationCount\", \"openAccessPdf\"]\n",
        "    )\n",
        "    return results\n",
        "QUERY = \"mental health deep learning\"\n",
        "papers = search_papers(QUERY, limit=10)\n"
      ],
      "metadata": {
        "id": "0-Rrk9YYR2wj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_pdf(url, save_path):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=30)\n",
        "        if r.status_code == 200:\n",
        "            with open(save_path, \"wb\") as f:\n",
        "                f.write(r.content)\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Download failed: {e}\")\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "sBehNnzOSCeN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF → Raw Text Extraction"
      ],
      "metadata": {
        "id": "8uqtYiN0UfDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Text extraction failed for {pdf_path.name}: {e}\")\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "aT7NmS2RUIoz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper_metadata = []\n",
        "\n",
        "for i, paper in enumerate(tqdm(papers, desc=\"Downloading PDFs\")):\n",
        "    if not paper.openAccessPdf:\n",
        "        continue\n",
        "\n",
        "    pdf_url = paper.openAccessPdf.get(\"url\")\n",
        "    if not pdf_url:\n",
        "        continue\n",
        "\n",
        "    paper_id = f\"paper_{i+1}\"\n",
        "    pdf_path = PDF_DIR / f\"{paper_id}.pdf\"\n",
        "\n",
        "    if download_pdf(pdf_url, pdf_path):\n",
        "        paper_metadata.append({\n",
        "            \"paper_id\": paper_id,\n",
        "            \"title\": paper.title,\n",
        "            \"year\": paper.year,\n",
        "            \"citations\": paper.citationCount\n",
        "        })\n"
      ],
      "metadata": {
        "id": "gxIYnQStWt2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_texts = {}\n",
        "\n",
        "for pdf_file in tqdm(list(PDF_DIR.glob(\"*.pdf\")), desc=\"Extracting text\"):\n",
        "    text = extract_text_from_pdf(pdf_file)\n",
        "    paper_id = pdf_file.stem\n",
        "    raw_texts[paper_id] = text\n",
        "\n",
        "    with open(RAW_TEXT_DIR / f\"{paper_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUAoDAygUjM1",
        "outputId": "778d0539-4004-42fa-a3d5-4ba3040155a7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting text: 100%|██████████| 43/43 [00:04<00:00,  9.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SECTION_PATTERNS = {\n",
        "    \"abstract\": r\"\\babstract\\b\",\n",
        "    \"introduction\": r\"\\bintroduction\\b\",\n",
        "    \"methodology\": r\"\\b(methodology|methods)\\b\",\n",
        "    \"results\": r\"\\b(results|experiments)\\b\",\n",
        "    \"conclusion\": r\"\\b(conclusion|conclusions)\\b\",\n",
        "    \"references\": r\"\\breferences\\b\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "af5UmLevUojY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sections(text):\n",
        "    sections = {}\n",
        "    text_lower = text.lower()\n",
        "    matches = []\n",
        "\n",
        "    for name, pattern in SECTION_PATTERNS.items():\n",
        "        match = re.search(pattern, text_lower)\n",
        "        if match:\n",
        "            matches.append((match.start(), name))\n",
        "\n",
        "    matches.sort()\n",
        "\n",
        "    for i, (start, name) in enumerate(matches):\n",
        "        end = matches[i + 1][0] if i + 1 < len(matches) else len(text)\n",
        "        sections[name] = text[start:end].strip()\n",
        "\n",
        "    return sections\n"
      ],
      "metadata": {
        "id": "GXDkksUEUy1z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_docs = {}\n",
        "\n",
        "for pid, text in tqdm(raw_texts.items(), desc=\"Section parsing\"):\n",
        "    sections = extract_sections(text)\n",
        "    structured_docs[pid] = sections\n",
        "\n",
        "    with open(STRUCTURED_DIR / f\"{pid}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(sections, f, indent=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSosUAA9U3SE",
        "outputId": "f4edd234-fef0-4cd2-8265-c2643fc5fd54"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Section parsing: 100%|██████████| 43/43 [00:00<00:00, 79.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key-Finding Extraction (TF-IDF)"
      ],
      "metadata": {
        "id": "155lt7EoVBjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "doc_ids = []\n",
        "\n",
        "for pid, sections in structured_docs.items():\n",
        "    content = sections.get(\"abstract\", \"\") + sections.get(\"conclusion\", \"\")\n",
        "    if content.strip():\n",
        "        docs.append(content)\n",
        "        doc_ids.append(pid)\n"
      ],
      "metadata": {
        "id": "Cio9KTjgU5ly"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=10)\n",
        "tfidf = vectorizer.fit_transform(docs)\n",
        "terms = vectorizer.get_feature_names_out()\n"
      ],
      "metadata": {
        "id": "sUILk9XcVHqk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paper_keywords = {}\n",
        "\n",
        "for i, pid in enumerate(doc_ids):\n",
        "    scores = tfidf[i].toarray()[0]\n",
        "    top_terms = [terms[j] for j in scores.argsort()[-5:][::-1]]\n",
        "    paper_keywords[pid] = top_terms\n",
        "\n",
        "paper_keywords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyQ9W9_PVJDq",
        "outputId": "60e72dab-9cf9-4f8d-a466-5464a0cee280"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'paper_107': ['data', 'mental', 'model', 'learning', 'models'],\n",
              " 'paper_4': ['mental', 'deep', 'learning', 'health', 'models'],\n",
              " 'paper_36': ['mental', 'learning', 'analysis', 'data', 'model'],\n",
              " 'paper_124': ['10', 'health', 'mental', 'models', 'learning'],\n",
              " 'paper_30': ['analysis', 'model', 'data', 'learning', 'deep'],\n",
              " 'paper_108': ['mental', 'model', 'data', '10', 'health'],\n",
              " 'paper_50': ['data', 'mental', 'health', '10', 'analysis'],\n",
              " 'paper_58': ['data', 'models', 'based', 'mental', 'health'],\n",
              " 'paper_82': ['mental', 'learning', 'health', 'based', 'data'],\n",
              " 'paper_113': ['model', 'based', 'learning', 'analysis', 'deep'],\n",
              " 'paper_20': ['health', 'mental', 'learning', 'deep', 'analysis'],\n",
              " 'paper_43': ['health', 'mental', 'analysis', 'based', 'model'],\n",
              " 'paper_3': ['models', 'model', 'data', 'health', 'mental'],\n",
              " 'paper_100': ['based', 'learning', 'deep', 'mental', 'health'],\n",
              " 'paper_25': ['data', 'models', 'model', 'mental', 'health'],\n",
              " 'paper_91': ['health', 'mental', '10', 'analysis', 'model'],\n",
              " 'paper_69': ['mental', 'health', 'learning', 'based', 'deep'],\n",
              " 'paper_127': ['mental', 'health', 'learning', 'data', 'models'],\n",
              " 'paper_63': ['mental', 'learning', 'analysis', 'model', 'data'],\n",
              " 'paper_42': ['learning', 'data', 'mental', 'health', 'deep'],\n",
              " 'paper_16': ['health', 'mental', 'learning', 'deep', 'model'],\n",
              " 'paper_102': ['model', 'models', 'data', 'based', 'health'],\n",
              " 'paper_60': ['health', 'mental', 'model', '10', 'data'],\n",
              " 'paper_41': ['learning', 'data', 'deep', 'analysis', 'based'],\n",
              " 'paper_110': ['health', 'mental', 'model', 'based', 'deep'],\n",
              " 'paper_1': ['model', 'models', 'mental', 'data', 'health'],\n",
              " 'paper_67': ['health', 'analysis', 'mental', 'data', 'deep'],\n",
              " 'paper_88': ['mental', 'health', 'data', 'model', 'based'],\n",
              " 'paper_12': ['mental', 'health', 'model', 'models', 'learning'],\n",
              " 'paper_15': ['model', 'data', 'deep', 'models', 'learning'],\n",
              " 'paper_52': ['mental', 'health', 'model', 'data', 'models'],\n",
              " 'paper_105': ['model', 'learning', 'models', 'analysis', 'deep'],\n",
              " 'paper_61': ['mental', 'learning', 'deep', 'health', 'based'],\n",
              " 'paper_23': ['learning', 'models', 'mental', 'deep', 'health'],\n",
              " 'paper_86': ['health', 'deep', 'learning', 'models', 'mental'],\n",
              " 'paper_80': ['model', 'mental', 'health', 'analysis', 'based'],\n",
              " 'paper_35': ['10', 'based', 'model', 'analysis', 'data'],\n",
              " 'paper_120': ['health', 'mental', 'learning', 'deep', 'data']}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Paper Comparison"
      ],
      "metadata": {
        "id": "NFBAqYqoVQeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_matrix = cosine_similarity(tfidf)\n",
        "\n",
        "similarities = []\n",
        "\n",
        "for i in range(len(doc_ids)):\n",
        "    for j in range(i + 1, len(doc_ids)):\n",
        "        similarities.append({\n",
        "            \"paper_1\": doc_ids[i],\n",
        "            \"paper_2\": doc_ids[j],\n",
        "            \"similarity\": round(similarity_matrix[i][j], 3)\n",
        "        })\n",
        "\n",
        "similarities[:5]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKZT_oVCVRPc",
        "outputId": "41861214-ff17-4972-e4bf-e5c2449bd0d0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'paper_1': 'paper_107',\n",
              "  'paper_2': 'paper_4',\n",
              "  'similarity': np.float64(0.605)},\n",
              " {'paper_1': 'paper_107',\n",
              "  'paper_2': 'paper_36',\n",
              "  'similarity': np.float64(0.832)},\n",
              " {'paper_1': 'paper_107',\n",
              "  'paper_2': 'paper_124',\n",
              "  'similarity': np.float64(0.582)},\n",
              " {'paper_1': 'paper_107',\n",
              "  'paper_2': 'paper_30',\n",
              "  'similarity': np.float64(0.833)},\n",
              " {'paper_1': 'paper_107',\n",
              "  'paper_2': 'paper_108',\n",
              "  'similarity': np.float64(0.871)}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation & Completeness Check"
      ],
      "metadata": {
        "id": "WLa4lR8bVXlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation = {\n",
        "    \"total_pdfs\": len(raw_texts),\n",
        "    \"successful_extraction\": 0,\n",
        "    \"section_coverage\": Counter()\n",
        "}\n",
        "\n",
        "for pid, text in raw_texts.items():\n",
        "    if text.strip():\n",
        "        validation[\"successful_extraction\"] += 1\n",
        "\n",
        "    for section in SECTION_PATTERNS:\n",
        "        if section in structured_docs.get(pid, {}):\n",
        "            validation[\"section_coverage\"][section] += 1\n"
      ],
      "metadata": {
        "id": "jiCslWArVUga"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"VALIDATION REPORT\")\n",
        "print(\"-\" * 40)\n",
        "print(\"Total PDFs:\", validation[\"total_pdfs\"])\n",
        "print(\"Successful Extractions:\", validation[\"successful_extraction\"])\n",
        "print(\"\\nSection Coverage:\")\n",
        "for k, v in validation[\"section_coverage\"].items():\n",
        "    print(f\"{k.capitalize()}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIgla560VdL0",
        "outputId": "ecc1e973-0baa-4e6e-ccca-323081835077"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION REPORT\n",
            "----------------------------------------\n",
            "Total PDFs: 43\n",
            "Successful Extractions: 40\n",
            "\n",
            "Section Coverage:\n",
            "Abstract: 30\n",
            "Introduction: 32\n",
            "Methodology: 37\n",
            "Results: 38\n",
            "Conclusion: 30\n",
            "References: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook successfully integrates Milestone into a unified pipeline.\n",
        "Research papers retrieved using the Semantic Scholar API are transformed from PDFs into\n",
        "structured, section-wise text. Key findings are extracted and compared across papers, and\n",
        "validation metrics ensure correctness and completeness.\n",
        "\n",
        "This approach enables scalable scholarly document analysis."
      ],
      "metadata": {
        "id": "5BN8yydfVlaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Milestone 1 & 2 pipeline executed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgefXaIwVsaT",
        "outputId": "93cdeff9-23c7-4bd9-d21d-a07580681a2d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Milestone 1 & 2 pipeline executed successfully.\n"
          ]
        }
      ]
    }
  ]
}