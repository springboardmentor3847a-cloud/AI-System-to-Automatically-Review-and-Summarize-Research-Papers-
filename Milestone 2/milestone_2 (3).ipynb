{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Pzurlu1p8lJq"
      },
      "outputs": [],
      "source": [
        "!pip install semanticscholar python-dotenv requests -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "from semanticscholar import SemanticScholar\n",
        "from dotenv import load_dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTYxNWyy8994",
        "outputId": "333e61af-3a0b-419f-b722-7f65eba0482b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/81.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Enter research topic: parkinson\n",
            "\n",
            "üîé Searching for: parkinson\n",
            "\n",
            "üîç Searching Europe PMC...\n",
            "‚û° Europe PMC PDF results: 2\n",
            "\n",
            "üîç Searching arXiv...\n",
            "‚û° arXiv PDF results: 20\n",
            "\n",
            "üìä TOTAL PDF papers found: 22\n",
            "\n",
            "=========== PDF AVAILABLE PAPERS ===========\n",
            "\n",
            "1. Stigmatization and bias in interpreting lichen sclerosus risk factors.\n",
            "   Authors: \n",
            "   Year: 2025  | Source: Europe PMC\n",
            "   PDF: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12685388/pdf/\n",
            "\n",
            "2. Nursing consultation protocol for supported self-care for people with Parkinson's disease.\n",
            "   Authors: \n",
            "   Year: 2025  | Source: Europe PMC\n",
            "   PDF: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12707775/pdf/\n",
            "\n",
            "3. Deep 1D-Convnet for accurate Parkinson disease detection and severity prediction from gait\n",
            "   Authors: Imanne El Maachi, Guillaume-Alexandre Bilodeau, Wassim Bouachir\n",
            "   Year: 2019  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/1910.11509v4.pdf\n",
            "\n",
            "4. A Three-groups Non-local Model for Combining Heterogeneous Data Sources to Identify Genes Associated with Parkinson's Disease\n",
            "   Authors: Troy P. Wixson, Benjamin A. Shaby, Daisy L. Philtron, International Parkinson Disease Genomics Consortium\n",
            "   Year: 2024  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/2406.05262v1.pdf\n",
            "\n",
            "5. Optimizing baryon acoustic oscillation surveys II: curvature, redshifts, and external datasets\n",
            "   Authors: David Parkinson, Martin Kunz, Andrew R. Liddle, Bruce A. Bassett\n",
            "   Year: 2009  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/0905.3410v2.pdf\n",
            "\n",
            "6. Detection of 16 Gamma-Ray Pulsars Through Blind Frequency Searches Using the Fermi LAT\n",
            "   Authors: The Fermi-LAT Collaboration\n",
            "   Year: 2010  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/1009.0748v1.pdf\n",
            "\n",
            "7. Identification of a DAGLB Mutation in a Non-Chinese Patient with Parkinson's Disease\n",
            "   Authors: Christelle Tesson, Mohamed Sofiane Bouchetara, M√©lanie Ferrien, Suzanne Lesage\n",
            "   Year: 2023  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/2310.12521v1.pdf\n",
            "\n",
            "8. A Multivariate Biomarker for Parkinson's Disease\n",
            "   Authors: Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum\n",
            "   Year: 2015  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/1602.07264v1.pdf\n",
            "\n",
            "9. A hierarchical coding-window model of Parkinson's disease\n",
            "   Authors: Daniela Sabrina Andres, Florian Gomez, Daniel Cerquetti, Marcelo Merello\n",
            "   Year: 2013  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/1307.6028v3.pdf\n",
            "\n",
            "10. Parkinson's Law Quantified: Three Investigations on Bureaucratic Inefficiency\n",
            "   Authors: Peter Klimek, Rudolf Hanel, Stefan Thurner\n",
            "   Year: 2008  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/0808.1684v1.pdf\n",
            "\n",
            "\n",
            "üíæ Saved results ‚Üí data/search_results/paper_search_results_parkinson.json\n",
            "\n",
            "üéâ MODULE 1 COMPLETE ‚Äî PDF-ONLY MODE ENABLED ‚úî\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# PDF-ONLY RESULTS (SemanticScholar-Compatible Output)\n",
        "\n",
        "!pip install requests feedparser -q\n",
        "\n",
        "import requests\n",
        "import feedparser\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "\n",
        "def clean(x):\n",
        "    return x.replace(\"\\n\", \" \").strip() if isinstance(x, str) else x\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 1. Europe PMC ‚Üí only keep papers with PDF\n",
        "# ====================================================\n",
        "\n",
        "def search_europe_pmc(query, limit=20):\n",
        "    print(\"\\nüîç Searching Europe PMC...\")\n",
        "\n",
        "    url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query={query}&format=json&pageSize={limit}\"\n",
        "    try:\n",
        "        data = requests.get(url, timeout=10).json()\n",
        "        results = data.get(\"resultList\", {}).get(\"result\", [])\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    for p in results:\n",
        "        pdf_url = None\n",
        "\n",
        "        # FIXED: Improved PDF detection\n",
        "        # Check open access status first\n",
        "        if p.get(\"isOpenAccess\") == \"Y\":\n",
        "            # Try PMC ID based URL\n",
        "            pmcid = p.get(\"pmcid\")\n",
        "            if pmcid:\n",
        "                pdf_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/\"\n",
        "\n",
        "            # If no PMC ID, check fullTextUrlList\n",
        "            if not pdf_url and \"fullTextUrlList\" in p:\n",
        "                urls = p[\"fullTextUrlList\"].get(\"fullTextUrl\", [])\n",
        "                for u in urls:\n",
        "                    doc_style = u.get(\"documentStyle\", \"\").lower()\n",
        "                    site = u.get(\"site\", \"\").lower()\n",
        "                    availability = u.get(\"availability\", \"\")\n",
        "\n",
        "                    # Look for PDF specifically\n",
        "                    if \"pdf\" in doc_style or \"pdf\" in site:\n",
        "                        pdf_url = u.get(\"url\")\n",
        "                        break\n",
        "                    # Or free full text\n",
        "                    elif availability == \"Free\" and u.get(\"url\"):\n",
        "                        potential_url = u.get(\"url\")\n",
        "                        if potential_url and \"pdf\" in potential_url.lower():\n",
        "                            pdf_url = potential_url\n",
        "                            break\n",
        "\n",
        "        # üö´ Skip papers without PDFs (FIXED)\n",
        "        if not pdf_url:\n",
        "            continue\n",
        "        papers.append({\n",
        "            \"title\": clean(p.get(\"title\", \"\")),\n",
        "            \"authors\": [a.get(\"fullName\", \"\") for a in p.get(\"authorList\", {}).get(\"author\", [])],\n",
        "            \"year\": int(p[\"pubYear\"]) if p.get(\"pubYear\") else None,\n",
        "            \"paperId\": p.get(\"id\", \"\"),\n",
        "            \"abstract\": clean(p.get(\"abstractText\", \"\")),\n",
        "            \"citationCount\": p.get(\"citedByCount\", 0),\n",
        "            \"venue\": p.get(\"journalTitle\", \"\"),\n",
        "            \"url\": p.get(\"pubmedUrl\", \"\"),\n",
        "            \"pdf_url\": pdf_url,\n",
        "            \"has_pdf\": True,\n",
        "            \"source\": \"Europe PMC\"\n",
        "        })\n",
        "\n",
        "    print(f\"‚û° Europe PMC PDF results: {len(papers)}\")\n",
        "    return papers\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 2. arXiv ‚Üí ALL papers have PDF\n",
        "# ====================================================\n",
        "def search_arxiv(query, limit=20):\n",
        "    print(\"\\nüîç Searching arXiv...\")\n",
        "\n",
        "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={limit}\"\n",
        "    try:\n",
        "        feed = feedparser.parse(url)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        pdf_url = entry.id.replace(\"abs\", \"pdf\") + \".pdf\"\n",
        "\n",
        "        papers.append({\n",
        "            \"title\": clean(entry.title),\n",
        "            \"authors\": [a.name for a in entry.authors],\n",
        "            \"year\": int(entry.published[:4]),\n",
        "            \"paperId\": entry.id,\n",
        "            \"abstract\": clean(entry.summary),\n",
        "            \"citationCount\": 0,\n",
        "            \"venue\": \"arXiv\",\n",
        "            \"url\": entry.link,\n",
        "            \"pdf_url\": pdf_url,\n",
        "            \"has_pdf\": True,\n",
        "            \"source\": \"arXiv\"\n",
        "        })\n",
        "\n",
        "    print(f\"‚û° arXiv PDF results: {len(papers)}\")\n",
        "    return papers\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 3. Combine + PDF Only\n",
        "# ====================================================\n",
        "def search_papers(query, limit=20):\n",
        "    print(f\"\\nüîé Searching for: {query}\")\n",
        "\n",
        "    pmc_papers = search_europe_pmc(query, limit)\n",
        "    arxiv_papers = search_arxiv(query, limit)\n",
        "\n",
        "    all_papers = pmc_papers + arxiv_papers\n",
        "\n",
        "    print(f\"\\nüìä TOTAL PDF papers found: {len(all_papers)}\")\n",
        "    return all_papers\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# SAVE RESULTS\n",
        "# ====================================================\n",
        "def save_search_results(papers, topic):\n",
        "    os.makedirs(\"data/search_results\", exist_ok=True)\n",
        "\n",
        "    safe_topic = \"\".join(c for c in topic if c.isalnum() or c == \" \").replace(\" \", \"_\")\n",
        "    filename = f\"paper_search_results_{safe_topic}.json\"\n",
        "\n",
        "    path = f\"data/search_results/{filename}\"\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"topic\": topic,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"papers\": papers\n",
        "        }, f, indent=4)\n",
        "\n",
        "    print(f\"\\nüíæ Saved results ‚Üí {path}\")\n",
        "    return path\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# DISPLAY RESULTS (PDF ONLY)\n",
        "# ====================================================\n",
        "def display_results(papers, limit=10):\n",
        "    print(\"\\n=========== PDF AVAILABLE PAPERS ===========\\n\")\n",
        "    for i, p in enumerate(papers[:limit], 1):\n",
        "        print(f\"{i}. {p['title']}\")\n",
        "        print(f\"   Authors: {', '.join(p['authors'][:4])}\")\n",
        "        print(f\"   Year: {p['year']}  | Source: {p['source']}\")\n",
        "        print(f\"   PDF: {p['pdf_url']}\\n\")\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# MAIN FUNCTION\n",
        "# ====================================================\n",
        "def main_search():\n",
        "    query = input(\"Enter research topic: \").strip()\n",
        "\n",
        "    papers = search_papers(query, limit=20)\n",
        "    display_results(papers)\n",
        "    save_search_results(papers, query)\n",
        "\n",
        "    print(\"\\nüéâ MODULE 1 COMPLETE ‚Äî PDF-ONLY MODE ENABLED ‚úî\")\n",
        "    return papers\n",
        "\n",
        "\n",
        "# Run module\n",
        "if __name__ == \"__main__\":\n",
        "    main_search()\n",
        "def main_search():\n",
        "    query = input(\"Enter research topic: \").strip()\n",
        "\n",
        "    papers = search_papers(query, limit=20)\n",
        "\n",
        "    # ADDED: Check for empty results\n",
        "    if not papers:\n",
        "        print(\"\\n‚ùå No papers with PDFs found. Try a different search query.\")\n",
        "        return []\n",
        "\n",
        "    display_results(papers)\n",
        "    save_search_results(papers, query)\n",
        "\n",
        "    print(\"\\nüéâ MODULE 1 COMPLETE ‚Äì PDF-ONLY MODE ENABLED ‚úì\")\n",
        "    return papers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MveIANO29FZc",
        "outputId": "b03d27de-8df1-496f-9c96-b8428866b643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m245.8/253.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "=== MODULE 2: PDF Downloader (Updated) ===\n",
            "\n",
            "üìÑ Loaded 22 papers (PDF-only)\n",
            "\n",
            "======================================================================\n",
            "AVAILABLE PAPERS FOR DOWNLOAD\n",
            "======================================================================\n",
            "\n",
            "[1] Stigmatization and bias in interpreting lichen sclerosus risk factors....\n",
            "    Authors: \n",
            "    Year: 2025 | Source: Europe PMC\n",
            "\n",
            "[2] Nursing consultation protocol for supported self-care for people with Parki...\n",
            "    Authors: \n",
            "    Year: 2025 | Source: Europe PMC\n",
            "\n",
            "[3] Deep 1D-Convnet for accurate Parkinson disease detection and severity predi...\n",
            "    Authors: Imanne El Maachi, Guillaume-Alexandre Bilodeau, Wassim Bouachir\n",
            "    Year: 2019 | Source: arXiv\n",
            "\n",
            "[4] A Three-groups Non-local Model for Combining Heterogeneous Data Sources to ...\n",
            "    Authors: Troy P. Wixson, Benjamin A. Shaby, Daisy L. Philtron\n",
            "    Year: 2024 | Source: arXiv\n",
            "\n",
            "[5] Optimizing baryon acoustic oscillation surveys II: curvature, redshifts, an...\n",
            "    Authors: David Parkinson, Martin Kunz, Andrew R. Liddle\n",
            "    Year: 2009 | Source: arXiv\n",
            "\n",
            "[6] Detection of 16 Gamma-Ray Pulsars Through Blind Frequency Searches Using th...\n",
            "    Authors: The Fermi-LAT Collaboration\n",
            "    Year: 2010 | Source: arXiv\n",
            "\n",
            "[7] Identification of a DAGLB Mutation in a Non-Chinese Patient with Parkinson'...\n",
            "    Authors: Christelle Tesson, Mohamed Sofiane Bouchetara, M√©lanie Ferrien\n",
            "    Year: 2023 | Source: arXiv\n",
            "\n",
            "[8] A Multivariate Biomarker for Parkinson's Disease...\n",
            "    Authors: Giancarlo Crocetti, Michael Coakley, Phil Dressner\n",
            "    Year: 2015 | Source: arXiv\n",
            "\n",
            "[9] A hierarchical coding-window model of Parkinson's disease...\n",
            "    Authors: Daniela Sabrina Andres, Florian Gomez, Daniel Cerquetti\n",
            "    Year: 2013 | Source: arXiv\n",
            "\n",
            "[10] Parkinson's Law Quantified: Three Investigations on Bureaucratic Inefficien...\n",
            "    Authors: Peter Klimek, Rudolf Hanel, Stefan Thurner\n",
            "    Year: 2008 | Source: arXiv\n",
            "\n",
            "[11] End-to-End Parkinson Disease Diagnosis using Brain MR-Images by 3D-CNN...\n",
            "    Authors: Soheil Esmaeilzadeh, Yao Yang, Ehsan Adeli\n",
            "    Year: 2018 | Source: arXiv\n",
            "\n",
            "[12] Parkinson disease is a TH17 dominant autoimmune disorder against accumulate...\n",
            "    Authors: Wan-Chung Hu\n",
            "    Year: 2013 | Source: arXiv\n",
            "\n",
            "[13] Detection and Forecasting of Parkinson Disease Progression from Speech Sign...\n",
            "    Authors: Majid Ali, Hina Shakir, Asia Samreen\n",
            "    Year: 2024 | Source: arXiv\n",
            "\n",
            "[14] Comparative statistics of Garman-Klass, Parkinson, Roger-Satchell and bridg...\n",
            "    Authors: Alexander Saichev, Svetlana Lapinova\n",
            "    Year: 2012 | Source: arXiv\n",
            "\n",
            "[15] Neuropsychological Effects of Rock Steady Boxing in Patients with Parkinson...\n",
            "    Authors: Lorella Bonaccorsi, Ugo Santosuosso, Massimo Gulisano\n",
            "    Year: 2024 | Source: arXiv\n",
            "\n",
            "[16] Joy Learning: Smartphone Application For Children With Parkinson Disease...\n",
            "    Authors: Mujahid Rafiq, Ibrar Hussain, Muhammad Arif\n",
            "    Year: 2023 | Source: arXiv\n",
            "\n",
            "[17] Early Detection of Parkinson's Disease using Motor Symptoms and Machine Lea...\n",
            "    Authors: Poojaa C, John Sahaya Rani Alex\n",
            "    Year: 2023 | Source: arXiv\n",
            "\n",
            "[18] Assessing Progress of Parkinson s Disease Using Acoustic Analysis of Phonat...\n",
            "    Authors: Jiri Mekyska, Zoltan Galaz, Zdenek Mzourek\n",
            "    Year: 2022 | Source: arXiv\n",
            "\n",
            "[19] 1D-Convolutional transformer for Parkinson disease diagnosis from gait...\n",
            "    Authors: Safwen Naimi, Wassim Bouachir, Guillaume-Alexandre Bilodeau\n",
            "    Year: 2023 | Source: arXiv\n",
            "\n",
            "[20] Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical softwa...\n",
            "    Authors: Regan Bolton, Mohammadreza Sheikhfathollahi, Simon Parkinson\n",
            "    Year: 2025 | Source: arXiv\n",
            "\n",
            "[21] Multi-Stage Retrieval for Operational Technology Cybersecurity Compliance U...\n",
            "    Authors: Regan Bolton, Mohammadreza Sheikhfathollahi, Simon Parkinson\n",
            "    Year: 2025 | Source: arXiv\n",
            "\n",
            "[22] Interpretable and Granular Video-Based Quantification of Motor Characterist...\n",
            "    Authors: Tahereh Zarrat Ehsan, Michael Tangermann, Yaƒümur G√º√ßl√ºt√ºrk\n",
            "    Year: 2025 | Source: arXiv\n",
            "\n",
            "======================================================================\n",
            "Select papers to download (max 3)\n",
            "Examples: '1,3,5' or '1-3' or 'all' or 'top' (for top papers)\n",
            "======================================================================\n",
            "\n",
            "Your selection: 1,2,3\n",
            "\n",
            "‚úì Selected 3 paper(s) for download:\n",
            "  1. Stigmatization and bias in interpreting lichen sclerosus risk factors....\n",
            "  2. Nursing consultation protocol for supported self-care for people with ...\n",
            "  3. Deep 1D-Convnet for accurate Parkinson disease detection and severity ...\n",
            "\n",
            "[1] üìÑ Downloading: Stigmatization and bias in interpreting lichen sclerosus risk factors.\n",
            "   üîÑ Attempting download from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12685...\n",
            "   üì° Response status: 403\n",
            "   ‚úó HTTP Error 403\n",
            "   ‚úì DOCX created\n",
            "   ‚úì Metadata saved\n",
            "   üìÅ Folder: downloads/paper_1_Stigmatizationandbiasininterpretinglichensclerosus\n",
            "\n",
            "[2] üìÑ Downloading: Nursing consultation protocol for supported self-care for people with \n",
            "   üîÑ Attempting download from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12707...\n",
            "   üì° Response status: 403\n",
            "   ‚úó HTTP Error 403\n",
            "   ‚úì DOCX created\n",
            "   ‚úì Metadata saved\n",
            "   üìÅ Folder: downloads/paper_2_Nursingconsultationprotocolforsupportedselfcarefor\n",
            "\n",
            "[3] üìÑ Downloading: Deep 1D-Convnet for accurate Parkinson disease detection and severity \n",
            "   üîÑ Attempting download from: http://arxiv.org/pdf/1910.11509v4.pdf...\n",
            "   üì° Response status: 200\n",
            "   üìã Content-Type: application/pdf\n",
            "   üíæ File saved (812189 bytes)\n",
            "   ‚úì PDF downloaded and verified successfully!\n",
            "   ‚úì DOCX created\n",
            "   ‚úì Metadata saved\n",
            "   üìÅ Folder: downloads/paper_3_Deep1DConvnetforaccurateParkinsondiseasedetectiona\n",
            "   üîΩ Triggering browser download...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_70a5b3f8-2232-4eb5-b123-d9767b7a5c3f\", \"Deep1DConvnetforaccurateParkinsondiseasedetectiona.pdf\", 812189)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Saved report ‚Üí data/reports/report_20251224_172526.json\n",
            "\n",
            "======================================================================\n",
            "DOWNLOAD SUMMARY\n",
            "======================================================================\n",
            "Total papers: 3\n",
            "‚úì Successful PDF downloads: 1\n",
            "‚úó Failed PDF downloads: 2\n",
            "‚úì DOCX files created: 3\n",
            "======================================================================\n",
            "\n",
            "üéâ Module 2 completed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# MODULE 2: PDF Download + DOC Generation\n",
        "# Compatible with Updated Module 1 (PDF-only)\n",
        "# ============================================\n",
        "!pip install requests python-docx PyMuPDF -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "from datetime import datetime\n",
        "from docx import Document\n",
        "from pathlib import Path\n",
        "from IPython.display import FileLink, display\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# LOAD SEARCH RESULTS\n",
        "# ====================================================\n",
        "\n",
        "def load_search_results(filepath=None):\n",
        "    base = \"data/search_results\"\n",
        "    if not filepath:\n",
        "        files_list = sorted(\n",
        "            [f for f in os.listdir(base) if f.endswith(\".json\")],\n",
        "            key=lambda x: os.path.getmtime(os.path.join(base, x)),\n",
        "            reverse=True\n",
        "        )\n",
        "        if not files_list:\n",
        "            print(\"‚ùå No search results found. Run Module 1 first.\")\n",
        "            return None\n",
        "        filepath = os.path.join(base, files_list[0])\n",
        "\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"üìÑ Loaded {len(data['papers'])} papers (PDF-only)\")\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# FILTERING & SELECTION\n",
        "# ====================================================\n",
        "def select_top_papers(papers, count=3):\n",
        "    \"\"\"Allow user to select papers interactively\"\"\"\n",
        "\n",
        "    # Display available papers\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AVAILABLE PAPERS FOR DOWNLOAD\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for i, p in enumerate(papers, 1):\n",
        "        print(f\"\\n[{i}] {p['title'][:75]}...\")\n",
        "        print(f\"    Authors: {', '.join(p['authors'][:3])}\")\n",
        "        print(f\"    Year: {p.get('year', 'N/A')} | Source: {p['source']}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"Select papers to download (max {count})\")\n",
        "    print(\"Examples: '1,3,5' or '1-3' or 'all' or 'top' (for top papers)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"\\nYour selection: \").strip().lower()\n",
        "\n",
        "        # Option 1: Download all (up to limit)\n",
        "        if choice == 'all':\n",
        "            selected = papers[:min(len(papers), count)]\n",
        "            break\n",
        "\n",
        "        # Option 2: Top papers (sorted by citations)\n",
        "        elif choice == 'top':\n",
        "            sorted_papers = sorted(\n",
        "                papers,\n",
        "                key=lambda x: (x.get(\"year\") or 0),\n",
        "                reverse=True\n",
        "            )\n",
        "            selected = sorted_papers[:count]\n",
        "            break\n",
        "\n",
        "        # Option 3: Range (e.g., 1-3)\n",
        "        elif '-' in choice:\n",
        "            try:\n",
        "                start, end = map(int, choice.split('-'))\n",
        "                if 1 <= start <= end <= len(papers):\n",
        "                    selected = papers[start-1:min(end, start-1+count)]\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"‚ùå Invalid range. Use numbers between 1-{len(papers)}\")\n",
        "            except:\n",
        "                print(\"‚ùå Invalid format. Use: 1-3\")\n",
        "                continue\n",
        "\n",
        "        # Option 4: Comma-separated (e.g., 1,3,5)\n",
        "        else:\n",
        "            try:\n",
        "                indices = [int(x.strip()) for x in choice.split(',')]\n",
        "                if all(1 <= i <= len(papers) for i in indices):\n",
        "                    if len(indices) > count:\n",
        "                        print(f\"‚ö†Ô∏è Too many papers. Limiting to first {count}\")\n",
        "                        indices = indices[:count]\n",
        "                    selected = [papers[i-1] for i in indices]\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"‚ùå Numbers must be between 1-{len(papers)}\")\n",
        "            except:\n",
        "                print(\"‚ùå Invalid format. Use: 1,2,3\")\n",
        "                continue\n",
        "\n",
        "    print(f\"\\n‚úì Selected {len(selected)} paper(s) for download:\")\n",
        "    for i, p in enumerate(selected, 1):\n",
        "        print(f\"  {i}. {p['title'][:70]}...\")\n",
        "\n",
        "    return selected\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# DOC CREATION\n",
        "# ====================================================\n",
        "\n",
        "def create_doc(paper, filepath):\n",
        "    doc = Document()\n",
        "    doc.add_heading(paper[\"title\"], level=1)\n",
        "\n",
        "    doc.add_paragraph(f\"Authors: {', '.join(paper.get('authors', []))}\")\n",
        "    doc.add_paragraph(f\"Year: {paper.get('year')}\")\n",
        "    doc.add_paragraph(f\"Venue: {paper.get('venue')}\")\n",
        "    doc.add_paragraph(f\"Source: {paper.get('source')}\")\n",
        "\n",
        "    doc.add_heading(\"Abstract\", level=2)\n",
        "    doc.add_paragraph(paper.get(\"abstract\") or \"No abstract available\")\n",
        "\n",
        "    doc.add_heading(\"PDF Link\", level=2)\n",
        "    doc.add_paragraph(paper.get(\"pdf_url\") or \"Not Available\")\n",
        "\n",
        "    doc.save(filepath)\n",
        "    return filepath\n",
        "\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# PDF DOWNLOADER (FIXED)\n",
        "# ====================================================\n",
        "\n",
        "def verify_pdf(path):\n",
        "    try:\n",
        "        if os.path.getsize(path) < 5000:\n",
        "            return False\n",
        "        doc = fitz.open(path)\n",
        "        is_valid = len(doc) > 0\n",
        "        doc.close()\n",
        "        return is_valid\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "\n",
        "def download_papers(selected, output_dir=\"downloads\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    results = []\n",
        "\n",
        "    for i, p in enumerate(selected, 1):\n",
        "        print(f\"\\n[{i}] üìÑ Downloading: {p['title'][:70]}\")\n",
        "\n",
        "        safe = \"\".join(c for c in p[\"title\"] if c.isalnum())[:50]\n",
        "\n",
        "        # Create dedicated folder for each paper\n",
        "        paper_folder = f\"{output_dir}/paper_{i}_{safe}\"\n",
        "        os.makedirs(paper_folder, exist_ok=True)\n",
        "\n",
        "        pdf_path = f\"{paper_folder}/{safe}.pdf\"\n",
        "        doc_path = f\"{paper_folder}/{safe}.docx\"\n",
        "        meta_path = f\"{paper_folder}/metadata.json\"\n",
        "\n",
        "        # FIXED: Download PDF with proper headers and error handling\n",
        "        pdf_downloaded = False\n",
        "        try:\n",
        "            # Add headers to mimic browser request\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "                'Accept': 'application/pdf,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "                'Accept-Language': 'en-US,en;q=0.5',\n",
        "                'Connection': 'keep-alive',\n",
        "            }\n",
        "\n",
        "            print(f\"   üîÑ Attempting download from: {p['pdf_url'][:50]}...\")\n",
        "\n",
        "            # Make request with headers, redirects, and longer timeout\n",
        "            r = requests.get(\n",
        "                p[\"pdf_url\"],\n",
        "                headers=headers,\n",
        "                timeout=30,\n",
        "                allow_redirects=True,\n",
        "                stream=True  # Stream to handle large files\n",
        "            )\n",
        "\n",
        "            print(f\"   üì° Response status: {r.status_code}\")\n",
        "\n",
        "            if r.status_code == 200:\n",
        "                # Check if response is actually PDF\n",
        "                content_type = r.headers.get('content-type', '').lower()\n",
        "                print(f\"   üìã Content-Type: {content_type}\")\n",
        "\n",
        "                # Write file\n",
        "                with open(pdf_path, \"wb\") as f:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "\n",
        "                print(f\"   üíæ File saved ({os.path.getsize(pdf_path)} bytes)\")\n",
        "\n",
        "                # Verify PDF\n",
        "                if verify_pdf(pdf_path):\n",
        "                    print(\"   ‚úì PDF downloaded and verified successfully!\")\n",
        "                    pdf_downloaded = True\n",
        "                else:\n",
        "                    print(\"   ‚ö†Ô∏è PDF verification failed - file may be corrupted\")\n",
        "                    # Keep the file anyway for manual inspection\n",
        "                    pdf_downloaded = False\n",
        "            else:\n",
        "                print(f\"   ‚úó HTTP Error {r.status_code}\")\n",
        "                pdf_path = None\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(\"   ‚úó Download timeout - server took too long to respond\")\n",
        "            pdf_path = None\n",
        "        except requests.exceptions.ConnectionError:\n",
        "            print(\"   ‚úó Connection error - unable to reach server\")\n",
        "            pdf_path = None\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"   ‚úó Request error: {str(e)[:100]}\")\n",
        "            pdf_path = None\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚úó Unexpected error: {str(e)[:100]}\")\n",
        "            pdf_path = None\n",
        "\n",
        "        # Create DOC file\n",
        "        try:\n",
        "            create_doc(p, doc_path)\n",
        "            print(f\"   ‚úì DOCX created\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è DOCX creation warning: {str(e)[:50]}\")\n",
        "\n",
        "        # Save metadata in JSON\n",
        "        try:\n",
        "            metadata = {\n",
        "                \"paper_id\": f\"paper_{i}_{safe}\",\n",
        "                \"title\": p[\"title\"],\n",
        "                \"authors\": p[\"authors\"],\n",
        "                \"year\": p.get(\"year\"),\n",
        "                \"source\": p[\"source\"],\n",
        "                \"venue\": p.get(\"venue\"),\n",
        "                \"pdf_url\": p[\"pdf_url\"],\n",
        "                \"abstract\": p.get(\"abstract\"),\n",
        "                \"citation_count\": p.get(\"citationCount\", 0),\n",
        "                \"download_date\": datetime.now().isoformat(),\n",
        "                \"pdf_downloaded\": pdf_downloaded,\n",
        "                \"pdf_path\": pdf_path if pdf_downloaded else None,\n",
        "                \"doc_path\": doc_path,\n",
        "                \"folder_path\": paper_folder\n",
        "            }\n",
        "\n",
        "            with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
        "            print(f\"   ‚úì Metadata saved\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Metadata save warning: {str(e)[:50]}\")\n",
        "\n",
        "        print(f\"   üìÅ Folder: {paper_folder}\")\n",
        "\n",
        "        # Download button for PDF (colab) - only if PDF exists\n",
        "        if pdf_downloaded and pdf_path and os.path.exists(pdf_path):\n",
        "            try:\n",
        "                print(\"   üîΩ Triggering browser download...\")\n",
        "                files.download(os.path.abspath(pdf_path))\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è Browser download failed: {str(e)[:50]}\")\n",
        "\n",
        "        results.append({\n",
        "            \"paper\": p,\n",
        "            \"folder\": paper_folder,\n",
        "            \"pdf\": pdf_path if pdf_downloaded else None,\n",
        "            \"doc\": doc_path,\n",
        "            \"metadata\": meta_path,\n",
        "            \"pdf_downloaded\": pdf_downloaded\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# REPORT GENERATION\n",
        "# ====================================================\n",
        "\n",
        "def save_report(results, topic):\n",
        "    os.makedirs(\"data/reports\", exist_ok=True)\n",
        "    path = f\"data/reports/report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "    report = {\n",
        "        \"topic\": topic,\n",
        "        \"download_date\": datetime.now().isoformat(),\n",
        "        \"total_papers\": len(results),\n",
        "        \"successful_downloads\": sum(1 for r in results if r.get(\"pdf_downloaded\")),\n",
        "        \"failed_downloads\": sum(1 for r in results if not r.get(\"pdf_downloaded\")),\n",
        "        \"papers\": results\n",
        "    }\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nüíæ Saved report ‚Üí {path}\")\n",
        "    return path\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# MAIN FUNCTION\n",
        "# ====================================================\n",
        "\n",
        "def main_download(filepath=None, download_count=3):\n",
        "    print(\"\\n=== MODULE 2: PDF Downloader (Updated) ===\\n\")\n",
        "\n",
        "    data = load_search_results(filepath)\n",
        "    if not data:\n",
        "        return None\n",
        "\n",
        "    # Check if papers array is empty\n",
        "    if not data.get('papers') or len(data['papers']) == 0:\n",
        "        print(\"\\n‚ùå No papers available for download. Please run Module 1 first.\")\n",
        "        return None\n",
        "\n",
        "    selected = select_top_papers(data[\"papers\"], download_count)\n",
        "    downloaded = download_papers(selected)\n",
        "\n",
        "    save_report(downloaded, data[\"topic\"])\n",
        "\n",
        "    # Print summary\n",
        "    successful = sum(1 for r in downloaded if r.get(\"pdf_downloaded\"))\n",
        "    failed = len(downloaded) - successful\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DOWNLOAD SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total papers: {len(downloaded)}\")\n",
        "    print(f\"‚úì Successful PDF downloads: {successful}\")\n",
        "    print(f\"‚úó Failed PDF downloads: {failed}\")\n",
        "    print(f\"‚úì DOCX files created: {len(downloaded)}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\nüéâ Module 2 completed!\")\n",
        "    return downloaded\n",
        "\n",
        "\n",
        "# Run module\n",
        "if __name__ == \"__main__\":\n",
        "    main_download(download_count=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "RJ-Li5Jb9LNH",
        "outputId": "242738f9-5cea-4bdb-b309-00a8b23123b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "\u001b[96m\n",
            "=== MODULE 3: PDF EXTRACTION ===\u001b[0m\n",
            "\u001b[92mFound 1 PDFs\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### üìÑ Processing: `Deep1DConvnetforaccurateParkinsondiseasedetectiona.pdf`"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### ‚úÖ Sections Found: `['abstract', 'introduction', 'methods', 'results', 'conclusion', 'references']`"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### üîπ ABSTRACT"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Diagnosing Parkinson's disease is a complex task that requires the evaluation of several motor and non-motor symptoms. During diagnosis, gait abnormalities are among the important symptoms that physicians should consider. However, gait evaluation is challenging and relies on the expertise and subjectivity of clinicians. In this context, the use of an intelligent gait analysis algorithm may assist physicians in order to facilitate the diagnosis process. This paper proposes a novel intelligent Par"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### üîπ INTRODUCTION"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Today, over 10 millions people su er from Parkinson's disease. Unfortunately, no cure exists to heal this disorder. That is why early diagnosis is important to improve the patient's treatment. Currently, physicians evaluate symptoms such as shaking, diculty to initiate movements, slowness, and diculty to walk (Jankovic, 2008). One of the most used tools in Parkinson clinical evaluation is the Uni ed Parkinson's Disease Rating Scale (UPDRS). This scale consists of 42 criteria/questions that cover"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### üîπ METHODS"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "in the detection of Parkinson disease based on gait data. The proposed algorithm achieved an accuracy of 98 :7%. To our knowledge, this is the state-of-the-start performance in Parkinson's gait recognition. Furthermore, we achieved an accuracy of 85 :3% in Parkinson's severity prediction. To the best of our knowledge, this is the rst algorithm to perform a severity prediction based on the UPDRS. These results show that the model is able to learn intrinsic characteristics from gait data and to g"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### üîπ RESULTS"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "show that the model is able to learn intrinsic characteristics from gait data and to generalize to unseen subjects, which could be helpful in a clinical diagnosis. Keywords: 1D-Convnet, Parkinson, gait, Classi cation, Deep learning 1. Introduction Today, over 10 millions people su er from Parkinson's disease. Unfortunately, no cure exists to heal this disorder. That is why early diagnosis is important to improve the patient's treatment. Currently, physicians evaluate symptoms such as shaking, d"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### üîπ CONCLUSION"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ", and we nally conclude in section 6. 2. Related work and background 2.1. Previous gait classi cation methods Various feature extraction and classi cation approaches have been explored in previous work for gait analysis. Some of them extracted temporal patterns, while others used frequential features. In the temporal domain, Ertu grul et al. (2016) proposed an algorithm based on shifted 1D local binary patterns (1D-LBP) with machine learning classi ers. They used 18 VGRF input signals coming fro"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### üîπ REFERENCES"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Daliri, M.R., 2013. Chi-square distance kernel of the gaits for the diagnosis of parkinson's disease. Biomedical Signal Processing and Control 8, 66{70. Dozat, T., 2016. Incorporating nesterov momentum into adam . Ertu grul, O.F., Kaya, Y., Tekin, R., Almal, M.N., 2016. Detection of parkinson's disease by shifted one dimensional local binary patterns from gait. Expert Systems with Applications 56, 156{163. Fahn, S., Elton, R., et al., 1987. Updrs program members. uni ed parkinsons disease rati"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mSaved extracted data successfully\u001b[0m\n",
            "\u001b[92mDone! Extracted 1 papers\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# MODULE 3 ‚Äî FINAL PDF TEXT EXTRACTION (RED OUTPUT VERSION)\n",
        "# ============================================\n",
        "\n",
        "# ============================================\n",
        "# INSTALLS (Colab safe)\n",
        "# ============================================\n",
        "!pip install pymupdf4llm pymupdf tqdm python-dotenv -q\n",
        "!pip install PyPDF2\n",
        "\n",
        "\n",
        "import json\n",
        "import os\n",
        "import PyPDF2\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pymupdf4llm\n",
        "import fitz\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# ============================================\n",
        "# COLOR CODES\n",
        "# ============================================\n",
        "RED = \"\\033[91m\"\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "CYAN = \"\\033[96m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "# ============================================\n",
        "# BASIC TEXT CLEANER\n",
        "# ============================================\n",
        "def clean_text_basic(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'-\\s+', '', text)\n",
        "    text = ''.join(ch for ch in text if ord(ch) >= 32)\n",
        "    return text.strip()\n",
        "\n",
        "# ============================================\n",
        "# 1. TEXT EXTRACTION\n",
        "# ============================================\n",
        "# def extract_text_improved(pdf_path):\n",
        "#     try:\n",
        "#         doc = fitz.open(pdf_path)\n",
        "\n",
        "#         first_page = doc[0].get_text().lower()\n",
        "#         block_keywords = [\"copyright\", \"removed\", \"takedown\", \"deleted\"]\n",
        "#         if any(k in first_page for k in block_keywords):\n",
        "#             print(f\"{RED}  Blocked PDF (copyright notice){RESET}\")\n",
        "#             return None\n",
        "\n",
        "#         text_candidates = []\n",
        "\n",
        "#         try:\n",
        "#             md = pymupdf4llm.to_markdown(pdf_path)\n",
        "#             if md and len(md) > 800:\n",
        "#                 text_candidates.append(md)\n",
        "#         except:\n",
        "#             pass\n",
        "\n",
        "#         raw = \"\"\n",
        "#         for i in range(min(40, len(doc))):\n",
        "#             raw += doc[i].get_text() + \"\\n\"\n",
        "#         if len(raw) > 500:\n",
        "#             text_candidates.append(raw)\n",
        "\n",
        "#         doc.close()\n",
        "\n",
        "#         if not text_candidates:\n",
        "#             return None\n",
        "\n",
        "#         return max(text_candidates, key=len)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"{RED}Extraction error: {e}{RESET}\")\n",
        "#         return None\n",
        "\n",
        "def extract_text(pdf_path):\n",
        "    text = \"\"\n",
        "\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text\n",
        "\n",
        "        if len(text.strip()) < 200:\n",
        "            return None\n",
        "\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"error in pdf extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 2. SECTION EXTRACTION\n",
        "# ============================================\n",
        "def extract_sections_improved(text):\n",
        "    sections = {\n",
        "        \"title\": \"\",\n",
        "        \"abstract\": \"\",\n",
        "        \"introduction\": \"\",\n",
        "        \"methods\": \"\",\n",
        "        \"results\": \"\",\n",
        "        \"conclusion\": \"\",\n",
        "        \"references\": \"\",\n",
        "        \"extracted_text\": text[:20000]\n",
        "    }\n",
        "\n",
        "    if not text or len(text) < 500:\n",
        "        return sections\n",
        "\n",
        "    text = clean_text_basic(text)\n",
        "\n",
        "    # -------------------------\n",
        "    # 1Ô∏è‚É£ TITLE (first long line)\n",
        "    # -------------------------\n",
        "    for line in text.split(\"\\n\")[:10]:\n",
        "        if 20 < len(line.strip()) < 200:\n",
        "            sections[\"title\"] = line.strip()\n",
        "            break\n",
        "\n",
        "    # -------------------------\n",
        "    # 2Ô∏è‚É£ ABSTRACT (keyword based)\n",
        "    # -------------------------\n",
        "    abs_match = re.search(\n",
        "        r\"(abstract\\s*[:\\-‚Äì]?\\s*)(.{300,2000})\",\n",
        "        text,\n",
        "        re.IGNORECASE | re.DOTALL\n",
        "    )\n",
        "    if abs_match:\n",
        "        sections[\"abstract\"] = abs_match.group(2)[:2000]\n",
        "\n",
        "    # -------------------------\n",
        "    # 3Ô∏è‚É£ HEADING-BASED SECTIONS\n",
        "    # -------------------------\n",
        "    patterns = {\n",
        "        \"introduction\": r\"(introduction\\s*[:\\-‚Äì]?\\s*)(.{300,3000})\",\n",
        "        \"methods\": r\"(methods?|methodology\\s*[:\\-‚Äì]?\\s*)(.{300,3000})\",\n",
        "        \"results\": r\"(results?|findings\\s*[:\\-‚Äì]?\\s*)(.{300,3000})\",\n",
        "        \"conclusion\": r\"(conclusion|discussion\\s*[:\\-‚Äì]?\\s*)(.{300,3000})\",\n",
        "        \"references\": r\"(references|bibliography\\s*[:\\-‚Äì]?\\s*)(.{200,2000})\",\n",
        "    }\n",
        "\n",
        "    for sec, pattern in patterns.items():\n",
        "        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            sections[sec] = match.group(2)[:3000]\n",
        "\n",
        "    return sections\n",
        "\n",
        "# ============================================\n",
        "# 3. SMART PDF PROCESSOR\n",
        "# ============================================\n",
        "# def process_paper_smart(pdf_path):\n",
        "#     print(f\"{CYAN}\\nProcessing: {pdf_path.name}{RESET}\")\n",
        "\n",
        "#     if pdf_path.stat().st_size < 10_000:\n",
        "#         print(f\"{YELLOW}File too small, skipping{RESET}\")\n",
        "#         return None\n",
        "\n",
        "#     text = extract_text(pdf_path)\n",
        "#     if not text:\n",
        "#         print(f\"{RED}No text extracted{RESET}\")\n",
        "#         return None\n",
        "\n",
        "#     sections = extract_sections_improved(text)\n",
        "#     found = [k for k in sections if sections[k] and k != \"extracted_text\"]\n",
        "\n",
        "#     # ‚úÖ DEBUG: SECTION-WISE OUTPUT\n",
        "#     print(f\"{GREEN}Sections found:{RESET}\", found)\n",
        "#     for sec in found:\n",
        "#         print(f\"\\n--- {sec.upper()} ---\")\n",
        "#         print(sections[sec][:300])\n",
        "\n",
        "#     return {\n",
        "#         \"paper_id\": pdf_path.stem,\n",
        "#         \"filename\": pdf_path.name,\n",
        "#         \"total_characters\": len(text),\n",
        "#         \"sections_found\": found,\n",
        "#         \"sections\": sections,\n",
        "#         \"status\": \"success\"\n",
        "#     }\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def process_paper_smart(pdf_path):\n",
        "    display(Markdown(f\"### üìÑ Processing: `{pdf_path.name}`\"))\n",
        "\n",
        "    if pdf_path.stat().st_size < 10_000:\n",
        "        display(Markdown(\"‚ö†Ô∏è File too small, skipping\"))\n",
        "        return None\n",
        "\n",
        "    text = extract_text(pdf_path)\n",
        "    if not text:\n",
        "        display(Markdown(\"‚ùå No text extracted\"))\n",
        "        return None\n",
        "\n",
        "    sections = extract_sections_improved(text)\n",
        "\n",
        "    found_sections = [\n",
        "        k for k in sections\n",
        "        if sections[k] and k != \"extracted_text\"\n",
        "    ]\n",
        "\n",
        "    display(Markdown(f\"### ‚úÖ Sections Found: `{found_sections}`\"))\n",
        "\n",
        "    for sec in found_sections:\n",
        "        preview = sections[sec][:500].strip()\n",
        "        display(Markdown(f\"#### üîπ {sec.upper()}\"))\n",
        "        display(Markdown(preview if preview else \"_Empty section_\"))\n",
        "\n",
        "    return {\n",
        "        \"paper_id\": pdf_path.stem,\n",
        "        \"filename\": pdf_path.name,\n",
        "        \"total_characters\": len(text),\n",
        "        \"sections_found\": found_sections,\n",
        "        \"sections\": sections,\n",
        "        \"status\": \"success\"\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 4. AUTO PDF LOCATOR (FIXED)\n",
        "# ============================================\n",
        "def get_downloaded_papers():\n",
        "    possible_dirs = [\n",
        "        \"downloads\",\n",
        "        \"data/pdfs\",\n",
        "        \"papers\",\n",
        "        \".\"\n",
        "    ]\n",
        "\n",
        "    pdfs = []\n",
        "    for d in possible_dirs:\n",
        "        path = Path(d)\n",
        "        if path.exists():\n",
        "            found = list(path.rglob(\"*.pdf\"))\n",
        "            pdfs.extend(found)\n",
        "\n",
        "    pdfs = list(set(pdfs))\n",
        "    print(f\"{GREEN}Found {len(pdfs)} PDFs{RESET}\")\n",
        "    return pdfs\n",
        "\n",
        "# ============================================\n",
        "# 5. SAVE OUTPUT\n",
        "# ============================================\n",
        "def save_results(results):\n",
        "    out = Path(\"data/extracted\")\n",
        "    out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for r in results:\n",
        "        with open(out / f\"{r['paper_id']}.json\", \"w\", encoding=\"utf8\") as f:\n",
        "            json.dump(r, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"{GREEN}Saved extracted data successfully{RESET}\")\n",
        "\n",
        "# ============================================\n",
        "# 6. RUN MODULE 3\n",
        "# ============================================\n",
        "def run_complete_extraction(max_papers=5):\n",
        "    print(f\"{CYAN}\\n=== MODULE 3: PDF EXTRACTION ==={RESET}\")\n",
        "\n",
        "    pdfs = get_downloaded_papers()\n",
        "    if not pdfs:\n",
        "        print(f\"{RED}No PDFs found. Check Module-2 paths.{RESET}\")\n",
        "        return []\n",
        "\n",
        "    pdfs = pdfs[:max_papers]\n",
        "    results = []\n",
        "\n",
        "    for pdf in tqdm(pdfs):\n",
        "        r = process_paper_smart(pdf)\n",
        "        if r:\n",
        "            results.append(r)\n",
        "\n",
        "    if results:\n",
        "        save_results(results)\n",
        "\n",
        "    print(f\"{GREEN}Done! Extracted {len(results)} papers{RESET}\")\n",
        "    return results\n",
        "\n",
        "# ============================================\n",
        "# AUTO RUN\n",
        "# ============================================\n",
        "results = run_complete_extraction()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAtNeFhr9QZI",
        "outputId": "45350f25-eadc-460b-b935-d203ab4ede23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== PAPER ANALYSIS MODULE ===\n",
            "No extracted papers found. Run Module 3 first.\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Key Findings:\n",
            "\n",
            "Deep1DConvnetforaccurateParkinsondiseasedetectiona:\n",
            "\n",
            "Similarity Matrix:\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# MODULE 4: CROSS-PAPER ANALYSIS + GRAPH VISUALIZATIONS\n",
        "\n",
        "\n",
        "!pip install scikit-learn numpy matplotlib seaborn -q\n",
        "\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "\n",
        "# 1. LOAD EXTRACTED PAPERS\n",
        "\n",
        "\n",
        "def load_extracted_papers(data_dir=\"data/extracted\"):\n",
        "    data_path = Path(data_dir)\n",
        "    papers = []\n",
        "    json_files = list(data_path.glob(\"*_extracted.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(\"No extracted papers found. Run Module 3 first.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Loading {len(json_files)} extracted papers...\")\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                papers.append(data)\n",
        "                print(f\"  ‚úì {data['paper_id']}: {data['total_characters']:,} chars\")\n",
        "        except Exception as e:\n",
        "            print(\"Error loading\", json_file, \"->\", e)\n",
        "\n",
        "    return papers\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 2. VISUALIZATIONS FOR SINGLE PAPER\n",
        "# ================================\n",
        "\n",
        "def visualize_single_paper_analysis(analysis):\n",
        "    print(\"\\nGenerating visual graphs for SINGLE PAPER analysis...\")\n",
        "\n",
        "    # 1. SECTION LENGTH BAR CHART\n",
        "    structure = analysis[\"paper_structure\"]\n",
        "    sec_lengths = structure[\"section_lengths\"]\n",
        "\n",
        "    if sec_lengths:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.bar(sec_lengths.keys(), sec_lengths.values(), color=\"skyblue\")\n",
        "        plt.title(\"Section Lengths in Paper\")\n",
        "        plt.ylabel(\"Characters\")\n",
        "        plt.xticks(rotation=40)\n",
        "        plt.show()\n",
        "\n",
        "    # 2. RESEARCH QUALITY PIE CHART\n",
        "    quality = analysis[\"research_quality_indicators\"]\n",
        "    labels = [\"Score\", \"Remaining\"]\n",
        "    values = [quality[\"percentage\"], 100 - quality[\"percentage\"]]\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.pie(values, labels=labels, autopct=\"%1.1f%%\", colors=[\"green\", \"lightgray\"])\n",
        "    plt.title(\"Research Quality Score\")\n",
        "    plt.show()\n",
        "\n",
        "    # 3. METHODS / FINDINGS / LIMITATIONS COUNT BAR CHART\n",
        "    lbls = [\"Methods\", \"Findings\", \"Limitations\"]\n",
        "    counts = [\n",
        "        len(analysis[\"methods_used\"]),\n",
        "        len(analysis[\"key_findings\"]),\n",
        "        len(analysis[\"limitations\"]),\n",
        "    ]\n",
        "\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.bar(lbls, counts, color=[\"steelblue\", \"orange\", \"red\"])\n",
        "    plt.title(\"Extracted Insights Overview\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 3. VISUALIZATIONS FOR MULTIPLE PAPERS\n",
        "# ================================\n",
        "\n",
        "def visualize_comparison(papers_info, comparison, similarity_scores):\n",
        "    print(\"\\nGenerating visual graphs for MULTI-PAPER comparison...\")\n",
        "\n",
        "    paper_ids = [p[\"paper_id\"] for p in papers_info]\n",
        "\n",
        "    # 1. METHODS COUNT PER PAPER\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    method_counts = [len(p[\"methods\"]) for p in papers_info]\n",
        "    plt.bar(paper_ids, method_counts, color=\"purple\")\n",
        "    plt.title(\"Methods Identified per Paper\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=40)\n",
        "    plt.show()\n",
        "\n",
        "    # 2. DATASET COUNT PER PAPER\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    dataset_counts = [len(p[\"datasets\"]) for p in papers_info]\n",
        "    plt.bar(paper_ids, dataset_counts, color=\"teal\")\n",
        "    plt.title(\"Datasets Mentioned per Paper\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=40)\n",
        "    plt.show()\n",
        "\n",
        "    # 3. TIMELINE (YEARS)\n",
        "    years = []\n",
        "    valid_ids = []\n",
        "    for p in papers_info:\n",
        "        if p[\"year\"].isdigit():\n",
        "            years.append(int(p[\"year\"]))\n",
        "            valid_ids.append(p[\"paper_id\"])\n",
        "\n",
        "    if years:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(valid_ids, years, marker=\"o\")\n",
        "        plt.title(\"Publication Timeline\")\n",
        "        plt.ylabel(\"Year\")\n",
        "        plt.xticks(rotation=40)\n",
        "        plt.show()\n",
        "\n",
        "    # 4. SIMILARITY HEATMAP\n",
        "    matrix = []\n",
        "    for pid1 in paper_ids:\n",
        "        row = []\n",
        "        for pid2 in paper_ids:\n",
        "            if pid1 == pid2:\n",
        "                row.append(1.0)\n",
        "            else:\n",
        "                row.append(similarity_scores.get(pid1, {}).get(pid2, 0))\n",
        "        matrix.append(row)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(matrix, annot=True, cmap=\"coolwarm\", xticklabels=paper_ids, yticklabels=paper_ids)\n",
        "    plt.title(\"Paper Similarity Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4. INFORMATION EXTRACTION FUNCTIONS\n",
        "# ================================\n",
        "def extract_key_information(paper):\n",
        "    return {\n",
        "        \"paper_id\": paper[\"paper_id\"],\n",
        "        \"title\": paper[\"sections\"].get(\"title\", \"Unknown\"),\n",
        "        \"year\": extract_year(paper),\n",
        "        \"methods\": extract_methods(paper),\n",
        "        \"datasets\": extract_datasets(paper),\n",
        "        \"key_findings\": extract_key_findings(paper),\n",
        "        \"limitations\": extract_limitations(paper),\n",
        "        \"contributions\": extract_contributions(paper),\n",
        "        \"metrics\": extract_metrics(paper),\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# VALIDATION FUNCTION (FIXED INDENTATION)\n",
        "# ============================================\n",
        "def validate_extraction(paper):\n",
        "    \"\"\"\n",
        "    Validates correctness and completeness of extracted textual data.\n",
        "    \"\"\"\n",
        "\n",
        "    sections = paper.get(\"sections\", {})\n",
        "\n",
        "    validation = {\n",
        "        \"text_present\": False,\n",
        "        \"critical_sections_present\": False,\n",
        "        \"key_findings_present\": False,\n",
        "        \"issues\": []\n",
        "    }\n",
        "\n",
        "    # 1Ô∏è‚É£ Check extracted text length\n",
        "    extracted_text = sections.get(\"extracted_text\", \"\")\n",
        "    if extracted_text and len(extracted_text) > 500:\n",
        "        validation[\"text_present\"] = True\n",
        "    else:\n",
        "        validation[\"issues\"].append(\"Insufficient extracted text\")\n",
        "\n",
        "    # 2Ô∏è‚É£ Check Results / Conclusion\n",
        "    if sections.get(\"results\") or sections.get(\"conclusion\"):\n",
        "        validation[\"critical_sections_present\"] = True\n",
        "    else:\n",
        "        validation[\"issues\"].append(\"Results/Conclusion missing\")\n",
        "\n",
        "    # 3Ô∏è‚É£ Check key findings\n",
        "    findings = extract_key_findings(paper)\n",
        "    if findings:\n",
        "        validation[\"key_findings_present\"] = True\n",
        "    else:\n",
        "        validation[\"issues\"].append(\"No key findings extracted\")\n",
        "\n",
        "    validation[\"is_valid\"] = (\n",
        "        validation[\"text_present\"]\n",
        "        and validation[\"critical_sections_present\"]\n",
        "        and validation[\"key_findings_present\"]\n",
        "    )\n",
        "\n",
        "    return validation\n",
        "\n",
        "\n",
        "\n",
        "# --- (All your original extraction functions remain unchanged)\n",
        "# I am keeping them EXACTLY as they were ‚Äî no modification ‚Äî to avoid breaking logic.\n",
        "\n",
        "# ---------------- YEAR EXTRACTION ------------------\n",
        "def extract_year(paper):\n",
        "    title = paper[\"sections\"].get(\"title\", \"\")\n",
        "    year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", title)\n",
        "    if year_match:\n",
        "        return year_match.group()\n",
        "\n",
        "    text = paper[\"sections\"].get(\"extracted_text\", \"\")\n",
        "    year_match = re.search(r\"\\b(19|20)\\d{2}\\b\", text[:5000])\n",
        "    return year_match.group() if year_match else \"Unknown\"\n",
        "\n",
        "# ---------------- METHODS EXTRACTION ------------------\n",
        "def extract_methods(paper):\n",
        "    methods_text = paper[\"sections\"].get(\"methods\", \"\")\n",
        "    if not methods_text:\n",
        "        methods_text = paper[\"sections\"].get(\"extracted_text\", \"\")[:5000]\n",
        "\n",
        "    keywords = [\n",
        "        \"deep learning\", \"machine learning\", \"neural network\", \"transformer\",\n",
        "        \"cnn\", \"rnn\", \"lstm\", \"bert\", \"gpt\", \"reinforcement learning\",\n",
        "        \"svm\", \"xgboost\", \"bayesian\", \"regression\", \"classification\"\n",
        "    ]\n",
        "\n",
        "    found = []\n",
        "    sentences = re.split(r\"[.!?]+\", methods_text.lower())\n",
        "\n",
        "    for s in sentences:\n",
        "        for k in keywords:\n",
        "            if k in s and len(s) > 20:\n",
        "                s_clean = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "                found.append(s_clean[:200])\n",
        "                break\n",
        "\n",
        "    return list(set(found))[:5]\n",
        "\n",
        "# ---------------- DATASETS EXTRACTION ------------------\n",
        "def extract_datasets(paper):\n",
        "    text = paper[\"sections\"].get(\"extracted_text\", \"\")[:10000].lower()\n",
        "    dataset_keywords = [\n",
        "        \"imagenet\", \"cifar\", \"mnist\", \"coco\", \"pascal\", \"kaggle\", \"uci\", \"dataset\"\n",
        "    ]\n",
        "\n",
        "    found = []\n",
        "    for k in dataset_keywords:\n",
        "        if k in text:\n",
        "            found.append(k)\n",
        "\n",
        "    return list(set(found))[:5]\n",
        "\n",
        "# ---------------- FINDINGS EXTRACTION ------------------\n",
        "def extract_key_findings(paper):\n",
        "    text = paper[\"sections\"].get(\"results\", \"\") + \" \" + paper[\"sections\"].get(\"conclusion\", \"\")\n",
        "    if not text:\n",
        "        text = paper[\"sections\"].get(\"extracted_text\", \"\")[:3000]\n",
        "\n",
        "    keywords = [\"result\", \"improve\", \"increase\", \"reduce\", \"outperform\", \"accuracy\", \"significant\"]\n",
        "\n",
        "    findings = []\n",
        "    sentences = re.split(r\"[.!?]+\", text.lower())\n",
        "\n",
        "    for s in sentences:\n",
        "        if any(k in s for k in keywords) and len(s) > 30:\n",
        "            findings.append(s[:300])\n",
        "\n",
        "    return list(set(findings))[:5]\n",
        "\n",
        "# ---------------- LIMITATIONS EXTRACT ------------------\n",
        "def extract_limitations(paper):\n",
        "    text = paper[\"sections\"].get(\"conclusion\", \"\") + paper[\"sections\"].get(\"extracted_text\", \"\")[:3000]\n",
        "    keywords = [\"limitation\", \"future\", \"issue\", \"weakness\", \"challenge\"]\n",
        "\n",
        "    limits = []\n",
        "    for s in re.split(r\"[.!?]+\", text.lower()):\n",
        "        if any(k in s for k in keywords) and len(s) > 30:\n",
        "            limits.append(s[:300])\n",
        "\n",
        "    return list(set(limits))[:3]\n",
        "\n",
        "# ---------------- CONTRIBUTIONS ------------------\n",
        "def extract_contributions(paper):\n",
        "    text = paper[\"sections\"].get(\"abstract\", \"\") + paper[\"sections\"].get(\"introduction\", \"\")\n",
        "    keywords = [\"contribution\", \"propose\", \"introduce\", \"present\"]\n",
        "\n",
        "    contrib = []\n",
        "    for s in re.split(r\"[.!?]+\", text.lower()):\n",
        "        if any(k in s for k in keywords) and len(s) > 30:\n",
        "            contrib.append(s[:300])\n",
        "\n",
        "    return list(set(contrib))[:3]\n",
        "\n",
        "# ---------------- METRICS ------------------\n",
        "def extract_metrics(paper):\n",
        "    text = paper[\"sections\"].get(\"results\", \"\")\n",
        "    patterns = [r\"accuracy\\s*[:,=]?\\s*\\d+\\.?\\d*%?\"]\n",
        "    metrics = []\n",
        "\n",
        "    for p in patterns:\n",
        "        metrics.extend(re.findall(p, text.lower()))\n",
        "\n",
        "    return list(set(metrics))[:5]\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 5. COMPARISON FUNCTIONS\n",
        "# ================================\n",
        "\n",
        "def compare_papers(papers_info):\n",
        "    return {\n",
        "        \"total_papers\": len(papers_info),\n",
        "        \"papers\": papers_info,\n",
        "        \"common_methods\": find_common_elements(papers_info, \"methods\"),\n",
        "        \"common_datasets\": find_common_elements(papers_info, \"datasets\"),\n",
        "        \"similarities\": find_similarities(papers_info),\n",
        "        \"differences\": find_differences(papers_info),\n",
        "        \"timeline_analysis\": analyze_timeline(papers_info),\n",
        "        \"research_gaps\": identify_research_gaps(papers_info)\n",
        "    }\n",
        "\n",
        "# ---- SUPPORTING FUNCTIONS REMAIN THE SAME ----\n",
        "def find_similarities(papers_info):\n",
        "    sims = defaultdict(int)\n",
        "    for paper in papers_info:\n",
        "        for m in paper[\"methods\"]:\n",
        "            sims[m[:50]] += 1\n",
        "    return [k for k, v in sims.items() if v > 1]\n",
        "\n",
        "def find_differences(papers_info):\n",
        "    uniq = {}\n",
        "    all_methods = set()\n",
        "    for p in papers_info:\n",
        "        all_methods.update(p[\"methods\"])\n",
        "    for p in papers_info:\n",
        "        uniq[p[\"paper_id\"]] = list(set(p[\"methods\"]) - (all_methods - set(p[\"methods\"])))\n",
        "    return uniq\n",
        "\n",
        "def find_common_elements(papers_info, key):\n",
        "    sets = [set(p[key]) for p in papers_info if p[key]]\n",
        "    return list(set.intersection(*sets)) if sets else []\n",
        "\n",
        "def analyze_timeline(papers_info):\n",
        "    years = [int(p[\"year\"]) for p in papers_info if p[\"year\"].isdigit()]\n",
        "    if not years:\n",
        "        return {\"note\": \"Not enough year data\"}\n",
        "    return {\"earliest\": min(years), \"latest\": max(years), \"range\": max(years) - min(years)}\n",
        "\n",
        "def identify_research_gaps(papers_info):\n",
        "    gaps = []\n",
        "    all_limits = []\n",
        "    for p in papers_info:\n",
        "        all_limits.extend(p[\"limitations\"])\n",
        "    if not all_limits:\n",
        "        return gaps\n",
        "    return list(set(all_limits))[:3]\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6. SIMILARITY SCORES\n",
        "# ================================\n",
        "\n",
        "def calculate_similarity_scores(papers_info):\n",
        "    texts = [\" \".join([p[\"title\"], \" \".join(p[\"methods\"])]) for p in papers_info]\n",
        "    ids = [p[\"paper_id\"] for p in papers_info]\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    mat = vectorizer.fit_transform(texts)\n",
        "    sim = cosine_similarity(mat)\n",
        "\n",
        "    scores = {}\n",
        "    for i, pid in enumerate(ids):\n",
        "        scores[pid] = {}\n",
        "        for j, pid2 in enumerate(ids):\n",
        "            if i != j:\n",
        "                scores[pid][pid2] = float(f\"{sim[i][j]:.3f}\")\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "\n",
        "# 7. SAVE RESULTS (UNCHANGED)\n",
        "\n",
        "\n",
        "def save_results(analysis_type, data, output_dir=\"data/analysis\"):\n",
        "    output = Path(output_dir)\n",
        "    output.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if analysis_type == \"single\":\n",
        "        file = output / \"single_paper_analysis.json\"\n",
        "        with open(file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "        print(\"Saved:\", file)\n",
        "\n",
        "    else:\n",
        "        file = output / \"comparison.json\"\n",
        "        with open(file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data[\"comparison\"], f, indent=2)\n",
        "\n",
        "        file2 = output / \"similarity_scores.json\"\n",
        "        with open(file2, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data[\"similarity_scores\"], f, indent=2)\n",
        "\n",
        "        print(\"Saved:\", file, \"and\", file2)\n",
        "\n",
        "    return str(output)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 8. MAIN ANALYSIS FUNCTION\n",
        "# ================================\n",
        "\n",
        "def run_analysis():\n",
        "    print(\"\\n=== PAPER ANALYSIS MODULE ===\")\n",
        "\n",
        "    papers = load_extracted_papers()\n",
        "    if not papers:\n",
        "        return None\n",
        "\n",
        "    # ---- SINGLE PAPER ----\n",
        "    if len(papers) == 1:\n",
        "        print(\"\\nOnly 1 paper found ‚Üí Running single paper analysis...\\n\")\n",
        "\n",
        "        paper = papers[0]\n",
        "        analysis = analyze_single_paper(paper=paper)\n",
        "\n",
        "        save_results(\"single\", analysis)\n",
        "\n",
        "        visualize_single_paper_analysis(analysis)\n",
        "\n",
        "        return {\"type\": \"single\", \"analysis\": analysis}\n",
        "\n",
        "    # ---- MULTIPLE PAPERS ----\n",
        "    print(f\"\\nAnalyzing {len(papers)} papers...\")\n",
        "\n",
        "    papers_info = [extract_key_information(p) for p in papers]\n",
        "    comparison = compare_papers(papers_info)\n",
        "    similarity_scores = calculate_similarity_scores(papers_info)\n",
        "\n",
        "    data = {\n",
        "        \"comparison\": comparison,\n",
        "        \"similarity_scores\": similarity_scores,\n",
        "    }\n",
        "\n",
        "    save_results(\"comparison\", data)\n",
        "\n",
        "    visualize_comparison(papers_info, comparison, similarity_scores)\n",
        "\n",
        "    return {\"type\": \"comparison\", \"data\": data}\n",
        "\n",
        "\n",
        "# ================================\n",
        "# RUN MODULE\n",
        "# ================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_analysis()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# INSTALLS (Colab safe)\n",
        "# ============================================\n",
        "!pip install pymupdf4llm pymupdf tqdm python-dotenv scikit-learn -q\n",
        "!pip install PyPDF2\n",
        "\n",
        "# ============================================\n",
        "# IMPORTS\n",
        "# ============================================\n",
        "import json\n",
        "import os\n",
        "import PyPDF2\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pymupdf4llm\n",
        "import fitz\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ============================================\n",
        "# COLOR CODES\n",
        "# ============================================\n",
        "RED = \"\\033[91m\"\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "CYAN = \"\\033[96m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "# ============================================\n",
        "# BASIC TEXT CLEANER\n",
        "# ============================================\n",
        "def clean_text_basic(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'-\\s+', '', text)\n",
        "    text = ''.join(ch for ch in text if ord(ch) >= 32)\n",
        "    return text.strip()\n",
        "\n",
        "# ============================================\n",
        "# MODULE 3 ‚Äî TEXT EXTRACTION\n",
        "# ============================================\n",
        "def extract_text(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            for page in reader.pages:\n",
        "                t = page.extract_text()\n",
        "                if t:\n",
        "                    text += t\n",
        "\n",
        "        if len(text.strip()) < 200:\n",
        "            return None\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{RED}PDF extraction error: {e}{RESET}\")\n",
        "        return None\n",
        "\n",
        "# ============================================\n",
        "# SECTION EXTRACTION\n",
        "# ============================================\n",
        "def extract_sections_improved(text):\n",
        "    sections = {\n",
        "        \"title\": \"\",\n",
        "        \"abstract\": \"\",\n",
        "        \"introduction\": \"\",\n",
        "        \"methods\": \"\",\n",
        "        \"results\": \"\",\n",
        "        \"conclusion\": \"\",\n",
        "        \"references\": \"\",\n",
        "        \"extracted_text\": text[:20000]\n",
        "    }\n",
        "\n",
        "    if not text or len(text) < 500:\n",
        "        return sections\n",
        "\n",
        "    text = clean_text_basic(text)\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    patterns = {\n",
        "        \"abstract\": [\"abstract\"],\n",
        "        \"introduction\": [\"introduction\", \"background\"],\n",
        "        \"methods\": [\"method\", \"methodology\"],\n",
        "        \"results\": [\"results\", \"findings\"],\n",
        "        \"conclusion\": [\"conclusion\", \"discussion\"],\n",
        "        \"references\": [\"references\", \"bibliography\"]\n",
        "    }\n",
        "\n",
        "    boundaries = {}\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        clean_line = line.strip()\n",
        "        for section, pats in patterns.items():\n",
        "            for p in pats:\n",
        "                regex = rf\"^{p}\\s*[:\\-‚Äì]?\\s*$\"\n",
        "                if re.match(regex, clean_line, re.IGNORECASE):\n",
        "                    boundaries[section] = i\n",
        "\n",
        "    keys = sorted(boundaries.items(), key=lambda x: x[1])\n",
        "    for i, (sec, start) in enumerate(keys):\n",
        "        end = keys[i+1][1] if i+1 < len(keys) else len(lines)\n",
        "        chunk = \"\\n\".join(lines[start+1:end])\n",
        "        if len(chunk) > 200:\n",
        "            sections[sec] = chunk[:5000]\n",
        "\n",
        "    for line in lines[:8]:\n",
        "        if 20 < len(line.strip()) < 180:\n",
        "            sections[\"title\"] = line.strip()\n",
        "            break\n",
        "\n",
        "    return sections\n",
        "\n",
        "# ============================================\n",
        "# SMART PDF PROCESSOR\n",
        "# ============================================\n",
        "def process_paper_smart(pdf_path):\n",
        "    text = extract_text(pdf_path)\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    sections = extract_sections_improved(text)\n",
        "\n",
        "    return {\n",
        "        \"paper_id\": pdf_path.stem,\n",
        "        \"filename\": pdf_path.name,\n",
        "        \"sections\": sections,\n",
        "        \"status\": \"success\"\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# PDF LOCATOR\n",
        "# ============================================\n",
        "def get_downloaded_papers():\n",
        "    paths = [\"data/pdfs\", \"papers\", \"downloads\", \".\"]\n",
        "    pdfs = []\n",
        "    for p in paths:\n",
        "        p = Path(p)\n",
        "        if p.exists():\n",
        "            pdfs.extend(p.rglob(\"*.pdf\"))\n",
        "    return list(set(pdfs))\n",
        "\n",
        "# ============================================\n",
        "# SAVE MODULE-3 OUTPUT\n",
        "# ============================================\n",
        "def save_results(results):\n",
        "    out = Path(\"data/extracted\")\n",
        "    out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for r in results:\n",
        "        with open(out / f\"{r['paper_id']}.json\", \"w\", encoding=\"utf8\") as f:\n",
        "            json.dump(r, f, indent=2)\n",
        "\n",
        "# ============================================\n",
        "# RUN MODULE-3\n",
        "# ============================================\n",
        "def run_complete_extraction():\n",
        "    pdfs = get_downloaded_papers()\n",
        "    results = []\n",
        "\n",
        "    for pdf in tqdm(pdfs):\n",
        "        r = process_paper_smart(pdf)\n",
        "        if r:\n",
        "            results.append(r)\n",
        "\n",
        "    if results:\n",
        "        save_results(results)\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================\n",
        "# MODULE-4 ‚Äî KEY FINDING EXTRACTION\n",
        "# ============================================\n",
        "def extract_key_findings_from_sections(sections):\n",
        "    findings = []\n",
        "    text = sections.get(\"results\", \"\") + \" \" + sections.get(\"conclusion\", \"\")\n",
        "    sentences = text.split(\".\")\n",
        "\n",
        "    keywords = [\n",
        "        \"accuracy\", \"result\", \"improve\",\n",
        "        \"performance\", \"achieve\",\n",
        "        \"significant\", \"%\", \"increase\"\n",
        "    ]\n",
        "\n",
        "    for s in sentences:\n",
        "        s = s.strip()\n",
        "        if len(s) > 40 and any(k in s.lower() for k in keywords):\n",
        "            findings.append(s)\n",
        "\n",
        "    return findings[:5]\n",
        "\n",
        "\n",
        "#ye naya hai function\n",
        "# ============================================\n",
        "# VALIDATION OF CORRECTNESS & COMPLETENESS\n",
        "# ============================================\n",
        "def validate_extracted_paper(paper):\n",
        "    sections = paper.get(\"sections\", {})\n",
        "\n",
        "    validation = {\n",
        "        \"paper_id\": paper.get(\"paper_id\"),\n",
        "        \"text_ok\": False,\n",
        "        \"sections_ok\": False,\n",
        "        \"key_findings_ok\": False,\n",
        "        \"issues\": []\n",
        "    }\n",
        "\n",
        "    # 1Ô∏è‚É£ Text validation\n",
        "    extracted_text = sections.get(\"extracted_text\", \"\")\n",
        "    if extracted_text and len(extracted_text) > 1000:\n",
        "        validation[\"text_ok\"] = True\n",
        "    else:\n",
        "        validation[\"issues\"].append(\"Insufficient extracted text\")\n",
        "\n",
        "    # 2Ô∏è‚É£ Section completeness\n",
        "    required_sections = [\"abstract\", \"introduction\", \"results\", \"conclusion\"]\n",
        "    present = [s for s in required_sections if sections.get(s)]\n",
        "\n",
        "    if len(present) >= 2:\n",
        "        validation[\"sections_ok\"] = True\n",
        "    else:\n",
        "        validation[\"issues\"].append(\"Core sections missing\")\n",
        "\n",
        "    # 3Ô∏è‚É£ Key findings validation\n",
        "    if paper.get(\"key_findings\"):\n",
        "        validation[\"key_findings_ok\"] = True\n",
        "    else:\n",
        "        validation[\"issues\"].append(\"No key findings extracted\")\n",
        "\n",
        "    validation[\"is_valid\"] = (\n",
        "        validation[\"text_ok\"]\n",
        "        and validation[\"sections_ok\"]\n",
        "        and validation[\"key_findings_ok\"]\n",
        "    )\n",
        "\n",
        "    return validation\n",
        "\n",
        "# ============================================\n",
        "# SINGLE PAPER ANALYSIS (FIXED)\n",
        "# ============================================\n",
        "def analyze_single_paper(paper):\n",
        "    return {\n",
        "        \"paper_id\": paper[\"paper_id\"],\n",
        "        \"key_findings\": extract_key_findings_from_sections(paper[\"sections\"])\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# CROSS-PAPER COMPARISON\n",
        "# ============================================\n",
        "def cross_paper_comparison(papers):\n",
        "    docs = [\" \".join(p[\"key_findings\"]) for p in papers if p[\"key_findings\"]]\n",
        "\n",
        "    if len(docs) < 2:\n",
        "        return None\n",
        "\n",
        "    tfidf = TfidfVectorizer(stop_words=\"english\").fit_transform(docs)\n",
        "    return cosine_similarity(tfidf)\n",
        "\n",
        "# ============================================\n",
        "# MODULE-4 RUNNER\n",
        "# ============================================\n",
        "def run_analysis():\n",
        "    extracted_files = list(Path(\"data/extracted\").glob(\"*.json\"))\n",
        "    if not extracted_files:\n",
        "        print(\"‚ùå No extracted papers found. Run Module-3 first.\")\n",
        "        return\n",
        "\n",
        "    papers = []\n",
        "    for f in extracted_files:\n",
        "        with open(f, \"r\") as fp:\n",
        "            papers.append(json.load(fp))\n",
        "\n",
        "    for p in papers:\n",
        "        p[\"key_findings\"] = extract_key_findings_from_sections(p[\"sections\"])\n",
        "\n",
        "    similarity = cross_paper_comparison(papers)\n",
        "\n",
        "    print(\"\\nKey Findings:\")\n",
        "    for p in papers:\n",
        "        print(f\"\\n{p['paper_id']}:\")\n",
        "        for f in p[\"key_findings\"]:\n",
        "            print(\"-\", f)\n",
        "\n",
        "    print(\"\\nSimilarity Matrix:\")\n",
        "    print(similarity)\n",
        "\n",
        "# ============================================\n",
        "# MAIN\n",
        "# ============================================\n",
        "if __name__ == \"__main__\":\n",
        "    run_complete_extraction()\n",
        "    run_analysis()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "XD64o7dtNgc9",
        "outputId": "d735922a-ff43-4816-90de-9c12b5145998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "MODULE 5: DATASET GENERATOR\n",
            "======================================================================\n",
            "üìÑ Found 1 extracted paper files.\n",
            "‚úÖ Loaded 1 valid papers.\n",
            "\n",
            "üìä Building dataset...\n",
            "  [1/1] Processing: Deep1DConvnetforaccurateParkinsondiseasedetectiona\n",
            "‚úÖ Dataset created: 1 rows √ó 21 columns\n",
            "\n",
            "üìå Dataset Preview:\n",
            "                                             paper_id                                                filename  year  total_characters title abstract introduction methods_text results_text conclusion_text methods datasets key_findings limitations contributions metrics  num_methods  num_datasets  num_findings  num_limitations  num_contributions\n",
            "0  Deep1DConvnetforaccurateParkinsondiseasedetectiona  Deep1DConvnetforaccurateParkinsondiseasedetectiona.pdf  None                 0                                                                                                                                                  0             0             0                0                  0\n",
            "\n",
            "üìÅ Dataset saved successfully!\n",
            "üìÇ Location: /content/data/dataset/dataset_20251224_172702\n",
            "\n",
            "üéâ MODULE 5 COMPLETED SUCCESSFULLY!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            paper_id  \\\n",
              "0  Deep1DConvnetforaccurateParkinsondiseasedetect...   \n",
              "\n",
              "                                            filename  year  total_characters  \\\n",
              "0  Deep1DConvnetforaccurateParkinsondiseasedetect...  None                 0   \n",
              "\n",
              "  title abstract introduction methods_text results_text conclusion_text  ...  \\\n",
              "0                                                                        ...   \n",
              "\n",
              "  datasets key_findings limitations contributions metrics num_methods  \\\n",
              "0                                                                   0   \n",
              "\n",
              "   num_datasets  num_findings  num_limitations  num_contributions  \n",
              "0             0             0                0                  0  \n",
              "\n",
              "[1 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-596e423a-cc81-4ab8-bb35-361bb0a9be64\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>filename</th>\n",
              "      <th>year</th>\n",
              "      <th>total_characters</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>introduction</th>\n",
              "      <th>methods_text</th>\n",
              "      <th>results_text</th>\n",
              "      <th>conclusion_text</th>\n",
              "      <th>...</th>\n",
              "      <th>datasets</th>\n",
              "      <th>key_findings</th>\n",
              "      <th>limitations</th>\n",
              "      <th>contributions</th>\n",
              "      <th>metrics</th>\n",
              "      <th>num_methods</th>\n",
              "      <th>num_datasets</th>\n",
              "      <th>num_findings</th>\n",
              "      <th>num_limitations</th>\n",
              "      <th>num_contributions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Deep1DConvnetforaccurateParkinsondiseasedetect...</td>\n",
              "      <td>Deep1DConvnetforaccurateParkinsondiseasedetect...</td>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows √ó 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-596e423a-cc81-4ab8-bb35-361bb0a9be64')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-596e423a-cc81-4ab8-bb35-361bb0a9be64 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-596e423a-cc81-4ab8-bb35-361bb0a9be64');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# ============================================================\n",
        "# MODULE 5: DATASET GENERATOR FOR RESEARCH PAPERS (FINAL FIXED)\n",
        "# ============================================================\n",
        "\n",
        "!pip install pandas openpyxl -q\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD ALL EXTRACTED PAPERS (FIXED)\n",
        "# ============================================================\n",
        "\n",
        "def load_all_extracted(data_dir=\"data/extracted\"):\n",
        "    path = Path(data_dir)\n",
        "\n",
        "    if not path.exists():\n",
        "        print(f\"‚ùå Directory not found: {data_dir}\")\n",
        "        print(\"   Please run Module 3 first.\")\n",
        "        return []\n",
        "\n",
        "    # Load all JSON files except summary/stat files\n",
        "    files = [\n",
        "        f for f in path.glob(\"*.json\")\n",
        "        if not any(x in f.name.lower() for x in [\"summary\", \"stats\"])\n",
        "    ]\n",
        "\n",
        "    if not files:\n",
        "        print(\"‚ùå No extracted paper JSON files found.\")\n",
        "        print(f\"   Checked path: {path.resolve()}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"üìÑ Found {len(files)} extracted paper files.\")\n",
        "\n",
        "    papers = []\n",
        "    for f in files:\n",
        "        try:\n",
        "            with open(f, \"r\", encoding=\"utf-8\") as fp:\n",
        "                data = json.load(fp)\n",
        "                if \"sections\" in data:\n",
        "                    papers.append(data)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Skipped invalid JSON: {f.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed reading {f.name}: {str(e)[:60]}\")\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(papers)} valid papers.\")\n",
        "    return papers\n",
        "\n",
        "# ============================================================\n",
        "# 2. TEXT CLEANING HELPERS\n",
        "# ============================================================\n",
        "\n",
        "def clean_text(t):\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    t = re.sub(r'\\s+', ' ', t)\n",
        "    return t.strip()\n",
        "\n",
        "def extract_year(paper):\n",
        "    if \"year\" in paper and paper[\"year\"]:\n",
        "        return paper[\"year\"]\n",
        "    match = re.search(r\"(19|20)\\d{2}\", paper.get(\"filename\", \"\"))\n",
        "    return int(match.group()) if match else None\n",
        "\n",
        "# ============================================================\n",
        "# 3. RULE-BASED INFO EXTRACTION\n",
        "# ============================================================\n",
        "\n",
        "def keyword_extract(text, keywords, max_items=5):\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    found = []\n",
        "    text_low = text.lower()\n",
        "    sentences = re.split(r'[.!?]', text)\n",
        "\n",
        "    for kw in keywords:\n",
        "        for sent in sentences:\n",
        "            if kw in sent.lower() and len(sent.strip()) > 25:\n",
        "                found.append(clean_text(sent)[:300])\n",
        "                if len(found) >= max_items:\n",
        "                    return found\n",
        "    return found\n",
        "\n",
        "def extract_methods(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"methods\", \"\"),\n",
        "        [\"we use\", \"our method\", \"approach\", \"technique\", \"implementation\"]\n",
        "    )\n",
        "\n",
        "def extract_datasets(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"methods\", \"\"),\n",
        "        [\"dataset\", \"benchmark\", \"data source\", \"collected data\"]\n",
        "    )\n",
        "\n",
        "def extract_findings(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"results\", \"\"),\n",
        "        [\"result\", \"significant\", \"improved\", \"outperforms\"]\n",
        "    )\n",
        "\n",
        "def extract_limitations(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"conclusion\", \"\"),\n",
        "        [\"limitation\", \"future work\", \"challenge\", \"not address\"]\n",
        "    )\n",
        "\n",
        "def extract_contributions(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"introduction\", \"\"),\n",
        "        [\"contribution\", \"we propose\", \"novel\", \"we present\"]\n",
        "    )\n",
        "\n",
        "def extract_metrics(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"methods\", \"\"),\n",
        "        [\"accuracy\", \"precision\", \"recall\", \"f1\", \"metric\"]\n",
        "    )\n",
        "\n",
        "def normalize_list(lst):\n",
        "    return \"; \".join(lst) if lst else \"\"\n",
        "\n",
        "# ============================================================\n",
        "# 4. BUILD DATASET\n",
        "# ============================================================\n",
        "\n",
        "def build_dataset(papers):\n",
        "    rows = []\n",
        "    print(\"\\nüìä Building dataset...\")\n",
        "\n",
        "    for i, p in enumerate(papers, 1):\n",
        "        sections = p.get(\"sections\", {})\n",
        "\n",
        "        print(f\"  [{i}/{len(papers)}] Processing: {p.get('paper_id', 'unknown')}\")\n",
        "\n",
        "        row = {\n",
        "            \"paper_id\": p.get(\"paper_id\", f\"paper_{i}\"),\n",
        "            \"filename\": p.get(\"filename\", \"\"),\n",
        "            \"year\": extract_year(p),\n",
        "            \"total_characters\": p.get(\"total_characters\", 0),\n",
        "\n",
        "            \"title\": clean_text(sections.get(\"title\", \"\"))[:500],\n",
        "            \"abstract\": clean_text(sections.get(\"abstract\", \"\"))[:2000],\n",
        "            \"introduction\": clean_text(sections.get(\"introduction\", \"\"))[:2000],\n",
        "            \"methods_text\": clean_text(sections.get(\"methods\", \"\"))[:2000],\n",
        "            \"results_text\": clean_text(sections.get(\"results\", \"\"))[:2000],\n",
        "            \"conclusion_text\": clean_text(sections.get(\"conclusion\", \"\"))[:2000],\n",
        "\n",
        "            \"methods\": normalize_list(extract_methods(p)),\n",
        "            \"datasets\": normalize_list(extract_datasets(p)),\n",
        "            \"key_findings\": normalize_list(extract_findings(p)),\n",
        "            \"limitations\": normalize_list(extract_limitations(p)),\n",
        "            \"contributions\": normalize_list(extract_contributions(p)),\n",
        "            \"metrics\": normalize_list(extract_metrics(p)),\n",
        "\n",
        "            \"num_methods\": len(extract_methods(p)),\n",
        "            \"num_datasets\": len(extract_datasets(p)),\n",
        "            \"num_findings\": len(extract_findings(p)),\n",
        "            \"num_limitations\": len(extract_limitations(p)),\n",
        "            \"num_contributions\": len(extract_contributions(p)),\n",
        "        }\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    print(f\"‚úÖ Dataset created: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "    return df\n",
        "\n",
        "# ============================================================\n",
        "# 5. SAVE DATASET (PROPER FOLDER STRUCTURE)\n",
        "# ============================================================\n",
        "\n",
        "def save_dataset(df, base_dir=\"data/dataset\"):\n",
        "    base = Path(base_dir)\n",
        "    base.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    folder = base / f\"dataset_{timestamp}\"\n",
        "    formats = folder / \"formats\"\n",
        "    analysis = folder / \"analysis\"\n",
        "\n",
        "    formats.mkdir(parents=True, exist_ok=True)\n",
        "    analysis.mkdir(exist_ok=True)\n",
        "\n",
        "    csv_path = formats / \"papers_dataset.csv\"\n",
        "    xlsx_path = formats / \"papers_dataset.xlsx\"\n",
        "    json_path = formats / \"papers_dataset.json\"\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_excel(xlsx_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2, force_ascii=False)\n",
        "\n",
        "    stats = {\n",
        "        \"created_at\": datetime.now().isoformat(),\n",
        "        \"total_papers\": len(df),\n",
        "        \"total_columns\": len(df.columns),\n",
        "        \"papers_with_year\": int(df[\"year\"].notna().sum()),\n",
        "        \"avg_characters\": int(df[\"total_characters\"].mean()),\n",
        "    }\n",
        "\n",
        "    with open(analysis / \"dataset_statistics.json\", \"w\") as f:\n",
        "        json.dump(stats, f, indent=2)\n",
        "\n",
        "    # backward-compatible CSV\n",
        "    df.to_csv(base / \"papers_dataset.csv\", index=False)\n",
        "\n",
        "    print(\"\\nüìÅ Dataset saved successfully!\")\n",
        "    print(f\"üìÇ Location: {folder.resolve()}\")\n",
        "\n",
        "    return folder\n",
        "\n",
        "# ============================================================\n",
        "# 6. MAIN RUNNER\n",
        "# ============================================================\n",
        "\n",
        "def generate_paper_dataset():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"MODULE 5: DATASET GENERATOR\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    papers = load_all_extracted()\n",
        "    if not papers:\n",
        "        print(\"‚ùå No papers loaded. Aborting.\")\n",
        "        return None\n",
        "\n",
        "    df = build_dataset(papers)\n",
        "\n",
        "    print(\"\\nüìå Dataset Preview:\")\n",
        "    print(df.head(3).to_string())\n",
        "\n",
        "    save_dataset(df)\n",
        "\n",
        "    print(\"\\nüéâ MODULE 5 COMPLETED SUCCESSFULLY!\")\n",
        "    return df\n",
        "\n",
        "# ============================================================\n",
        "# AUTO RUN (COLAB FRIENDLY)\n",
        "# ============================================================\n",
        "\n",
        "generate_paper_dataset()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}