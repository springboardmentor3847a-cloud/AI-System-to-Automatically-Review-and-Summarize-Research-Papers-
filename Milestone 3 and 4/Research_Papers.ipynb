{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPteDFDemZEm"
      },
      "source": [
        "AI System to Automatically Review and Summarize Research Papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LRZl39Ymanr"
      },
      "source": [
        "Module 1:Topic Input and Paper search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jKdX4_N_mgh4"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "!pip install semanticscholar python-dotenv requests -q\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from semanticscholar import SemanticScholar\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qsTv9fkcucu7"
      },
      "outputs": [],
      "source": [
        "#fallback papers(when api fails)\n",
        "FALLBACK_PAPERS = [\n",
        "    {\n",
        "        \"title\": \"Deep Learning\",\n",
        "        \"authors\": [\"LeCun\", \"Bengio\", \"Hinton\"],\n",
        "        \"year\": 2015,\n",
        "        \"paperId\": \"DL001\",\n",
        "        \"abstract\": \"Overview of deep learning...\",\n",
        "        \"citationCount\": 50000,\n",
        "        \"venue\": \"Nature\",\n",
        "        \"url\": \"https://arxiv.org/abs/1502.01852\",\n",
        "        \"pdf_url\": \"https://arxiv.org/pdf/1502.01852.pdf\",\n",
        "        \"has_pdf\": True\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Attention Is All You Need\",\n",
        "        \"authors\": [\"Vaswani\", \"Shazeer\"],\n",
        "        \"year\": 2017,\n",
        "        \"paperId\": \"DL002\",\n",
        "        \"abstract\": \"Transformer architecture...\",\n",
        "        \"citationCount\": 100000,\n",
        "        \"venue\": \"NeurIPS\",\n",
        "        \"url\": \"https://arxiv.org/abs/1706.03762\",\n",
        "        \"pdf_url\": \"https://arxiv.org/pdf/1706.03762.pdf\",\n",
        "        \"has_pdf\": True\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Machine Learning Foundations\",\n",
        "        \"authors\": [\"Mitchell\"],\n",
        "        \"year\": 1997,\n",
        "        \"paperId\": \"DL003\",\n",
        "        \"abstract\": \"Introduction to machine learning foundations...\",\n",
        "        \"citationCount\": 20000,\n",
        "        \"venue\": \"McGraw Hill\",\n",
        "        \"url\": None,\n",
        "        \"pdf_url\": None,\n",
        "        \"has_pdf\": False\n",
        "    }\n",
        "]\n",
        "# safe api initilaization\n",
        "def setup_api_key():\n",
        "    load_dotenv()\n",
        "    API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
        "\n",
        "    if not API_KEY:\n",
        "        print(\"No API key found. Running without API (fallback mode).\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        sch = SemanticScholar(api_key=API_KEY)\n",
        "        # Test request to validate key\n",
        "        sch.search_paper(\"test\", limit=1)\n",
        "        print(\"Semantic Scholar initialized with API key\")\n",
        "        return sch\n",
        "    except Exception as e:\n",
        "        print(f\"API key failed ({e}) â†’ Using fallback mode.\")\n",
        "        return None\n",
        "# Buid result dictionary\n",
        "def build_result(topic, papers):\n",
        "    return {\n",
        "        \"topic\": topic,\n",
        "        \"search_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"total_results\": len(papers),\n",
        "        \"papers_with_pdf\": sum(p[\"has_pdf\"] for p in papers),\n",
        "        \"papers\": papers\n",
        "    }\n",
        "# search papers\n",
        "def search_papers(topic, limit=20):\n",
        "    print(f\"\\n Searching for papers on topic: '{topic}'\")\n",
        "\n",
        "    sch = setup_api_key()\n",
        "\n",
        "    # If API not available â†’ fallback\n",
        "    if sch is None:\n",
        "        print(\" Using fallback sample dataset.\\n\")\n",
        "        return build_result(topic, FALLBACK_PAPERS)\n",
        "\n",
        "    try:\n",
        "        results = sch.search_paper(\n",
        "            query=topic,\n",
        "            limit=limit,\n",
        "            fields=[\"paperId\", \"title\", \"abstract\", \"year\", \"authors\",\n",
        "                    \"citationCount\", \"openAccessPdf\", \"url\", \"venue\"]\n",
        "        )\n",
        "\n",
        "        papers = []\n",
        "        for p in results:\n",
        "            papers.append({\n",
        "                \"title\": p.title,\n",
        "                \"authors\": [a[\"name\"] for a in p.authors] if p.authors else [],\n",
        "                \"year\": p.year,\n",
        "                \"paperId\": p.paperId,\n",
        "                \"abstract\": (p.abstract[:300] + \"...\") if p.abstract else \"No abstract\",\n",
        "                \"citationCount\": p.citationCount or 0,\n",
        "                \"venue\": getattr(p, \"venue\", None),\n",
        "                \"url\": p.url,\n",
        "                \"pdf_url\": p.openAccessPdf[\"url\"] if p.openAccessPdf else None,\n",
        "                \"has_pdf\": bool(p.openAccessPdf)\n",
        "            })\n",
        "\n",
        "        print(\"\\n Semantic Scholar search completed successfully!\")\n",
        "        return build_result(topic, papers)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Semantic Scholar search failed: {e}\")\n",
        "        print(\" Using fallback dataset.\\n\")\n",
        "        return build_result(topic, FALLBACK_PAPERS)\n",
        "# save search results\n",
        "def save_search_results(data):\n",
        "    os.makedirs(\"data/search_results\", exist_ok=True)\n",
        "    fname = f\"{data['topic'].replace(' ', '_')}_results.json\"\n",
        "    path = f\"data/search_results/{fname}\"\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "    print(f\"\\n Results saved to: {path}\")\n",
        "    return path\n",
        "# display results\n",
        "def display_search_results(data):\n",
        "    print(f\" SEARCH RESULTS FOR: {data['topic']}\")\n",
        "\n",
        "    print(f\"\\n Total papers found: {data['total_results']}\")\n",
        "    print(f\" Papers with PDF: {data['papers_with_pdf']}\")\n",
        "\n",
        "    print(\"\\n TOP PAPERS:\")\n",
        "\n",
        "    for i, p in enumerate(data[\"papers\"], start=1):\n",
        "        print(f\"\\n{i}. {p['title']}\")\n",
        "        print(f\"   Authors: {', '.join(p['authors'])}\")\n",
        "        print(f\"    Year: {p['year']}\")\n",
        "        print(f\"    Citations: {p['citationCount']}\")\n",
        "        print(f\"    PDF: {'YES' if p['has_pdf'] else 'NO'}\")\n",
        "#main function\n",
        "def main_search():\n",
        "    print(\" MODULE 1: TOPIC INPUT & PAPER SEARCH\")\n",
        "\n",
        "    topic = input(\"\\nEnter research topic: \").strip()\n",
        "    if not topic:\n",
        "        topic = \"machine learning\"\n",
        "\n",
        "    results = search_papers(topic)\n",
        "    path = save_search_results(results)\n",
        "    display_search_results(results)\n",
        "\n",
        "    print(\"\\n MODULE 1 COMPLETE!\")\n",
        "    print(f\" Proceed to Module 2\\n\")\n",
        "\n",
        "    return results, path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P18o-9I9A1w"
      },
      "source": [
        "Module 2:Paper selection and Pdf download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU22mcXWbp_m",
        "outputId": "51098df2-4f41-4b95-cda5-defd412c1f4c"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF -q\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "os.makedirs(\"downloads\", exist_ok=True)\n",
        "def load_search_results(path):\n",
        "    print(\"\\n Loading Module 1 results...\")\n",
        "\n",
        "    if not path or not os.path.exists(path):\n",
        "        print(f\" ERROR: Cannot read results â†’ {path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        print(\"Results loaded.\\n\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\" JSON read error: {e}\")\n",
        "        return None\n",
        "\n",
        "def rank_papers(papers):\n",
        "    return sorted(\n",
        "        papers,\n",
        "        key=lambda p: ((p.get(\"citationCount\") or 0), (p.get(\"year\") or 0)),\n",
        "        reverse=True\n",
        "    )\n",
        "def download_pdf(url, title):\n",
        "    print(f\"\\n Downloading: {title}\")\n",
        "\n",
        "    if not url:\n",
        "        print(\"    No PDF URL available.\")\n",
        "        return False, \"no_url\"\n",
        "\n",
        "    # Safe standardized filename\n",
        "    safe_title = \"\".join(char if char.isalnum() or char in \" _-\" else \"_\" for char in title)[:50]\n",
        "    filename = f\"{safe_title}_{abs(hash(url)) % 99999}.pdf\"\n",
        "    filepath = os.path.join(\"downloads\", filename)\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            print(f\"   HTTP error: {r.status_code}\")\n",
        "            return False, f\"http_{r.status_code}\"\n",
        "\n",
        "        with open(filepath, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "\n",
        "        # Validate PDF\n",
        "        try:\n",
        "            doc = fitz.open(filepath)\n",
        "            if doc.page_count == 0:\n",
        "                os.remove(filepath)\n",
        "                print(\"    Corrupted PDF.\")\n",
        "                return False, \"empty_pdf\"\n",
        "        except:\n",
        "            os.remove(filepath)\n",
        "            print(\"    Could not open PDF.\")\n",
        "            return False, \"invalid_pdf\"\n",
        "\n",
        "        print(f\"    Saved at: {filepath}\")\n",
        "        return True, filepath\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ERROR: {e}\")\n",
        "        return False, str(e)\n",
        "def ask_user_to_select(papers):\n",
        "\n",
        "    print(\"\\n AVAILABLE PAPERS WITH PDF:\")\n",
        "    for i, p in enumerate(papers, start=1):\n",
        "        print(f\"{i}. {p['title'][:60]}\")\n",
        "        print(f\"    Citations: {p['citationCount']} | Year: {p['year']}\")\n",
        "        print()\n",
        "\n",
        "    print(\"\\n Press ENTER to auto-select top papers\")\n",
        "    choice = input(\"Or enter paper numbers (comma separated): \").strip()\n",
        "\n",
        "    if choice == \"\":\n",
        "        print(\"\\n Auto-selecting top papers based on citation count.\\n\")\n",
        "        return None  # automatic mode\n",
        "\n",
        "    try:\n",
        "        indexes = [int(x.strip()) for x in choice.split(\",\")]\n",
        "        selected = [papers[i - 1] for i in indexes if 1 <= i <= len(papers)]\n",
        "        print(\"\\n User-selected papers:\")\n",
        "        return selected\n",
        "\n",
        "    except:\n",
        "        print(\"Invalid input â†’ Using automatic selection.\")\n",
        "        return None\n",
        "\n",
        "def main_module_2(results_path):\n",
        "\n",
        "    print(\" MODULE 2: PDF DOWNLOAD WITH USER OR AUTO SELECTION\")\n",
        "    data = load_search_results(results_path)\n",
        "    if not data:\n",
        "        return\n",
        "\n",
        "    papers = data.get(\"papers\", [])\n",
        "    pdf_papers = [p for p in papers if p.get(\"has_pdf\")]\n",
        "\n",
        "    print(f\"Total papers: {len(papers)}\")\n",
        "    print(f\" Papers with PDF: {len(pdf_papers)}\")\n",
        "\n",
        "    if len(pdf_papers) == 0:\n",
        "        print(\"\\nâ˜¢ No PDF papers found â†’ Using fallback PDF paper.\")\n",
        "        pdf_papers = [{\n",
        "            \"title\": \"Deep Learning (Fallback PDF)\",\n",
        "            \"pdf_url\": \"https://arxiv.org/pdf/1502.01852.pdf\",\n",
        "            \"citationCount\": 50000,\n",
        "            \"year\": 2015\n",
        "        }]\n",
        "    selected = ask_user_to_select(pdf_papers)\n",
        "    if selected is None:\n",
        "        ranked = rank_papers(pdf_papers)\n",
        "        selected = ranked[:3]  # choose top 3 papers automatically\n",
        "\n",
        "    print(\"\\n SELECTED PAPERS FOR DOWNLOAD:\")\n",
        "    for p in selected:\n",
        "        print(f\" - {p['title']} (Citations: {p.get('citationCount',0)})\")\n",
        "    print(\"\\n STARTING DOWNLOADS...\\n\")\n",
        "\n",
        "    for p in selected:\n",
        "        success, info = download_pdf(p.get(\"pdf_url\"), p[\"title\"])\n",
        "        print(f\"  RESULT: {'SUCCESS' if success else 'FAILED'} ({info})\")\n",
        "\n",
        "    print(\"\\n MODULE 2 COMPLETE!\")\n",
        "    print(\" PDFs stored in: downloads/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Nkzpshu84_",
        "outputId": "0113d292-ad03-48c5-9f6a-a1e6ead975a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " MODULE 1: TOPIC INPUT & PAPER SEARCH\n"
          ]
        }
      ],
      "source": [
        "results, path = main_search()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6O-00B2vvth",
        "outputId": "024af1e0-7d25-4c6a-f00f-afeff3b823a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " MODULE 2: PDF DOWNLOAD WITH USER OR AUTO SELECTION\n",
            "\n",
            " Loading Module 1 results...\n",
            "Results loaded.\n",
            "\n",
            "Total papers: 3\n",
            " Papers with PDF: 2\n",
            "\n",
            " AVAILABLE PAPERS WITH PDF:\n",
            "1. Deep Learning\n",
            "    Citations: 50000 | Year: 2015\n",
            "\n",
            "2. Attention Is All You Need\n",
            "    Citations: 100000 | Year: 2017\n",
            "\n",
            "\n",
            " Press ENTER to auto-select top papers\n",
            "Or enter paper numbers (comma separated): \n",
            "\n",
            " Auto-selecting top papers based on citation count.\n",
            "\n",
            "\n",
            " SELECTED PAPERS FOR DOWNLOAD:\n",
            " - Attention Is All You Need (Citations: 100000)\n",
            " - Deep Learning (Citations: 50000)\n",
            "\n",
            " STARTING DOWNLOADS...\n",
            "\n",
            "\n",
            " Downloading: Attention Is All You Need\n",
            "    Saved at: downloads/Attention Is All You Need_80141.pdf\n",
            "  RESULT: SUCCESS (downloads/Attention Is All You Need_80141.pdf)\n",
            "\n",
            " Downloading: Deep Learning\n",
            "    Saved at: downloads/Deep Learning_81224.pdf\n",
            "  RESULT: SUCCESS (downloads/Deep Learning_81224.pdf)\n",
            "\n",
            " MODULE 2 COMPLETE!\n",
            " PDFs stored in: downloads/\n"
          ]
        }
      ],
      "source": [
        "main_module_2(path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijy7G_fncuBf"
      },
      "source": [
        "Module 3:PDF Text Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-ZEGF5MWc03T"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from datetime import datetime\n",
        "\n",
        "DOWNLOAD_DIR = \"downloads\"\n",
        "EXTRACT_DIR = \"data/extracted_text\"\n",
        "\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path, max_pages=20):\n",
        "    text = \"\"\n",
        "\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            pages_to_read = min(len(doc), max_pages)\n",
        "\n",
        "            for page_num in range(pages_to_read):\n",
        "                page = doc.load_page(page_num)\n",
        "                text += page.get_text()\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Failed to extract text from {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def run_module_3():\n",
        "    print(\"MODULE 3: PDF TEXT EXTRACTION\")\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.endswith(\".pdf\")]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"No PDF files found. Run Module 2 first.\")\n",
        "        return []\n",
        "\n",
        "    extracted_files = []\n",
        "\n",
        "    for pdf in pdf_files:\n",
        "        pdf_path = os.path.join(DOWNLOAD_DIR, pdf)\n",
        "        print(f\"\\n Processing: {pdf}\")\n",
        "\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        if not text:\n",
        "            print(\" No text extracted.\")\n",
        "            continue\n",
        "\n",
        "        base_name = os.path.splitext(pdf)[0]\n",
        "        text_path = os.path.join(EXTRACT_DIR, base_name + \".txt\")\n",
        "        meta_path = os.path.join(EXTRACT_DIR, base_name + \"_meta.json\")\n",
        "\n",
        "        # Save extracted text\n",
        "        with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "\n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            \"source_pdf\": pdf,\n",
        "            \"characters_extracted\": len(text),\n",
        "            \"extraction_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "\n",
        "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(metadata, f, indent=4)\n",
        "\n",
        "        extracted_files.append(text_path)\n",
        "\n",
        "        print(f\" Text extracted and saved: {text_path}\")\n",
        "\n",
        "    print(\"\\n MODULE 3 COMPLETE!\")\n",
        "    print(f\" Extracted files saved in: {EXTRACT_DIR}\")\n",
        "\n",
        "    return extracted_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1E_HvTlVVUX",
        "outputId": "932d2894-7cf5-44e0-fa70-e48f59366ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODULE 3: PDF TEXT EXTRACTION\n",
            "\n",
            " Processing: Deep Learning_81224.pdf\n",
            " Text extracted and saved: data/extracted_text/Deep Learning_81224.txt\n",
            "\n",
            " Processing: Attention Is All You Need_80141.pdf\n",
            " Text extracted and saved: data/extracted_text/Attention Is All You Need_80141.txt\n",
            "\n",
            " MODULE 3 COMPLETE!\n",
            " Extracted files saved in: data/extracted_text\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['data/extracted_text/Deep Learning_81224.txt',\n",
              " 'data/extracted_text/Attention Is All You Need_80141.txt']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_module_3()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp80ioeaVlcK"
      },
      "source": [
        "Module 4:Paper Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IsCJAymRVitt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "EXTRACT_DIR = \"data/extracted_text\"\n",
        "SUMMARY_DIR = \"data/summaries\"\n",
        "\n",
        "os.makedirs(SUMMARY_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def simple_summary(text, max_sentences=5):\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "    return \" \".join(sentences[:max_sentences]).strip()\n",
        "\n",
        "\n",
        "def run_module_4():\n",
        "    print(\" MODULE 4: PAPER SUMMARIZATION\")\n",
        "\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        print(\" Extracted text folder missing. Run Module 3 first.\")\n",
        "        return\n",
        "\n",
        "    text_files = [f for f in os.listdir(EXTRACT_DIR) if f.endswith(\".txt\")]\n",
        "\n",
        "    print(f\" Text files found: {len(text_files)}\")\n",
        "\n",
        "    if not text_files:\n",
        "        print(\" No extracted text files found. Module 4 cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    for txt in text_files:\n",
        "        txt_path = os.path.join(EXTRACT_DIR, txt)\n",
        "        print(f\"\\n Generating summary for: {txt}\")\n",
        "\n",
        "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        if not text.strip():\n",
        "            print(\" Empty text. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        summary = simple_summary(text)\n",
        "\n",
        "        base = os.path.splitext(txt)[0]\n",
        "        summary_path = os.path.join(SUMMARY_DIR, base + \"_summary.txt\")\n",
        "        meta_path = os.path.join(SUMMARY_DIR, base + \"_summary_meta.json\")\n",
        "\n",
        "        with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(summary)\n",
        "\n",
        "        meta = {\n",
        "            \"source_text\": txt,\n",
        "            \"summary_method\": \"extractive_first_n_sentences\",\n",
        "            \"sentences_used\": 5,\n",
        "            \"generated_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "\n",
        "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, indent=4)\n",
        "\n",
        "        print(f\" Summary saved to: {summary_path}\")\n",
        "\n",
        "    print(\"\\n MODULE 4 COMPLETE!\")\n",
        "    print(f\" Summaries stored in: {SUMMARY_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AObbIA6NV-jG",
        "outputId": "26d4eb07-09fb-480f-cae4-7ad88bbd3060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " MODULE 4: PAPER SUMMARIZATION\n",
            " Text files found: 2\n",
            "\n",
            " Generating summary for: Attention Is All You Need_80141.txt\n",
            " Summary saved to: data/summaries/Attention Is All You Need_80141_summary.txt\n",
            "\n",
            " Generating summary for: Deep Learning_81224.txt\n",
            " Summary saved to: data/summaries/Deep Learning_81224_summary.txt\n",
            "\n",
            " MODULE 4 COMPLETE!\n",
            " Summaries stored in: data/summaries\n"
          ]
        }
      ],
      "source": [
        "run_module_4()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdVSRMOVWiEx"
      },
      "source": [
        "Module 5:Knowledge Indexing & Question and answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ixToZxfUWmnn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "SUMMARY_DIR = \"data/summaries\"\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def build_index():\n",
        "    print(\"\\n Building knowledge index from summaries...\")\n",
        "\n",
        "    if not os.path.exists(SUMMARY_DIR):\n",
        "        print(\" Summary folder not found. Run Module 4 first.\")\n",
        "        return None, None\n",
        "\n",
        "    summary_files = [f for f in os.listdir(SUMMARY_DIR) if f.endswith(\"_summary.txt\")]\n",
        "\n",
        "    print(f\" Summary files found: {len(summary_files)}\")\n",
        "\n",
        "    if not summary_files:\n",
        "        print(\" No summaries available.\")\n",
        "        return None, None\n",
        "\n",
        "    index = defaultdict(set)\n",
        "    documents = {}\n",
        "\n",
        "    for file in summary_files:\n",
        "        path = os.path.join(SUMMARY_DIR, file)\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        documents[file] = text\n",
        "\n",
        "        for word in tokenize(text):\n",
        "            index[word].add(file)\n",
        "\n",
        "    print(\" Knowledge index created.\")\n",
        "    return index, documents\n",
        "\n",
        "\n",
        "def answer_question(index, documents):\n",
        "    print(\" MODULE 5: QUESTION ANSWERING\")\n",
        "\n",
        "    query = input(\"\\nAsk a question (or press ENTER to exit): \").strip()\n",
        "    if not query:\n",
        "        print(\" Exiting Q&A module.\")\n",
        "        return\n",
        "\n",
        "    query_words = tokenize(query)\n",
        "    matched_docs = defaultdict(int)\n",
        "\n",
        "    for word in query_words:\n",
        "        for doc in index.get(word, []):\n",
        "            matched_docs[doc] += 1\n",
        "\n",
        "    if not matched_docs:\n",
        "        print(\"\\n No relevant information found in summaries.\")\n",
        "        return\n",
        "\n",
        "    # Rank documents by keyword matches\n",
        "    ranked = sorted(matched_docs.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\"\\n Relevant answers found:\\n\")\n",
        "\n",
        "    for doc, score in ranked[:3]:\n",
        "        print(f\" Source: {doc} (match score: {score})\")\n",
        "        print(documents[doc])\n",
        "        print()\n",
        "\n",
        "\n",
        "def run_module_5():\n",
        "    print(\" MODULE 5: KNOWLEDGE INDEXING & Q&A\")\n",
        "\n",
        "    index, documents = build_index()\n",
        "\n",
        "    if index is None:\n",
        "        return\n",
        "\n",
        "    while True:\n",
        "        answer_question(index, documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ZxcOaSNqgv",
        "outputId": "1112defc-c623-45bc-993f-3eb749ffb5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " MODULE 5: KNOWLEDGE INDEXING & Q&A\n",
            "\n",
            " Building knowledge index from summaries...\n",
            " Summary files found: 2\n",
            " Knowledge index created.\n",
            " MODULE 5: QUESTION ANSWERING\n",
            "\n",
            "Ask a question (or press ENTER to exit): What is ML\n",
            "\n",
            " Relevant answers found:\n",
            "\n",
            " Source: Attention Is All You Need_80141_summary.txt (match score: 1)\n",
            "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswaniâˆ— Google Brain avaswani@google.com Noam Shazeerâˆ— Google Brain noam@google.com Niki Parmarâˆ— Google Research nikip@google.com Jakob Uszkoreitâˆ— Google Research usz@google.com Llion Jonesâˆ— Google Research llion@google.com Aidan N. Gomezâˆ—â€  University of Toronto aidan@cs.toronto.edu Åukasz Kaiserâˆ— Google Brain lukaszkaiser@google.com Illia Polosukhinâˆ—â€¡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
            "\n",
            " MODULE 5: QUESTION ANSWERING\n"
          ]
        }
      ],
      "source": [
        "run_module_5()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-D16zNHXaIf"
      },
      "source": [
        "MODULE 6: TOPIC CLUSTERING & INSIGHTS GENERATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHowOzX1XgbK"
      },
      "source": [
        "1)Read summaries from Module 4\n",
        "\n",
        "2)Convert text into TF-IDF vectors\n",
        "\n",
        "3)Apply K-Means clustering\n",
        "\n",
        "4)Display top keywords per cluster\n",
        "\n",
        "5)Assign each paper to a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5NLcFzUYMFX"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MODULE 6: TOPIC CLUSTERING & INSIGHTS\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Directories\n",
        "SUMMARY_DIR = \"data/summaries\"\n",
        "CLUSTER_DIR = \"data/clusters\"\n",
        "\n",
        "os.makedirs(CLUSTER_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Load summaries generated in Module 4\n",
        "# ------------------------------------------------------------\n",
        "def load_summaries(max_files=5, max_chars=3000):\n",
        "    \"\"\"\n",
        "    Loads a limited number of summaries for faster clustering.\n",
        "    \"\"\"\n",
        "    summaries = []\n",
        "    filenames = []\n",
        "\n",
        "    print(\"\\nðŸ“‚ Loading summaries...\")\n",
        "\n",
        "    if not os.path.exists(SUMMARY_DIR):\n",
        "        print(\"âŒ Summary folder not found. Run Module 4 first.\")\n",
        "        return [], []\n",
        "\n",
        "    files = [f for f in os.listdir(SUMMARY_DIR) if f.endswith(\"_summary.txt\")]\n",
        "    files = files[:max_files]  # limit for speed\n",
        "\n",
        "    print(f\"ðŸ“„ Using {len(files)} summaries for clustering\")\n",
        "\n",
        "    for file in files:\n",
        "        path = os.path.join(SUMMARY_DIR, file)\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().strip()[:max_chars]  # limit text size\n",
        "            if text:\n",
        "                summaries.append(text)\n",
        "                filenames.append(file)\n",
        "\n",
        "    return summaries, filenames\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Perform TF-IDF + KMeans clustering\n",
        "# ------------------------------------------------------------\n",
        "def run_clustering(texts, n_clusters):\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words=\"english\",\n",
        "        max_features=300\n",
        "    )\n",
        "\n",
        "    print(\"ðŸ”„ Vectorizing text...\")\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "\n",
        "    print(\"ðŸ”„ Running KMeans clustering...\")\n",
        "    model = KMeans(\n",
        "        n_clusters=n_clusters,\n",
        "        random_state=42,\n",
        "        n_init=5\n",
        "    )\n",
        "\n",
        "    labels = model.fit_predict(X)\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "    return labels, model, terms\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Display top keywords per cluster\n",
        "# ------------------------------------------------------------\n",
        "def show_cluster_keywords(model, terms, top_n=6):\n",
        "    print(\"\\nðŸ“Œ CLUSTER KEYWORDS\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for i, centroid in enumerate(model.cluster_centers_):\n",
        "        top_terms = centroid.argsort()[-top_n:][::-1]\n",
        "        keywords = [terms[j] for j in top_terms]\n",
        "        print(f\"Cluster {i}: {', '.join(keywords)}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Save clustering results\n",
        "# ------------------------------------------------------------\n",
        "def save_cluster_results(filenames, labels):\n",
        "    cluster_map = {}\n",
        "\n",
        "    for file, label in zip(filenames, labels):\n",
        "        cluster_map[file] = int(label)\n",
        "\n",
        "    path = os.path.join(CLUSTER_DIR, \"paper_clusters.json\")\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cluster_map, f, indent=4)\n",
        "\n",
        "    print(f\"\\nðŸ’¾ Cluster mapping saved to: {path}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Main execution function\n",
        "# ------------------------------------------------------------\n",
        "def run_module_6():\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸ“˜ MODULE 6: TOPIC CLUSTERING & INSIGHTS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    texts, filenames = load_summaries()\n",
        "\n",
        "    if len(texts) < 2:\n",
        "        print(\"âš  Not enough summaries to perform clustering.\")\n",
        "        return\n",
        "\n",
        "    n_clusters = min(2, len(texts))\n",
        "\n",
        "    labels, model, terms = run_clustering(texts, n_clusters)\n",
        "\n",
        "    print(\"\\nðŸ“„ PAPER â†’ CLUSTER ASSIGNMENT\")\n",
        "    for file, label in zip(filenames, labels):\n",
        "        print(f\" - {file} â†’ Cluster {label}\")\n",
        "\n",
        "    show_cluster_keywords(model, terms)\n",
        "    save_cluster_results(filenames, labels)\n",
        "\n",
        "    print(\"\\n MODULE 6 COMPLETE!\")\n",
        "    print(\"Cluster data saved in: data/clusters/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxwSchhChZMk"
      },
      "outputs": [],
      "source": [
        "run_module_6()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
