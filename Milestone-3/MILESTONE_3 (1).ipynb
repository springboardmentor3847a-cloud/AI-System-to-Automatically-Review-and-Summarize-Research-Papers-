{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzurlu1p8lJq"
      },
      "outputs": [],
      "source": [
        "!pip install semanticscholar python-dotenv requests -q\n",
        "from openai import OpenAI\n",
        "from typing import List\n",
        "import json\n",
        "import os\n",
        "from semanticscholar import SemanticScholar\n",
        "from dotenv import load_dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTYxNWyy8994",
        "outputId": "634c0d51-fc18-4643-f39f-9ab6bd7a612f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter research topic: parkinson\n",
            "\n",
            "üîé Searching for: parkinson\n",
            "\n",
            "üîç Searching Europe PMC...\n",
            "‚û° Europe PMC PDF results: 5\n",
            "\n",
            "üîç Searching arXiv...\n",
            "‚û° arXiv PDF results: 20\n",
            "\n",
            "üìä TOTAL PDF papers found: 25\n",
            "\n",
            "=========== PDF AVAILABLE PAPERS ===========\n",
            "\n",
            "1. Correction: Metabolic modeling of microbial communities in the chicken ceca reveals a landscape of competition and co-operation.\n",
            "   Authors: \n",
            "   Year: 2025  | Source: Europe PMC\n",
            "   PDF: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12720474/pdf/\n",
            "\n",
            "2. Stigmatization and bias in interpreting lichen sclerosus risk factors.\n",
            "   Authors: \n",
            "   Year: 2025  | Source: Europe PMC\n",
            "   PDF: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12685388/pdf/\n",
            "\n",
            "3. A bibliometric analysis of non-coding RNAs in Parkinson disease: Research hotspots and emerging trends (2013-2022).\n",
            "   Authors: \n",
            "   Year: 2025  | Source: Europe PMC\n",
            "   PDF: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12727337/pdf/\n",
            "\n",
            "4. Recruitment of Latinx Older Adults With Parkinson Disease for a Remote Physical Activity Intervention Trial\n",
            "   Authors: \n",
            "   Year: 2025  | Source: Europe PMC\n",
            "   PDF: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12759663/pdf/\n",
            "\n",
            "5. Revisiting the 2015 MDS diagnostic criteria for Parkinson disease: insights from autopsy-confirmed cases.\n",
            "   Authors: \n",
            "   Year: 2025  | Source: Europe PMC\n",
            "   PDF: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12748991/pdf/\n",
            "\n",
            "6. Deep 1D-Convnet for accurate Parkinson disease detection and severity prediction from gait\n",
            "   Authors: Imanne El Maachi, Guillaume-Alexandre Bilodeau, Wassim Bouachir\n",
            "   Year: 2019  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/1910.11509v4.pdf\n",
            "\n",
            "7. A Three-groups Non-local Model for Combining Heterogeneous Data Sources to Identify Genes Associated with Parkinson's Disease\n",
            "   Authors: Troy P. Wixson, Benjamin A. Shaby, Daisy L. Philtron, International Parkinson Disease Genomics Consortium\n",
            "   Year: 2024  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/2406.05262v1.pdf\n",
            "\n",
            "8. Optimizing baryon acoustic oscillation surveys II: curvature, redshifts, and external datasets\n",
            "   Authors: David Parkinson, Martin Kunz, Andrew R. Liddle, Bruce A. Bassett\n",
            "   Year: 2009  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/0905.3410v2.pdf\n",
            "\n",
            "9. Detection of 16 Gamma-Ray Pulsars Through Blind Frequency Searches Using the Fermi LAT\n",
            "   Authors: The Fermi-LAT Collaboration\n",
            "   Year: 2010  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/1009.0748v1.pdf\n",
            "\n",
            "10. Identification of a DAGLB Mutation in a Non-Chinese Patient with Parkinson's Disease\n",
            "   Authors: Christelle Tesson, Mohamed Sofiane Bouchetara, M√©lanie Ferrien, Suzanne Lesage\n",
            "   Year: 2023  | Source: arXiv\n",
            "   PDF: http://arxiv.org/pdf/2310.12521v1.pdf\n",
            "\n",
            "\n",
            "üíæ Saved results ‚Üí data/search_results/paper_search_results_parkinson.json\n",
            "\n",
            "üéâ MODULE 1 COMPLETE ‚Äî PDF-ONLY MODE ENABLED ‚úî\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# PDF-ONLY RESULTS (SemanticScholar-Compatible Output)\n",
        "\n",
        "!pip install requests feedparser -q\n",
        "from openai import OpenAI\n",
        "from typing import List\n",
        "import requests\n",
        "import feedparser\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "\n",
        "key = OpenAI(api_key=\"\")\n",
        "\n",
        "\n",
        "def clean(x):\n",
        "    return x.replace(\"\\n\", \" \").strip() if isinstance(x, str) else x\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 1. Europe PMC ‚Üí only keep papers with PDF\n",
        "# ====================================================\n",
        "\n",
        "def search_europe_pmc(query, limit=20):\n",
        "    print(\"\\nüîç Searching Europe PMC...\")\n",
        "\n",
        "    url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query={query}&format=json&pageSize={limit}\"\n",
        "    try:\n",
        "        data = requests.get(url, timeout=10).json()\n",
        "        results = data.get(\"resultList\", {}).get(\"result\", [])\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    for p in results:\n",
        "        pdf_url = None\n",
        "\n",
        "        # FIXED: Improved PDF detection\n",
        "        # Check open access status first\n",
        "        if p.get(\"isOpenAccess\") == \"Y\":\n",
        "            # Try PMC ID based URL\n",
        "            pmcid = p.get(\"pmcid\")\n",
        "            if pmcid:\n",
        "                pdf_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/\"\n",
        "\n",
        "            # If no PMC ID, check fullTextUrlList\n",
        "            if not pdf_url and \"fullTextUrlList\" in p:\n",
        "                urls = p[\"fullTextUrlList\"].get(\"fullTextUrl\", [])\n",
        "                for u in urls:\n",
        "                    doc_style = u.get(\"documentStyle\", \"\").lower()\n",
        "                    site = u.get(\"site\", \"\").lower()\n",
        "                    availability = u.get(\"availability\", \"\")\n",
        "\n",
        "                    # Look for PDF specifically\n",
        "                    if \"pdf\" in doc_style or \"pdf\" in site:\n",
        "                        pdf_url = u.get(\"url\")\n",
        "                        break\n",
        "                    # Or free full text\n",
        "                    elif availability == \"Free\" and u.get(\"url\"):\n",
        "                        potential_url = u.get(\"url\")\n",
        "                        if potential_url and \"pdf\" in potential_url.lower():\n",
        "                            pdf_url = potential_url\n",
        "                            break\n",
        "\n",
        "        # üö´ Skip papers without PDFs (FIXED)\n",
        "        if not pdf_url:\n",
        "            continue\n",
        "        papers.append({\n",
        "            \"title\": clean(p.get(\"title\", \"\")),\n",
        "            \"authors\": [a.get(\"fullName\", \"\") for a in p.get(\"authorList\", {}).get(\"author\", [])],\n",
        "            \"year\": int(p[\"pubYear\"]) if p.get(\"pubYear\") else None,\n",
        "            \"paperId\": p.get(\"id\", \"\"),\n",
        "            \"abstract\": clean(p.get(\"abstractText\", \"\")),\n",
        "            \"citationCount\": p.get(\"citedByCount\", 0),\n",
        "            \"venue\": p.get(\"journalTitle\", \"\"),\n",
        "            \"url\": p.get(\"pubmedUrl\", \"\"),\n",
        "            \"pdf_url\": pdf_url,\n",
        "            \"has_pdf\": True,\n",
        "            \"source\": \"Europe PMC\"\n",
        "        })\n",
        "\n",
        "    print(f\"‚û° Europe PMC PDF results: {len(papers)}\")\n",
        "    return papers\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 2. arXiv ‚Üí ALL papers have PDF\n",
        "# ====================================================\n",
        "def search_arxiv(query, limit=20):\n",
        "    print(\"\\nüîç Searching arXiv...\")\n",
        "\n",
        "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={limit}\"\n",
        "    try:\n",
        "        feed = feedparser.parse(url)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        pdf_url = entry.id.replace(\"abs\", \"pdf\") + \".pdf\"\n",
        "\n",
        "        papers.append({\n",
        "            \"title\": clean(entry.title),\n",
        "            \"authors\": [a.name for a in entry.authors],\n",
        "            \"year\": int(entry.published[:4]),\n",
        "            \"paperId\": entry.id,\n",
        "            \"abstract\": clean(entry.summary),\n",
        "            \"citationCount\": 0,\n",
        "            \"venue\": \"arXiv\",\n",
        "            \"url\": entry.link,\n",
        "            \"pdf_url\": pdf_url,\n",
        "            \"has_pdf\": True,\n",
        "            \"source\": \"arXiv\"\n",
        "        })\n",
        "\n",
        "    print(f\"‚û° arXiv PDF results: {len(papers)}\")\n",
        "    return papers\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 3. Combine + PDF Only\n",
        "# ====================================================\n",
        "def search_papers(query, limit=20):\n",
        "    print(f\"\\nüîé Searching for: {query}\")\n",
        "\n",
        "    pmc_papers = search_europe_pmc(query, limit)\n",
        "    arxiv_papers = search_arxiv(query, limit)\n",
        "\n",
        "    all_papers = pmc_papers + arxiv_papers\n",
        "\n",
        "    print(f\"\\nüìä TOTAL PDF papers found: {len(all_papers)}\")\n",
        "    return all_papers\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# SAVE RESULTS\n",
        "# ====================================================\n",
        "def save_search_results(papers, topic):\n",
        "    os.makedirs(\"data/search_results\", exist_ok=True)\n",
        "\n",
        "    safe_topic = \"\".join(c for c in topic if c.isalnum() or c == \" \").replace(\" \", \"_\")\n",
        "    filename = f\"paper_search_results_{safe_topic}.json\"\n",
        "\n",
        "    path = f\"data/search_results/{filename}\"\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"topic\": topic,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"papers\": papers\n",
        "        }, f, indent=4)\n",
        "\n",
        "    print(f\"\\nüíæ Saved results ‚Üí {path}\")\n",
        "    return path\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# DISPLAY RESULTS (PDF ONLY)\n",
        "# ====================================================\n",
        "def display_results(papers, limit=10):\n",
        "    print(\"\\n=========== PDF AVAILABLE PAPERS ===========\\n\")\n",
        "    for i, p in enumerate(papers[:limit], 1):\n",
        "        print(f\"{i}. {p['title']}\")\n",
        "        print(f\"   Authors: {', '.join(p['authors'][:4])}\")\n",
        "        print(f\"   Year: {p['year']}  | Source: {p['source']}\")\n",
        "        print(f\"   PDF: {p['pdf_url']}\\n\")\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# MAIN FUNCTION\n",
        "# ====================================================\n",
        "def main_search():\n",
        "    query = input(\"Enter research topic: \").strip()\n",
        "\n",
        "    papers = search_papers(query, limit=20)\n",
        "    display_results(papers)\n",
        "    save_search_results(papers, query)\n",
        "\n",
        "    print(\"\\nüéâ MODULE 1 COMPLETE ‚Äî PDF-ONLY MODE ENABLED ‚úî\")\n",
        "    return papers\n",
        "\n",
        "\n",
        "# Run module\n",
        "if __name__ == \"__main__\":\n",
        "    main_search()\n",
        "def main_search():\n",
        "    query = input(\"Enter research topic: \").strip()\n",
        "\n",
        "    papers = search_papers(query, limit=20)\n",
        "\n",
        "    # ADDED: Check for empty results\n",
        "    if not papers:\n",
        "        print(\"\\n‚ùå No papers with PDFs found. Try a different search query.\")\n",
        "        return []\n",
        "\n",
        "    display_results(papers)\n",
        "    save_search_results(papers, query)\n",
        "\n",
        "    print(\"\\nüéâ MODULE 1 COMPLETE ‚Äì PDF-ONLY MODE ENABLED ‚úì\")\n",
        "    return papers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MveIANO29FZc",
        "outputId": "689dfc53-591a-4057-e66c-4a08aff6856a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Correction: Metabolic modeling of microbial communities in the chicken ceca reve ‚Äî PDF downloaded: False\n",
            "[2] Stigmatization and bias in interpreting lichen sclerosus risk factors. ‚Äî PDF downloaded: False\n",
            "[3] A bibliometric analysis of non-coding RNAs in Parkinson disease: Research hotspo ‚Äî PDF downloaded: False\n",
            "[4] Recruitment of Latinx Older Adults With Parkinson Disease for a Remote Physical  ‚Äî PDF downloaded: False\n",
            "[5] Revisiting the 2015 MDS diagnostic criteria for Parkinson disease: insights from ‚Äî PDF downloaded: False\n",
            "[6] Deep 1D-Convnet for accurate Parkinson disease detection and severity prediction ‚Äî PDF downloaded: True\n",
            "[7] A Three-groups Non-local Model for Combining Heterogeneous Data Sources to Ident ‚Äî PDF downloaded: True\n",
            "[8] Optimizing baryon acoustic oscillation surveys II: curvature, redshifts, and ext ‚Äî PDF downloaded: True\n",
            "[9] Detection of 16 Gamma-Ray Pulsars Through Blind Frequency Searches Using the Fer ‚Äî PDF downloaded: True\n",
            "[10] Identification of a DAGLB Mutation in a Non-Chinese Patient with Parkinson's Dis ‚Äî PDF downloaded: True\n",
            "Done. Report: downloads/download_report_20260108_153410.json\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# MODULE 2: ROBUST PDF DOWNLOADER (multi-source + page-scan)\n",
        "# ============================================\n",
        "# Requirements:\n",
        "# pip install requests python-docx PyMuPDF beautifulsoup4 -q\n",
        "\n",
        "import os, json, requests, fitz\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from docx import Document\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0 Safari/537.36\"\n",
        "}\n",
        "REQUEST_TIMEOUT = 20  # seconds\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Helper: safe HEAD check if URL looks like a PDF\n",
        "# ------------------------------\n",
        "def try_head_is_pdf(url):\n",
        "    \"\"\"Return True if HEAD/GET indicates content-type PDF (handles redirects).\"\"\"\n",
        "    if not url:\n",
        "        return False\n",
        "    try:\n",
        "        # Try HEAD first (faster). Some servers disallow HEAD; fall back to GET with stream=True.\n",
        "        r = requests.head(url, headers=HEADERS, allow_redirects=True, timeout=REQUEST_TIMEOUT)\n",
        "        if r.status_code == 200 and \"pdf\" in r.headers.get(\"content-type\", \"\").lower():\n",
        "            return True\n",
        "        # fallback to GET but do not download body\n",
        "        r = requests.get(url, headers=HEADERS, stream=True, allow_redirects=True, timeout=REQUEST_TIMEOUT)\n",
        "        ct = r.headers.get(\"content-type\", \"\").lower()\n",
        "        r.close()\n",
        "        if r.status_code == 200 and \"pdf\" in ct:\n",
        "            return True\n",
        "    except Exception:\n",
        "        return False\n",
        "    return False\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Helper: try download PDF safely\n",
        "# ------------------------------\n",
        "def download_pdf(url, out_path):\n",
        "    \"\"\"Download PDF and verify by opening with PyMuPDF; return True if ok.\"\"\"\n",
        "    try:\n",
        "        r = requests.get(url, headers=HEADERS, stream=True, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
        "        if r.status_code != 200:\n",
        "            return False\n",
        "        content_type = r.headers.get(\"content-type\", \"\").lower()\n",
        "        # quickly accept if content-type contains pdf OR url endswith .pdf\n",
        "        if \"pdf\" not in content_type and not url.lower().split('?')[0].endswith('.pdf'):\n",
        "            # still possible a PDF served with wrong header; try small peek\n",
        "            chunk = r.raw.read(4)\n",
        "            r.raw.close()\n",
        "            r.close()\n",
        "            if not chunk.startswith(b'%PDF'):\n",
        "                return False\n",
        "            # re-download properly\n",
        "            r = requests.get(url, headers=HEADERS, stream=True, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
        "        # write to file\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "        # verify with pymupdf\n",
        "        fitz.open(str(out_path)).close()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        # cleanup if file partially written\n",
        "        try:\n",
        "            if out_path.exists():\n",
        "                out_path.unlink()\n",
        "        except:\n",
        "            pass\n",
        "        return False\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Normalize / quick transforms for canonical PDF urls\n",
        "# ------------------------------\n",
        "def normalize_known_pdf_url(url):\n",
        "    \"\"\"Handle arXiv and PMC quick conversions, else return None.\"\"\"\n",
        "    if not url:\n",
        "        return None\n",
        "    u = url.strip()\n",
        "    if \"arxiv.org/abs/\" in u:\n",
        "        return u.replace(\"/abs/\", \"/pdf/\") + \".pdf\"\n",
        "    if \"ncbi.nlm.nih.gov/pmc/articles\" in u:\n",
        "        return u.rstrip(\"/\") + \"/pdf/\"\n",
        "    # add other simple heuristics if desired\n",
        "    return None\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Extract candidate PDF links from landing page HTML\n",
        "# ------------------------------\n",
        "def extract_pdf_from_page(page_url):\n",
        "    \"\"\"Fetch landing page HTML and extract candidate PDF URLs using multiple heuristics.\"\"\"\n",
        "    candidates = []\n",
        "    try:\n",
        "        r = requests.get(page_url, headers=HEADERS, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
        "        if r.status_code != 200:\n",
        "            return candidates\n",
        "        text = r.text\n",
        "        soup = BeautifulSoup(text, \"html.parser\")\n",
        "\n",
        "        # 1) meta tags frequently used by publishers (citation_pdf_url)\n",
        "        #    e.g., <meta name=\"citation_pdf_url\" content=\"...pdf\">\n",
        "        for meta in soup.find_all(\"meta\"):\n",
        "            name = (meta.get(\"name\") or \"\").lower()\n",
        "            prop = (meta.get(\"property\") or \"\").lower()\n",
        "            content = meta.get(\"content\") or meta.get(\"value\") or \"\"\n",
        "            if content and (\"citation_pdf_url\" in name or \"citation_pdf_url\" in prop):\n",
        "                candidates.append(urljoin(page_url, content.strip()))\n",
        "\n",
        "        # 2) link rel=alternate type=application/pdf\n",
        "        for link in soup.find_all(\"link\"):\n",
        "            if (link.get(\"type\") or \"\").lower() == \"application/pdf\" or (link.get(\"rel\") and \"alternate\" in link.get(\"rel\")):\n",
        "                href = link.get(\"href\")\n",
        "                if href:\n",
        "                    candidates.append(urljoin(page_url, href.strip()))\n",
        "\n",
        "        # 3) anchor tags pointing to .pdf\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            href = a[\"href\"].strip()\n",
        "            if \".pdf\" in href.lower() or href.lower().endswith(\"/pdf\"):\n",
        "                candidates.append(urljoin(page_url, href))\n",
        "\n",
        "        # 4) some publishers provide direct \"download\" links or query endpoints\n",
        "        #    collect links that contain 'download' and later check via HEAD\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            href = a[\"href\"].lower()\n",
        "            if \"download\" in href or \"fulltext\" in href:\n",
        "                candidates.append(urljoin(page_url, a[\"href\"]))\n",
        "\n",
        "        # Deduplicate while preserving order\n",
        "        seen = set()\n",
        "        out = []\n",
        "        for c in candidates:\n",
        "            if c not in seen:\n",
        "                seen.add(c)\n",
        "                out.append(c)\n",
        "        return out\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Main search for a candidate PDF for a given article URL (pdf_url from search results)\n",
        "# ------------------------------\n",
        "def find_candidate_pdf(article_url):\n",
        "    \"\"\"Return a list of candidate PDF URLs in order of priority.\"\"\"\n",
        "    candidates = []\n",
        "\n",
        "    # 1) If direct link already points to a PDF via header check, keep it\n",
        "    if article_url:\n",
        "        if try_head_is_pdf(article_url):\n",
        "            candidates.append(article_url)\n",
        "\n",
        "    # 2) Known quick normalizers (arXiv / PMC)\n",
        "    norm = normalize_known_pdf_url(article_url)\n",
        "    if norm:\n",
        "        if try_head_is_pdf(norm):\n",
        "            candidates.append(norm)\n",
        "        else:\n",
        "            # still include as candidate for download attempt (some arXiv links may require .pdf appended)\n",
        "            candidates.append(norm)\n",
        "\n",
        "    # 3) Try to parse landing page and extract PDF links\n",
        "    page_candidates = extract_pdf_from_page(article_url) if article_url else []\n",
        "    for pc in page_candidates:\n",
        "        if pc not in candidates:\n",
        "            candidates.append(pc)\n",
        "\n",
        "    # 4) Also try Semantic Scholar PDF link pattern (optional):\n",
        "    #    If article_url is a doi or known id, semantic scholar often hosts PDF. We'll try a search URL:\n",
        "    #    e.g., https://www.semanticscholar.org/paper/<paper-slug>\n",
        "    #    (Note: this is best-effort; may or may not help)\n",
        "    # (Skipping automated semantic scholar search to avoid scraping policies ‚Äî rely on page parsing instead.)\n",
        "\n",
        "    return candidates\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Create DOC fallback (metadata only)\n",
        "# ------------------------------\n",
        "def create_doc(paper, path):\n",
        "    doc = Document()\n",
        "    doc.add_heading(paper.get(\"title\", \"Untitled\"), level=1)\n",
        "    doc.add_paragraph(\"Authors: \" + \", \".join(paper.get(\"authors\", [])))\n",
        "    doc.add_paragraph(\"Year: \" + str(paper.get(\"year\", \"\")))\n",
        "    doc.add_paragraph(\"Source: \" + str(paper.get(\"source\", \"\")))\n",
        "    doc.add_heading(\"Abstract\", level=2)\n",
        "    doc.add_paragraph(paper.get(\"abstract\") or \"Not available\")\n",
        "    doc.add_heading(\"Original URL\", level=2)\n",
        "    doc.add_paragraph(paper.get(\"pdf_url\") or \"\")\n",
        "    doc.save(path)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Main pipeline for multiple papers\n",
        "# ------------------------------\n",
        "def download_papers_hybrid(papers, max_count=10, output_dir=\"downloads\"):\n",
        "    out_dir = Path(output_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    summary = []\n",
        "\n",
        "    for idx, p in enumerate(papers[:max_count], start=1):\n",
        "        title = p.get(\"title\", f\"paper_{idx}\")\n",
        "        safe = \"\".join(ch for ch in title if ch.isalnum())[:40] or f\"paper_{idx}\"\n",
        "        folder = out_dir / f\"{idx}_{safe}\"\n",
        "        folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        meta = {\n",
        "            \"paper_id\": folder.name,\n",
        "            \"title\": title,\n",
        "            \"source\": p.get(\"source\"),\n",
        "            \"original_url\": p.get(\"pdf_url\"),\n",
        "            \"candidates\": [],\n",
        "            \"downloaded\": False,\n",
        "            \"downloaded_path\": None,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # find candidates\n",
        "        candidates = find_candidate_pdf(p.get(\"pdf_url\"))\n",
        "        meta[\"candidates\"] = candidates\n",
        "\n",
        "        # try candidates in order\n",
        "        pdf_file_path = folder / \"paper.pdf\"\n",
        "        for cand in candidates:\n",
        "            if not cand:\n",
        "                continue\n",
        "            ok = download_pdf(cand, pdf_file_path)\n",
        "            if ok:\n",
        "                meta[\"downloaded\"] = True\n",
        "                meta[\"downloaded_path\"] = str(pdf_file_path)\n",
        "                meta[\"download_candidate\"] = cand\n",
        "                break\n",
        "\n",
        "        # Fallback: if nothing downloaded, keep metadata and write a DOC with abstract\n",
        "        doc_path = folder / \"paper.docx\"\n",
        "        try:\n",
        "            create_doc(p, doc_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Save metadata\n",
        "        with open(folder / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        summary.append(meta)\n",
        "        print(f\"[{idx}] {title[:80]} ‚Äî PDF downloaded: {meta['downloaded']}\")\n",
        "\n",
        "    # Save overall report\n",
        "    report_path = out_dir / f\"download_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return summary, str(report_path)\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Example usage (integrate with your Module 1 output)\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Expecting data/search_results/latest.json with {\"topic\":..., \"papers\":[{...}, ...]}\n",
        "    srch = None\n",
        "    try:\n",
        "        files = sorted(Path(\"data/search_results\").glob(\"*.json\"), key=lambda f: f.stat().st_mtime)\n",
        "        if not files:\n",
        "            print(\"‚ùå No search_results JSON files found. Run module 1 first.\")\n",
        "            raise SystemExit\n",
        "        with open(files[-1], \"r\", encoding=\"utf-8\") as f:\n",
        "            srch = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(\"Error loading search results:\", e)\n",
        "        raise SystemExit\n",
        "\n",
        "    papers = srch.get(\"papers\", [])\n",
        "    summary, report = download_papers_hybrid(papers, max_count=10, output_dir=\"downloads\")\n",
        "    print(\"Done. Report:\", report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ-Li5Jb9LNH",
        "outputId": "29897ece-2a52-4f5e-8690-33f9d5842ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== MODULE 3: PDF EXTRACTION ===\n",
            " Found 5 PDF files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|‚ñà‚ñà        | 1/5 [00:00<00:00,  5.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Processing: paper.pdf\n",
            "üìÑ Processing: paper.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:00<00:00,  7.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Processing: paper.pdf\n",
            "üìÑ Processing: paper.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  8.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Processing: paper.pdf\n",
            " Saved 5 extracted papers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# MODULE 3: ROBUST PDF TEXT EXTRACTION (FIXED VERSION)\n",
        "# ============================================================\n",
        "\n",
        "!pip install pymupdf tqdm -q\n",
        "\n",
        "import fitz\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "# ============================================================\n",
        "# SIMPLE CLEANER\n",
        "# ============================================================\n",
        "\n",
        "def clean_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "# ============================================================\n",
        "# FIND PDF FILES (NO FILTERING)\n",
        "# ============================================================\n",
        "\n",
        "def find_pdfs(root=\"downloads\"):\n",
        "    root = Path(root)\n",
        "    if not root.exists():\n",
        "        print(\" downloads folder not found\")\n",
        "        return []\n",
        "\n",
        "    pdfs = list(root.rglob(\"*.pdf\"))\n",
        "    print(f\" Found {len(pdfs)} PDF files\")\n",
        "    return pdfs\n",
        "\n",
        "# ============================================================\n",
        "# TEXT EXTRACTION (SIMPLE & RELIABLE)\n",
        "# ============================================================\n",
        "\n",
        "def extract_text(pdf_path):\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "\n",
        "        for page in doc:\n",
        "            text += page.get_text(\"text\") + \"\\n\"\n",
        "\n",
        "        doc.close()\n",
        "        return clean_text(text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed: {pdf_path.name} ‚Üí {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# ============================================================\n",
        "# BASIC SECTION SPLITTER (LENIENT)\n",
        "# ============================================================\n",
        "\n",
        "def split_sections(text):\n",
        "    sections = {\n",
        "        \"title\": \"\",\n",
        "        \"abstract\": \"\",\n",
        "        \"methods\": \"\",\n",
        "        \"results\": \"\",\n",
        "        \"conclusion\": \"\",\n",
        "        \"full_text\": text[:20000]\n",
        "    }\n",
        "\n",
        "    lower = text.lower()\n",
        "\n",
        "    def grab(start, end=None):\n",
        "        s = lower.find(start)\n",
        "        if s == -1:\n",
        "            return \"\"\n",
        "        s += len(start)\n",
        "        e = lower.find(end, s) if end else s + 3000\n",
        "        return text[s:e].strip()\n",
        "\n",
        "    sections[\"abstract\"] = grab(\"abstract\", \"introduction\")\n",
        "    sections[\"methods\"] = grab(\"methods\", \"results\")\n",
        "    sections[\"results\"] = grab(\"results\", \"conclusion\")\n",
        "    sections[\"conclusion\"] = grab(\"conclusion\")\n",
        "\n",
        "    # title guess\n",
        "    for line in text.split(\"\\n\")[:5]:\n",
        "        if 20 < len(line) < 150:\n",
        "            sections[\"title\"] = line.strip()\n",
        "            break\n",
        "\n",
        "    return sections\n",
        "\n",
        "# ============================================================\n",
        "# PROCESS SINGLE PDF\n",
        "# ============================================================\n",
        "\n",
        "def process_pdf(pdf):\n",
        "    print(f\"üìÑ Processing: {pdf.name}\")\n",
        "\n",
        "    text = extract_text(pdf)\n",
        "    if not text:\n",
        "        print(\" No text extracted\")\n",
        "        return None\n",
        "\n",
        "    sections = split_sections(text)\n",
        "\n",
        "    return {\n",
        "        \"paper_id\": pdf.stem,\n",
        "        \"filename\": pdf.name,\n",
        "        \"extracted_at\": datetime.now().isoformat(),\n",
        "        \"text_length\": len(text),\n",
        "        \"sections\": sections,\n",
        "        \"status\": \"extracted\"\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# SAVE OUTPUT\n",
        "# ============================================================\n",
        "\n",
        "def save_results(results):\n",
        "    out = Path(\"data/extracted\")\n",
        "    out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for r in results:\n",
        "        with open(out / f\"{r['paper_id']}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(r, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\" Saved {len(results)} extracted papers\")\n",
        "\n",
        "# ============================================================\n",
        "# RUN MODULE 3\n",
        "# ============================================================\n",
        "\n",
        "def run_module_3(max_papers=5):\n",
        "    print(\"\\n=== MODULE 3: PDF EXTRACTION ===\")\n",
        "\n",
        "    pdfs = find_pdfs()\n",
        "    pdfs = pdfs[:max_papers]\n",
        "\n",
        "    results = []\n",
        "    for pdf in tqdm(pdfs):\n",
        "        r = process_pdf(pdf)\n",
        "        if r:\n",
        "            results.append(r)\n",
        "\n",
        "    if results:\n",
        "        save_results(results)\n",
        "    else:\n",
        "        print(\" No PDFs could be processed\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================\n",
        "# AUTO RUN\n",
        "# ============================================================\n",
        "\n",
        "results = run_module_3()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAtNeFhr9QZI",
        "outputId": "8b52f09c-93a5-4bed-c9bc-3af865f5b32c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GEMINI API KEY INITIALISED\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "# üîë SET GEMINI API KEY (FLASH FREE)\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyA5WZeo2_KvziOWk0Doq3fu6VgZ15QBxFc\"\n",
        "\n",
        "print(\"‚úÖ GEMINI API KEY INITIALISED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y google-generativeai\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzDhcKp9cDJ0",
        "outputId": "1129f368-50d5-42e7-ce12-16780ee4ef8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: google-generativeai 0.3.2\n",
            "Uninstalling google-generativeai-0.3.2:\n",
            "  Successfully uninstalled google-generativeai-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"import os\"\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyBbv3izkdoCge_Edsnf98A2CJ3jwar_L_o\"\n",
        "\n"
      ],
      "metadata": {
        "id": "VVkHEsp7hAoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y google-generativeai\n",
        "!pip install -U google-generativeai scikit-learn matplotlib seaborn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TbFrzNvsYzBT",
        "outputId": "225c5e28-a86e-4483-e4df-f0715c4d0c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: google-generativeai 0.8.6\n",
            "Uninstalling google-generativeai-0.8.6:\n",
            "  Successfully uninstalled google-generativeai-0.8.6\n",
            "Collecting google-generativeai\n",
            "  Using cached google_generativeai-0.8.6-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.8)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.25.8)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.62.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
            "Using cached google_generativeai-0.8.6-py3-none-any.whl (155 kB)\n",
            "Installing collected packages: google-generativeai\n",
            "Successfully installed google-generativeai-0.8.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "e8b665cc2a5341818f4ea5bbacd0c66e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-cloud-aiplatform\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3V7bN6pd625Z",
        "outputId": "def2b9ec-b2cb-40a9-b8c3-05701ef3055e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.12/dist-packages (1.130.0)\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.132.0-py2.py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.28.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.47.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (4.25.8)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (25.0)\n",
            "Requirement already satisfied: google-cloud-storage<4.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (3.7.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (3.38.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.15.0)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.1.2)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.57.0)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.12.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (4.15.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.62.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.3)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<4.0.0,>=1.32.0->google-cloud-aiplatform) (1.7.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.12.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from shapely<3.0.0->google-cloud-aiplatform) (2.0.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.5.0)\n",
            "Downloading google_cloud_aiplatform-1.132.0-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-aiplatform\n",
            "  Attempting uninstall: google-cloud-aiplatform\n",
            "    Found existing installation: google-cloud-aiplatform 1.130.0\n",
            "    Uninstalling google-cloud-aiplatform-1.130.0:\n",
            "      Successfully uninstalled google-cloud-aiplatform-1.130.0\n",
            "Successfully installed google-cloud-aiplatform-1.132.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "626d740afa12446fa6bc74f50a7e00bd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n"
      ],
      "metadata": {
        "id": "oBnLi3po7BFL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U google-cloud-aiplatform\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K314C8c8oWK",
        "outputId": "d121207e-f0ad-4cca-ab90-1db0b2d8fc7d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.12/dist-packages (1.132.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.28.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.47.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (4.25.8)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (25.0)\n",
            "Requirement already satisfied: google-cloud-storage<4.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (3.7.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (3.38.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.15.0)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.1.2)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.37.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (1.57.0)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (2.12.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (4.15.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (1.62.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform) (0.14.3)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<4.0.0,>=1.32.0->google-cloud-aiplatform) (1.7.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (4.12.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from shapely<3.0.0->google-cloud-aiplatform) (2.0.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.37.0->google-cloud-aiplatform) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.45.0->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "# init vertex\n",
        "vertexai.init(\n",
        "    project=\"gemini-test-483718\",\n",
        "    location=\"us-central1\"\n",
        ")\n",
        "\n",
        "# EXACT model from your screenshot\n",
        "model = GenerativeModel(\"gemini-2.0-flash-lite-001\")\n",
        "\n",
        "response = model.generate_content(\"Say hello in one line.\")\n",
        "print(response.text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtGX4-KU8thJ",
        "outputId": "eba23be3-a898-4c60-b326-252cad7ca66f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "vertexai.init(\n",
        "    project=\"gemini-test-483718\",\n",
        "    location=\"us-central1\"\n",
        ")\n",
        "\n",
        "model = GenerativeModel(\"gemini-2.0-flash-lite-001\")\n"
      ],
      "metadata": {
        "id": "SEOX0NdGAwVj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_extracted_papers():\n",
        "    path = Path(\"data/extracted\")\n",
        "    papers = []\n",
        "\n",
        "    for f in path.glob(\"*.json\"):\n",
        "        with open(f, \"r\", encoding=\"utf-8\") as fp:\n",
        "            data = json.load(fp)\n",
        "            if \"sections\" in data:\n",
        "                papers.append(data)\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(papers)} extracted papers\")\n",
        "    return papers\n",
        "\n",
        "papers = load_extracted_papers()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvihLt61AyJb",
        "outputId": "229c4e6b-fdb5-4a71-caf0-990a8f3800be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 1 extracted papers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_summary(papers):\n",
        "    summary = \"\"\n",
        "    for p in papers:\n",
        "        summary += f\"\"\"\n",
        "TITLE: {p['sections'].get('title','')}\n",
        "METHODS: {p['sections'].get('methods','')[:500]}\n",
        "RESULTS: {p['sections'].get('results','')[:500]}\n",
        "\"\"\"\n",
        "    return summary\n",
        "\n",
        "base_text = build_summary(papers)\n"
      ],
      "metadata": {
        "id": "iAuuZYmEA39I"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract = model.generate_content(\n",
        "    \"Write an academic ABSTRACT based on the following papers:\\n\" + base_text\n",
        ").text\n",
        "\n",
        "methods = model.generate_content(\n",
        "    \"Write a METHODS COMPARISON section comparing the approaches:\\n\" + base_text\n",
        ").text\n",
        "\n",
        "results = model.generate_content(\n",
        "    \"Write a RESULTS SYNTHESIS highlighting trends and findings:\\n\" + base_text\n",
        ").text\n",
        "\n",
        "print(\"\\n===== ABSTRACT =====\\n\", abstract)\n",
        "print(\"\\n===== METHODS =====\\n\", methods)\n",
        "print(\"\\n===== RESULTS =====\\n\", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6zDT-F6A9Ce",
        "outputId": "1ab68161-f666-460c-acfc-0afca6d4e6c6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ABSTRACT =====\n",
            " Here's an abstract based on the provided information, focusing on the study's cohort and methodology:\n",
            "\n",
            "**Abstract:**\n",
            "\n",
            "This study investigates the genetic architecture of Parkinson's disease (PD) through an analysis of a large cohort of patients. The research utilized whole-exome sequencing (WES) data from a cohort of 683 index PD patients, primarily characterized by early-onset disease, assembled by the French and Mediterranean Parkinson‚Äôs Disease Genetics Study group (FMPD cohort). Prior to the current analysis, known mutations in a panel of Parkinsonism-associated genes (listed in Supplementary Table 4) had been excluded through WES and multiplex ligation-dependent probe amplification. Furthermore, expansions in the SCA2 and SCA17 genes were excluded using ExpansionHunter. This approach aims to provide a comprehensive genetic characterization of PD by focusing on a well-defined cohort and implementing rigorous pre-screening of known genetic contributors.\n",
            "\n",
            "\n",
            "===== METHODS =====\n",
            " ## METHODS COMPARISON\n",
            "\n",
            "This study utilizes a robust approach for investigating genetic factors in Parkinson's Disease (PD) within the French and Mediterranean Parkinson's Disease Genetics Study group (FMPD cohort). The core methodology centers on a comprehensive analysis of 683 index PD patients, leveraging whole-exome sequencing (WES) data. This approach is contrasted with potential alternative methods and strengths/weaknesses are outlined below:\n",
            "\n",
            "**1. Whole-Exome Sequencing (WES) vs. Targeted Sequencing/Candidate Gene Approach:**\n",
            "\n",
            "*   **Approach Used:** WES is employed, enabling a broad, unbiased scan of the protein-coding regions of the genome. This approach allows for the identification of novel genetic variants potentially contributing to early-onset PD, particularly in the absence of prior biases toward known PD genes.\n",
            "*   **Alternative Approaches:**\n",
            "    *   **Targeted Sequencing:** Focusing on a pre-defined set of genes known or suspected to be involved in PD. This approach is more cost-effective and provides deeper coverage of the targeted regions.\n",
            "    *   **Candidate Gene Approach:** Selecting specific genes based on existing literature or biological plausibility. This approach is suitable when focusing on a limited number of genes.\n",
            "*   **Comparison:**\n",
            "    *   **WES Advantages:** Higher discovery potential, capable of identifying unexpected genetic contributors to PD. Ideal for research seeking novel genetic associations.\n",
            "    *   **WES Disadvantages:** More expensive and computationally intensive than targeted approaches. May generate \"incidental findings\" (variants of uncertain significance in other genes).\n",
            "    *   **Targeted/Candidate Advantages:** Cost-effective, efficient for confirming known associations.\n",
            "    *   **Targeted/Candidate Disadvantages:** Limited by the genes selected for analysis, potentially missing novel genes.\n",
            "\n",
            "**2.  Mutation Exclusion Methods (WES & MLPA):**\n",
            "\n",
            "*   **Approach Used:** Mutations in known Parkinsonism-associated genes (listed in Supplementary Table 4) were *excluded* using WES, coupled with multiplex ligation-dependent probe amplification (MLPA) for copy number variation (CNV) detection. This strategy reduces the risk of confounding effects from known causative genes and allows for an investigation of novel genetic markers.\n",
            "*   **Alternative Approaches:**\n",
            "    *   **Direct Sanger Sequencing:** Confirming variants detected via WES, usually for more expensive approaches.\n",
            "    *   **Next-Generation Sequencing (NGS) of specific genes:** NGS of individual genes to confirm.\n",
            "*   **Comparison:**\n",
            "    *   **WES/MLPA Advantages:** efficient and comprehensive way to rule out known causal variants, allows you to focus on other genetic factors.\n",
            "    *   **Sanger/NGS of Specific Genes Advantages:** Highly accurate, suitable for follow-up validation.\n",
            "    *   **MLPA Disadvantages:** May have limitations in detection of small deletions/insertions.\n",
            "\n",
            "**3.  Repeat Expansion Analysis (ExpansionHunter):**\n",
            "\n",
            "*   **Approach Used:** ExpansionHunter was utilized to exclude expansions in SCA2 and SCA17 genes.\n",
            "*   **Alternative Approaches:**\n",
            "    *   **PCR-based methods with fragment analysis:**  A less automated, but common approach\n",
            "    *   **Long-read sequencing:** More accurate but more costly.\n",
            "*   **Comparison:**\n",
            "    *   **ExpansionHunter Advantages:** Efficient for detecting and sizing of repeat expansions.\n",
            "    *   **PCR/Fragment Analysis Advantages:** can be readily done with conventional equipment.\n",
            "    *   **Long-read sequencing advantages:** Highest accuracy.\n",
            "    *   **ExpansionHunter/PCR/Fragment Analysis Disadvantages:** Inherent difficulty with accurate sizing of long repeats.\n",
            "\n",
            "**4. Patient Characteristics and Cohort Composition:**\n",
            "\n",
            "*   **Approach Used:** The cohort consists of 683 PD patients. This study emphasizes early-onset PD patients, which might provide more powerful insights into genetic underpinnings.\n",
            "*   **Alternative Approaches:**\n",
            "    *   **Targeted recruitment:** Focus on specific clinical subgroups, such as patients with specific symptoms or family history.\n",
            "    *   **Prospective study:** Longitudinal design to observe disease progression.\n",
            "*   **Comparison:**\n",
            "    *   **Early-onset Cohort Advantages:** Higher likelihood of strong genetic effects due to the age of onset, less likely to be confounded by environmental factors.\n",
            "    *   **General PD cohort Advantage:** Higher sample size/more generalizable conclusions.\n",
            "    *   **Targeted subgroup Advantages:** Potentially increased power to detect genetic effects related to specific phenotypes\n",
            "    *   **Prospective study Advantages:** Can directly observe disease onset and progression.\n",
            "    *   **Disadvantages:** recruitment challenges; need to control for genetic variability/demographic factors.\n",
            "\n",
            "**In summary,** the study employs a comprehensive and targeted approach. This method is optimized for the discovery of novel genetic variants in early-onset PD by:  1) using WES to cover a large region of the genome, 2) Exclude known mutations and expansions, and 3)  focusing on early-onset PD. While WES has some cost and analytical complexities, it is well-suited to the research goal of uncovering new genetic associations.\n",
            "\n",
            "\n",
            "===== RESULTS =====\n",
            " ## RESULTS SYNTHESIS: French and Mediterranean Parkinson's Disease Cohort - Preliminary Findings\n",
            "\n",
            "**Overview:** This study analyzed whole-exome sequencing (WES) data from a cohort of 683 Parkinson's Disease (PD) patients, primarily characterized by early-onset PD. The cohort, assembled by the French and Mediterranean Parkinson‚Äôs Disease Genetics Study group (FMPD cohort), excluded known mutations in Parkinsonism-associated genes (Supplementary Table 4) and expansions in SCA2 and SCA17 through prior testing. This indicates the study is focused on identifying novel genetic variants or contributions in PD.\n",
            "\n",
            "**Key Findings & Trends (Based on Available Information):**\n",
            "\n",
            "*   **Focus on Novel Genetic Drivers:** The exclusion of previously known genetic causes of Parkinson's Disease indicates a primary objective of identifying new or less understood genetic factors contributing to PD, particularly in the early-onset subgroup.\n",
            "*   **WES as a Primary Method:** The use of whole-exome sequencing (WES) suggests a comprehensive investigation of the protein-coding regions of the genome to uncover potential disease-causing or modifying variants.\n",
            "*   **Cohort Characteristics & Implications:** The emphasis on early-onset PD implies a targeted investigation into a population where genetic factors are often more influential.\n",
            "*   **Exclusion of Known Mechanisms:** The careful exclusion of known causes (e.g., specific mutations, expansions) strongly indicates a search for less common, possibly more subtle genetic mechanisms, or perhaps additive effects.\n",
            "\n",
            "**Inferences & Expected Outcomes (Based on Methodology):**\n",
            "\n",
            "*   **Identification of New Candidate Genes:**  The primary outcome will likely be the identification of novel genetic variants, genes, or pathways implicated in early-onset PD that were previously unknown or poorly understood.\n",
            "*   **Variant Prioritization:** Identification of new variants would suggest that variants with a higher frequency in the PD cohort compared to control populations might be a focus.\n",
            "*   **Insights into Disease Mechanisms:** The study may contribute to understanding the underlying mechanisms of PD and potentially identify new therapeutic targets by revealing how the novel identified variants contribute to the disease.\n",
            "*   **Subgroup Analysis:** Further sub-analysis of the identified genetic variants might identify patients with specific clinical features associated with those variants.\n",
            "\n",
            "**Limitations (Based on Available Information):**\n",
            "\n",
            "*   **Lack of Detailed Results:** The provided summary does not include specific results (e.g., identified genes, variant frequencies, associations). The information given mainly focuses on the methodology.\n",
            "*   **Limited Scope:** The summary does not provide any information regarding any other clinical data related to the study.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio python-docx -q\n"
      ],
      "metadata": {
        "id": "S6dXPAj0C8Es"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from docx import Document\n",
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "jWr5XHRzDBcY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_report(\n",
        "    abstract,\n",
        "    methods,\n",
        "    results,\n",
        "    cross_analysis,\n",
        "    references\n",
        "):\n",
        "    output_dir = Path(\"data/final_report\")\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    file_path = output_dir / \"final_research_report.docx\"\n",
        "\n",
        "    doc = Document()\n",
        "    doc.add_heading(\"Final Research Report\", level=1)\n",
        "\n",
        "    doc.add_heading(\"Abstract\", level=2)\n",
        "    doc.add_paragraph(abstract)\n",
        "\n",
        "    doc.add_heading(\"Cross-Paper Analysis\", level=2)\n",
        "    doc.add_paragraph(cross_analysis)\n",
        "\n",
        "    doc.add_heading(\"Methods Comparison\", level=2)\n",
        "    doc.add_paragraph(methods)\n",
        "\n",
        "    doc.add_heading(\"Results Synthesis\", level=2)\n",
        "    doc.add_paragraph(results)\n",
        "\n",
        "    doc.add_heading(\"References\", level=2)\n",
        "    doc.add_paragraph(references)\n",
        "\n",
        "    doc.save(file_path)\n",
        "\n",
        "    return f\"‚úÖ Final report generated at:\\n{file_path.resolve()}\"\n"
      ],
      "metadata": {
        "id": "u76qdPavDPF0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================\n",
        "# # MODULE 5: REVIEW + FINAL REPORT UI (GEMINI)\n",
        "# # ============================================\n",
        "\n",
        "# !pip install gradio python-docx -q\n",
        "\n",
        "# import gradio as gr\n",
        "# import google.generativeai as genai\n",
        "# import os\n",
        "# from docx import Document\n",
        "# from pathlib import Path\n",
        "# from datetime import datetime\n",
        "\n",
        "# # ============================================\n",
        "# # GEMINI CONFIG\n",
        "# # ============================================\n",
        "\n",
        "# genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "# model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# def call_gemini(prompt: str) -> str:\n",
        "#     response = model.generate_content(prompt)\n",
        "#     return response.text.strip()\n",
        "\n",
        "# # ============================================\n",
        "# # CRITIQUE FUNCTION\n",
        "# # ============================================\n",
        "\n",
        "# def critique_text(abstract, methods, results):\n",
        "#     prompt = f\"\"\"\n",
        "# You are an academic reviewer.\n",
        "\n",
        "# Critically evaluate the following sections.\n",
        "\n",
        "# Provide:\n",
        "# - Strengths\n",
        "# - Weaknesses\n",
        "# - Improvement Suggestions\n",
        "\n",
        "# ABSTRACT:\n",
        "# {abstract}\n",
        "\n",
        "# METHODS:\n",
        "# {methods}\n",
        "\n",
        "# RESULTS:\n",
        "# {results}\n",
        "# \"\"\"\n",
        "#     return call_gemini(prompt)\n",
        "\n",
        "# # ============================================\n",
        "# # FINAL REPORT GENERATION\n",
        "# # ============================================\n",
        "\n",
        "# def generate_final_report(abstract, methods, results):\n",
        "#     output_dir = Path(\"data/final_report\")\n",
        "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#     filename = f\"Final_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx\"\n",
        "#     file_path = output_dir / filename\n",
        "\n",
        "#     # ---- Create DOCX ----\n",
        "#     doc = Document()\n",
        "#     doc.add_heading(\"Final Research Report\", level=1)\n",
        "\n",
        "#     doc.add_heading(\"Abstract\", level=2)\n",
        "#     doc.add_paragraph(abstract)\n",
        "\n",
        "#     doc.add_heading(\"Methods Comparison\", level=2)\n",
        "#     doc.add_paragraph(methods)\n",
        "\n",
        "#     doc.add_heading(\"Results Synthesis\", level=2)\n",
        "#     doc.add_paragraph(results)\n",
        "\n",
        "#     doc.save(file_path)\n",
        "\n",
        "#     return str(file_path)\n",
        "\n",
        "# # ============================================\n",
        "# # GRADIO UI\n",
        "# # ============================================\n",
        "\n",
        "# with gr.Blocks(title=\"Academic Paper Review & Report Generator\") as demo:\n",
        "\n",
        "#     gr.Markdown(\"## üìò Automated Academic Writing & Review System (Gemini)\")\n",
        "\n",
        "#     abstract_box = gr.Textbox(lines=8, label=\"Abstract\")\n",
        "#     methods_box = gr.Textbox(lines=8, label=\"Methods Comparison\")\n",
        "#     results_box = gr.Textbox(lines=8, label=\"Results Synthesis\")\n",
        "\n",
        "#     critique_btn = gr.Button(\"üîç Critique Sections\")\n",
        "#     generate_btn = gr.Button(\"üìÑ Generate Final Report\")\n",
        "\n",
        "#     critique_output = gr.Textbox(lines=10, label=\"Reviewer Feedback\")\n",
        "#     download_link = gr.File(label=\"Download Final Report\")\n",
        "\n",
        "#     critique_btn.click(\n",
        "#         critique_text,\n",
        "#         inputs=[abstract_box, methods_box, results_box],\n",
        "#         outputs=critique_output\n",
        "#     )\n",
        "\n",
        "#     generate_btn.click(\n",
        "#         generate_final_report,\n",
        "#         inputs=[abstract_box, methods_box, results_box],\n",
        "#         outputs=download_link\n",
        "#     )\n",
        "\n",
        "# demo.launch()\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# MODULE 5: REVIEW + FINAL REPORT UI (GEMINI)\n",
        "# ============================================\n",
        "\n",
        "!pip install gradio python-docx -q\n",
        "\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from docx import Document\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# ============================================\n",
        "# GEMINI CONFIG\n",
        "# ============================================\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "def call_gemini(prompt: str) -> str:\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "# ============================================\n",
        "# CRITIQUE FUNCTION\n",
        "# ============================================\n",
        "\n",
        "def critique_text(abstract, methods, results):\n",
        "    prompt = f\"\"\"\n",
        "You are an academic reviewer.\n",
        "\n",
        "Critically evaluate the following sections.\n",
        "\n",
        "Provide:\n",
        "- Strengths\n",
        "- Weaknesses\n",
        "- Improvement Suggestions\n",
        "\n",
        "ABSTRACT:\n",
        "{abstract}\n",
        "\n",
        "METHODS:\n",
        "{methods}\n",
        "\n",
        "RESULTS:\n",
        "{results}\n",
        "\"\"\"\n",
        "    return call_gemini(prompt)\n",
        "\n",
        "# ============================================\n",
        "# FINAL REPORT GENERATION\n",
        "# ============================================\n",
        "\n",
        "def generate_final_report(abstract, methods, results):\n",
        "    output_dir = Path(\"data/final_report\")\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    filename = f\"Final_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx\"\n",
        "    file_path = output_dir / filename\n",
        "\n",
        "    # ---- Create DOCX ----\n",
        "    doc = Document()\n",
        "    doc.add_heading(\"Final Research Report\", level=1)\n",
        "\n",
        "    doc.add_heading(\"Abstract\", level=2)\n",
        "    doc.add_paragraph(abstract)\n",
        "\n",
        "    doc.add_heading(\"Methods Comparison\", level=2)\n",
        "    doc.add_paragraph(methods)\n",
        "\n",
        "    doc.add_heading(\"Results Synthesis\", level=2)\n",
        "    doc.add_paragraph(results)\n",
        "\n",
        "    doc.save(file_path)\n",
        "\n",
        "    return str(file_path)\n",
        "\n",
        "# ============================================\n",
        "# GRADIO UI\n",
        "# ============================================\n",
        "\n",
        "with gr.Blocks(title=\"Academic Paper Review & Report Generator\") as demo:\n",
        "\n",
        "    gr.Markdown(\"## üìò Automated Academic Writing & Review System (Gemini)\")\n",
        "\n",
        "    abstract_box = gr.Textbox(lines=8, label=\"Abstract\")\n",
        "    methods_box = gr.Textbox(lines=8, label=\"Methods Comparison\")\n",
        "    results_box = gr.Textbox(lines=8, label=\"Results Synthesis\")\n",
        "\n",
        "    critique_btn = gr.Button(\"üîç Critique Sections\")\n",
        "    generate_btn = gr.Button(\"üìÑ Generate Final Report\")\n",
        "\n",
        "    critique_output = gr.Textbox(lines=10, label=\"Reviewer Feedback\")\n",
        "    download_link = gr.File(label=\"Download Final Report\")\n",
        "\n",
        "    critique_btn.click(\n",
        "        critique_text,\n",
        "        inputs=[abstract_box, methods_box, results_box],\n",
        "        outputs=critique_output\n",
        "    )\n",
        "\n",
        "    generate_btn.click(\n",
        "        generate_final_report,\n",
        "        inputs=[abstract_box, methods_box, results_box],\n",
        "        outputs=download_link\n",
        "    )\n",
        "\n",
        "demo.launch()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "cSzlF69_Fb8Z",
        "outputId": "440149a2-8a50-4d70-bac9-84fa7a193211"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://50f1e29ff7411067ab.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://50f1e29ff7411067ab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MODULE 5: DATASET GENERATOR FOR RESEARCH PAPERS (FINAL FIXED)\n",
        "# ============================================================\n",
        "\n",
        "!pip install pandas openpyxl -q\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD ALL EXTRACTED PAPERS (FIXED)\n",
        "# ============================================================\n",
        "\n",
        "def load_all_extracted(data_dir=\"data/extracted\"):\n",
        "    path = Path(data_dir)\n",
        "\n",
        "    if not path.exists():\n",
        "        print(f\"‚ùå Directory not found: {data_dir}\")\n",
        "        print(\"   Please run Module 3 first.\")\n",
        "        return []\n",
        "\n",
        "    # Load all JSON files except summary/stat files\n",
        "    files = [\n",
        "        f for f in path.glob(\"*.json\")\n",
        "        if not any(x in f.name.lower() for x in [\"summary\", \"stats\"])\n",
        "    ]\n",
        "\n",
        "    if not files:\n",
        "        print(\"‚ùå No extracted paper JSON files found.\")\n",
        "        print(f\"   Checked path: {path.resolve()}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"üìÑ Found {len(files)} extracted paper files.\")\n",
        "\n",
        "    papers = []\n",
        "    for f in files:\n",
        "        try:\n",
        "            with open(f, \"r\", encoding=\"utf-8\") as fp:\n",
        "                data = json.load(fp)\n",
        "                if \"sections\" in data:\n",
        "                    papers.append(data)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Skipped invalid JSON: {f.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed reading {f.name}: {str(e)[:60]}\")\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(papers)} valid papers.\")\n",
        "    return papers\n",
        "\n",
        "# ============================================================\n",
        "# 2. TEXT CLEANING HELPERS\n",
        "# ============================================================\n",
        "\n",
        "def clean_text(t):\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    t = re.sub(r'\\s+', ' ', t)\n",
        "    return t.strip()\n",
        "\n",
        "def extract_year(paper):\n",
        "    if \"year\" in paper and paper[\"year\"]:\n",
        "        return paper[\"year\"]\n",
        "    match = re.search(r\"(19|20)\\d{2}\", paper.get(\"filename\", \"\"))\n",
        "    return int(match.group()) if match else None\n",
        "\n",
        "# ============================================================\n",
        "# 3. RULE-BASED INFO EXTRACTION\n",
        "# ============================================================\n",
        "\n",
        "def keyword_extract(text, keywords, max_items=5):\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    found = []\n",
        "    text_low = text.lower()\n",
        "    sentences = re.split(r'[.!?]', text)\n",
        "\n",
        "    for kw in keywords:\n",
        "        for sent in sentences:\n",
        "            if kw in sent.lower() and len(sent.strip()) > 25:\n",
        "                found.append(clean_text(sent)[:300])\n",
        "                if len(found) >= max_items:\n",
        "                    return found\n",
        "    return found\n",
        "\n",
        "def extract_methods(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"methods\", \"\"),\n",
        "        [\"we use\", \"our method\", \"approach\", \"technique\", \"implementation\"]\n",
        "    )\n",
        "\n",
        "def extract_datasets(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"methods\", \"\"),\n",
        "        [\"dataset\", \"benchmark\", \"data source\", \"collected data\"]\n",
        "    )\n",
        "\n",
        "def extract_findings(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"results\", \"\"),\n",
        "        [\"result\", \"significant\", \"improved\", \"outperforms\"]\n",
        "    )\n",
        "\n",
        "def extract_limitations(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"conclusion\", \"\"),\n",
        "        [\"limitation\", \"future work\", \"challenge\", \"not address\"]\n",
        "    )\n",
        "\n",
        "def extract_contributions(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"introduction\", \"\"),\n",
        "        [\"contribution\", \"we propose\", \"novel\", \"we present\"]\n",
        "    )\n",
        "\n",
        "def extract_metrics(p):\n",
        "    return keyword_extract(\n",
        "        p.get(\"sections\", {}).get(\"methods\", \"\"),\n",
        "        [\"accuracy\", \"precision\", \"recall\", \"f1\", \"metric\"]\n",
        "    )\n",
        "\n",
        "def normalize_list(lst):\n",
        "    return \"; \".join(lst) if lst else \"\"\n",
        "\n",
        "# ============================================================\n",
        "# 4. BUILD DATASET\n",
        "# ============================================================\n",
        "\n",
        "def build_dataset(papers):\n",
        "    rows = []\n",
        "    print(\"\\nüìä Building dataset...\")\n",
        "\n",
        "    for i, p in enumerate(papers, 1):\n",
        "        sections = p.get(\"sections\", {})\n",
        "\n",
        "        print(f\"  [{i}/{len(papers)}] Processing: {p.get('paper_id', 'unknown')}\")\n",
        "\n",
        "        row = {\n",
        "            \"paper_id\": p.get(\"paper_id\", f\"paper_{i}\"),\n",
        "            \"filename\": p.get(\"filename\", \"\"),\n",
        "            \"year\": extract_year(p),\n",
        "            \"total_characters\": p.get(\"total_characters\", 0),\n",
        "\n",
        "            \"title\": clean_text(sections.get(\"title\", \"\"))[:500],\n",
        "            \"abstract\": clean_text(sections.get(\"abstract\", \"\"))[:2000],\n",
        "            \"introduction\": clean_text(sections.get(\"introduction\", \"\"))[:2000],\n",
        "            \"methods_text\": clean_text(sections.get(\"methods\", \"\"))[:2000],\n",
        "            \"results_text\": clean_text(sections.get(\"results\", \"\"))[:2000],\n",
        "            \"conclusion_text\": clean_text(sections.get(\"conclusion\", \"\"))[:2000],\n",
        "\n",
        "            \"methods\": normalize_list(extract_methods(p)),\n",
        "            \"datasets\": normalize_list(extract_datasets(p)),\n",
        "            \"key_findings\": normalize_list(extract_findings(p)),\n",
        "            \"limitations\": normalize_list(extract_limitations(p)),\n",
        "            \"contributions\": normalize_list(extract_contributions(p)),\n",
        "            \"metrics\": normalize_list(extract_metrics(p)),\n",
        "\n",
        "            \"num_methods\": len(extract_methods(p)),\n",
        "            \"num_datasets\": len(extract_datasets(p)),\n",
        "            \"num_findings\": len(extract_findings(p)),\n",
        "            \"num_limitations\": len(extract_limitations(p)),\n",
        "            \"num_contributions\": len(extract_contributions(p)),\n",
        "        }\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    print(f\"‚úÖ Dataset created: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "    return df\n",
        "\n",
        "# ============================================================\n",
        "# 5. SAVE DATASET (PROPER FOLDER STRUCTURE)\n",
        "# ============================================================\n",
        "\n",
        "def save_dataset(df, base_dir=\"data/dataset\"):\n",
        "    base = Path(base_dir)\n",
        "    base.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    folder = base / f\"dataset_{timestamp}\"\n",
        "    formats = folder / \"formats\"\n",
        "    analysis = folder / \"analysis\"\n",
        "\n",
        "    formats.mkdir(parents=True, exist_ok=True)\n",
        "    analysis.mkdir(exist_ok=True)\n",
        "\n",
        "    csv_path = formats / \"papers_dataset.csv\"\n",
        "    xlsx_path = formats / \"papers_dataset.xlsx\"\n",
        "    json_path = formats / \"papers_dataset.json\"\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_excel(xlsx_path, index=False)\n",
        "    df.to_json(json_path, orient=\"records\", indent=2, force_ascii=False)\n",
        "\n",
        "    stats = {\n",
        "        \"created_at\": datetime.now().isoformat(),\n",
        "        \"total_papers\": len(df),\n",
        "        \"total_columns\": len(df.columns),\n",
        "        \"papers_with_year\": int(df[\"year\"].notna().sum()),\n",
        "        \"avg_characters\": int(df[\"total_characters\"].mean()),\n",
        "    }\n",
        "\n",
        "    with open(analysis / \"dataset_statistics.json\", \"w\") as f:\n",
        "        json.dump(stats, f, indent=2)\n",
        "\n",
        "    # backward-compatible CSV\n",
        "    df.to_csv(base / \"papers_dataset.csv\", index=False)\n",
        "\n",
        "    print(\"\\nüìÅ Dataset saved successfully!\")\n",
        "    print(f\"üìÇ Location: {folder.resolve()}\")\n",
        "\n",
        "    return folder\n",
        "\n",
        "# ============================================================\n",
        "# 6. MAIN RUNNER\n",
        "# ============================================================\n",
        "\n",
        "def generate_paper_dataset():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"MODULE 5: DATASET GENERATOR\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    papers = load_all_extracted()\n",
        "    if not papers:\n",
        "        print(\"‚ùå No papers loaded. Aborting.\")\n",
        "        return None\n",
        "\n",
        "    df = build_dataset(papers)\n",
        "\n",
        "    print(\"\\nüìå Dataset Preview:\")\n",
        "    print(df.head(3).to_string())\n",
        "\n",
        "    save_dataset(df)\n",
        "\n",
        "    print(\"\\nüéâ MODULE 5 COMPLETED SUCCESSFULLY!\")\n",
        "    return df\n",
        "\n",
        "# ============================================================\n",
        "# AUTO RUN (COLAB FRIENDLY)\n",
        "# ============================================================\n",
        "\n",
        "generate_paper_dataset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdqaS0ao6wHO",
        "outputId": "a8cd2c09-ab55-438b-d3fc-a664785e7bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "MODULE 5: DATASET GENERATOR\n",
            "======================================================================\n",
            "‚ùå Directory not found: data/extracted\n",
            "   Please run Module 3 first.\n",
            "‚ùå No papers loaded. Aborting.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}