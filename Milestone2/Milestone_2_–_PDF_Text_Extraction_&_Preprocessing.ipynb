{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LOwVbgg7Smz",
        "outputId": "3bc828de-827b-464f-9960-cda5e3248414"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a95xq8Lf7Qoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e59b783-b971-4679-eda1-3dfdb63f03e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.37.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install requests semanticscholar crossrefapi PyMuPDF -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from crossref.restful import Works\n",
        "from semanticscholar import SemanticScholar\n",
        "import fitz  # PyMuPDF\n"
      ],
      "metadata": {
        "id": "BYz7HOeQ8Cqg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç 1. Search Semantic Scholar"
      ],
      "metadata": {
        "id": "bxyn7F27-H0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_semantic_scholar(query, max_results=10):\n",
        "    \"\"\"\n",
        "    Searches Semantic Scholar for research papers\n",
        "    based on a topic input.\n",
        "    Returns structured metadata for each paper.\n",
        "    \"\"\"\n",
        "    print(\"üîç Searching Semantic Scholar...\")\n",
        "    sch = SemanticScholar(timeout=10)\n",
        "\n",
        "    papers = []\n",
        "    try:\n",
        "        # Perform API search\n",
        "        results = sch.search_paper(query, limit=max_results)\n",
        "\n",
        "        # Extract fields from each paper\n",
        "        for p in results:\n",
        "            papers.append({\n",
        "                \"title\": p.title,\n",
        "                \"authors\": [a.name for a in p.authors] if p.authors else [],\n",
        "                \"year\": p.year,\n",
        "                \"pdf_url\": p.openAccessPdf.get(\"url\") if p.openAccessPdf else None,\n",
        "                \"citationCount\": p.citationCount,\n",
        "                \"abstract\": p.abstract,\n",
        "                \"url\": p.url\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(\"Semantic Scholar error:\", e)\n",
        "\n",
        "    return papers\n"
      ],
      "metadata": {
        "id": "P_w1YJvV8VNw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9SlvigBm0la"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_semantic_scholar(query, max_results=10):\n",
        "    \"\"\"\n",
        "    Searches Semantic Scholar for research papers\n",
        "    based on a topic input.\n",
        "    Returns structured metadata for each paper.\n",
        "    \"\"\"\n",
        "    print(\"üîç Searching Semantic Scholar...\")\n",
        "    sch = SemanticScholar(timeout=10)\n",
        "\n",
        "    papers = []\n",
        "    try:\n",
        "        # Perform API search\n",
        "        results = sch.search_paper(query, limit=max_results)\n",
        "\n",
        "        # Extract fields from each paper\n",
        "        for p in results:\n",
        "            papers.append({\n",
        "                \"title\": p.title,\n",
        "                \"authors\": [a.name for a in p.authors] if p.authors else [],\n",
        "                \"year\": p.year,\n",
        "                \"pdf_url\": p.openAccessPdf.get(\"url\") if p.openAccessPdf else None,\n",
        "                \"citationCount\": p.citationCount,\n",
        "                \"abstract\": p.abstract,\n",
        "                \"url\": p.url\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(\"Semantic Scholar error:\", e)\n",
        "\n",
        "    return papers\n"
      ],
      "metadata": {
        "id": "4CVqDCSE8dfA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìö 2. Search CrossRef (Backup API)"
      ],
      "metadata": {
        "id": "R0njzwcK-MCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_crossref(query, max_results=5):\n",
        "    \"\"\"\n",
        "    Searches CrossRef as a fallback API source.\n",
        "    Useful when Semantic Scholar returns fewer results.\n",
        "    \"\"\"\n",
        "    print(\"üîç Searching CrossRef...\")\n",
        "    works = Works()\n",
        "    papers = []\n",
        "\n",
        "    try:\n",
        "        # Search CrossRef and sort by relevance (score)\n",
        "        results = works.query(query).sort(\"score\")\n",
        "\n",
        "        count = 0\n",
        "        for item in results:\n",
        "            if count >= max_results:\n",
        "                break\n",
        "\n",
        "            # Extract minimal metadata\n",
        "            papers.append({\n",
        "                \"title\": item.get(\"title\", [\"\"])[0],\n",
        "                \"authors\": [a.get(\"family\",\"\") for a in item.get(\"author\", [])],\n",
        "                \"year\": item.get(\"issued\", {}).get(\"date-parts\", [[None]])[0][0],\n",
        "                \"pdf_url\": None,            # CrossRef does not give PDFs\n",
        "                \"citationCount\": None,\n",
        "                \"abstract\": None,\n",
        "                \"url\": item.get(\"URL\", \"\")\n",
        "            })\n",
        "            count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"CrossRef error:\", e)\n",
        "\n",
        "    return papers\n"
      ],
      "metadata": {
        "id": "TmM08zZN9eGX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† 3. Main Function ‚Äî Combine Both APIs & Save JSON"
      ],
      "metadata": {
        "id": "FxV4AoaH-TuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_module1(topic):\n",
        "    \"\"\"\n",
        "    Runs the full Module-1 workflow:\n",
        "    1. Search Semantic Scholar\n",
        "    2. Search CrossRef\n",
        "    3. Combine results\n",
        "    4. Save output to JSON\n",
        "    \"\"\"\n",
        "    print(\"\\n===============================\")\n",
        "    print(\"MODULE 1: Paper Search Started\")\n",
        "    print(\"===============================\\n\")\n",
        "\n",
        "    # API calls\n",
        "    ss_results = search_semantic_scholar(topic, max_results=10)\n",
        "    cr_results = search_crossref(topic, max_results=5)\n",
        "\n",
        "    # Merge results\n",
        "    all_papers = ss_results + cr_results\n",
        "\n",
        "    print(f\"\\nüìö Total papers found: {len(all_papers)}\")\n",
        "\n",
        "    # Create output folder\n",
        "    os.makedirs(\"data/search_results\", exist_ok=True)\n",
        "\n",
        "    # Dynamic filename\n",
        "    filename = f\"data/search_results/search_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "    # Save JSON data\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"topic\": topic,\n",
        "            \"papers\": all_papers\n",
        "        }, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(\"\\n‚úÖ Module 1 Completed!\")\n",
        "    print(f\"üìÅ Results saved to: {filename}\")\n",
        "\n",
        "    return filename\n"
      ],
      "metadata": {
        "id": "h5bZUoal9oSz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ñ∂ Run Module-1"
      ],
      "metadata": {
        "id": "-Rz5SXin-luf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic = \"AI system to automatically review and summarize research papers\"\n",
        "run_module1(topic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "3VUSofrV-jP8",
        "outputId": "4cf26dab-30e1-43a1-cf84-14fd2c748ede"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===============================\n",
            "MODULE 1: Paper Search Started\n",
            "===============================\n",
            "\n",
            "üîç Searching Semantic Scholar...\n",
            "üîç Searching CrossRef...\n",
            "\n",
            "üìö Total papers found: 16\n",
            "\n",
            "‚úÖ Module 1 Completed!\n",
            "üìÅ Results saved to: data/search_results/search_results_20251224_092539.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/search_results/search_results_20251224_092539.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß© MODULE 2 ‚Äî PDF Filtering, Ranking, Downloading\n",
        "Select PDFs ‚Üí Download with Verification ‚Üí Save Report"
      ],
      "metadata": {
        "id": "GveW_MQy-3WG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üì• 1. Load Search Results"
      ],
      "metadata": {
        "id": "QaMuP7Vi_Hu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_search_results(filepath=None):\n",
        "    \"\"\"\n",
        "    Loads the JSON file generated in Module-1.\n",
        "    \"\"\"\n",
        "    if not filepath:\n",
        "        print(\"‚ùå ERROR: Provide a valid filepath.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        print(f\" Loaded {len(data['papers'])} papers on topic: {data['topic']}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading search results: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "oeSDjtjg_QX6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìù 2. Filter Papers Having PDF Links"
      ],
      "metadata": {
        "id": "VOUhETsG_WRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_papers_with_pdfs(papers):\n",
        "    \"\"\"\n",
        "    Selects only papers that have a valid PDF URL.\n",
        "    \"\"\"\n",
        "    papers_with_pdf = [\n",
        "        p for p in papers\n",
        "        if p.get(\"pdf_url\") and (\"pdf\" in p[\"pdf_url\"].lower())\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n PDF Check:\")\n",
        "    print(f\" ‚Ä¢ Total papers: {len(papers)}\")\n",
        "    print(f\" ‚Ä¢ Papers with PDF URLs: {len(papers_with_pdf)}\")\n",
        "\n",
        "    return papers_with_pdf\n"
      ],
      "metadata": {
        "id": "E5V1W_4c_UMA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚≠ê 3. Rank Papers (Citations + Year)"
      ],
      "metadata": {
        "id": "CjNI57DG_jV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_papers(papers):\n",
        "    \"\"\"\n",
        "    Sorts papers by highest citation count and recent year.\n",
        "    \"\"\"\n",
        "    valid = [\n",
        "        p for p in papers\n",
        "        if p.get(\"citationCount\") is not None and p.get(\"year\")\n",
        "    ]\n",
        "\n",
        "    ranked = sorted(valid, key=lambda x: (x[\"citationCount\"], x[\"year\"]), reverse=True)\n",
        "    return ranked\n"
      ],
      "metadata": {
        "id": "ZimztT2i_bwC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ 4. Select Top N Papers"
      ],
      "metadata": {
        "id": "OQ9QeUGv_n1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_top_papers(papers, count=3):\n",
        "    \"\"\"\n",
        "    Selects the top N papers after:\n",
        "    1. PDF filtering\n",
        "    2. Ranking\n",
        "    \"\"\"\n",
        "    papers_with_pdf = filter_papers_with_pdfs(papers)\n",
        "    ranked = rank_papers(papers_with_pdf)\n",
        "    selected = ranked[:count]\n",
        "\n",
        "    print(f\"\\n Top {count} Selected Papers:\")\n",
        "    for i, p in enumerate(selected):\n",
        "        print(f\"\\n{i+1}. {p['title']}\")\n",
        "        print(f\"   ‚Ü≥ Citations: {p['citationCount']}, Year: {p['year']}\")\n",
        "\n",
        "    return selected\n"
      ],
      "metadata": {
        "id": "EvtmVEBn_r6e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. PDF Verification"
      ],
      "metadata": {
        "id": "UBfLS5B1_xOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_pdf(filepath):\n",
        "    \"\"\"\n",
        "    Checks if the file is a valid PDF using PyMuPDF.\n",
        "    Size <1KB is marked invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(filepath):\n",
        "            return False\n",
        "        if os.path.getsize(filepath) < 1024:\n",
        "            return False\n",
        "        with fitz.open(filepath) as doc:\n",
        "            return len(doc) > 0\n",
        "    except:\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "2n0JXf9q_0mV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚¨á 6. Download PDF with Retry + Verification"
      ],
      "metadata": {
        "id": "DbDFKvpF_5TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_pdf_with_verification(url, filename, max_retries=3):\n",
        "    \"\"\"\n",
        "    Downloads a PDF from a URL.\n",
        "    Includes:\n",
        "    - Retry logic\n",
        "    - User-Agent header\n",
        "    - PDF file validation\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0',\n",
        "        'Accept': 'application/pdf'\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"  Attempt {attempt + 1}/{max_retries}...\")\n",
        "            response = requests.get(url, headers=headers, timeout=30, stream=True)\n",
        "\n",
        "            # Blocked access\n",
        "            if response.status_code == 403:\n",
        "                print(\"    HTTP 403 Forbidden. Retrying...\")\n",
        "                continue\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"    HTTP Error: {response.status_code}\")\n",
        "                continue\n",
        "\n",
        "            # Save PDF\n",
        "            with open(filename, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "\n",
        "            # Validate\n",
        "            if verify_pdf(filename):\n",
        "                print(f\"    ‚úÖ Downloaded: {os.path.getsize(filename):,} bytes\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"    ‚ùå Invalid PDF. Retrying...\")\n",
        "                os.remove(filename)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error: {str(e)[:50]}\")\n",
        "\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "08DCXZ-g_3Cj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üì© 7. Download All Selected Papers"
      ],
      "metadata": {
        "id": "MFThcpekAUj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_selected_papers(selected, output_dir=\"downloads\"):\n",
        "    \"\"\"\n",
        "    Downloads the top N research paper PDFs into a folder.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    downloaded = []\n",
        "\n",
        "    for i, paper in enumerate(selected):\n",
        "        print(f\"\\n[{i+1}/{len(selected)}] Downloading:\")\n",
        "        print(\" \", paper['title'])\n",
        "\n",
        "        # Safe filename handling\n",
        "        safe_title = \"\".join(c for c in paper['title'] if c.isalnum())[:30]\n",
        "        filename = f\"{output_dir}/{safe_title}.pdf\"\n",
        "\n",
        "        if download_pdf_with_verification(paper[\"pdf_url\"], filename):\n",
        "            print(\"   ‚úÖ Success:\", filename)\n",
        "            paper[\"local_file\"] = filename\n",
        "            downloaded.append(paper)\n",
        "        else:\n",
        "            print(\"   ‚ùå Failed\")\n",
        "\n",
        "    return downloaded\n"
      ],
      "metadata": {
        "id": "b6ZOb6aoARC4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìÑ 8. Save JSON Download Report"
      ],
      "metadata": {
        "id": "Agmnmrm_Af89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_download_report(downloaded, topic):\n",
        "    \"\"\"\n",
        "    Creates a summary report of downloaded papers.\n",
        "    \"\"\"\n",
        "    os.makedirs(\"data/reports\", exist_ok=True)\n",
        "\n",
        "    report = {\n",
        "        \"topic\": topic,\n",
        "        \"download_count\": len(downloaded),\n",
        "        \"papers\": downloaded\n",
        "    }\n",
        "\n",
        "    output_file = f\"data/reports/download_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "\n",
        "    print(\"\\n Report saved to:\", output_file)\n",
        "    return output_file\n"
      ],
      "metadata": {
        "id": "aiq6ve4wAeJI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üöÄ 9. Final Master Function (Module-2)"
      ],
      "metadata": {
        "id": "DKNp74nzAtnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# AUTO-LOAD LATEST SEARCH RESULTS\n",
        "# =======================================================\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def get_latest_search_results():\n",
        "    \"\"\"\n",
        "    Finds the most recent JSON file from data/search_results/\n",
        "    \"\"\"\n",
        "    folder_path = \"data/search_results/\"\n",
        "    json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        raise FileNotFoundError(\"‚ùå No search results found. Run Module 1 first.\")\n",
        "\n",
        "    latest_file = max(json_files, key=os.path.getctime)\n",
        "    print(f\"üìÑ Latest search results auto-loaded:\\n{latest_file}\")\n",
        "    return latest_file\n"
      ],
      "metadata": {
        "id": "kI-iQFZyAloV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_module2(top_n=3):\n",
        "    \"\"\"\n",
        "    MODULE 2 DRIVER FUNCTION\n",
        "    ------------------------\n",
        "    Auto-loads latest search results, selects top papers,\n",
        "    downloads PDFs, and generates a report.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n========== MODULE 2 STARTED ==========\\n\")\n",
        "\n",
        "    # Step 1: Auto-load latest search_results file\n",
        "    try:\n",
        "        json_path = get_latest_search_results()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR: Failed to load latest search results.\\n{e}\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Load JSON content\n",
        "    data = load_search_results(json_path)\n",
        "    if not data:\n",
        "        print(\"‚ùå ERROR: search_results JSON is empty or unreadable.\")\n",
        "        return None\n",
        "\n",
        "    # Step 3: Filter & rank papers\n",
        "    print(\"\\nüìä Selecting top papers...\\n\")\n",
        "    selected = select_top_papers(data[\"papers\"], count=top_n)\n",
        "\n",
        "    # Step 4: Download selected PDFs\n",
        "    print(\"\\nüì• Downloading PDFs...\\n\")\n",
        "    downloaded = download_selected_papers(selected)\n",
        "\n",
        "    # Step 5: Save results\n",
        "    save_download_report(downloaded, data[\"topic\"])\n",
        "\n",
        "    print(\"\\n========== MODULE 2 COMPLETED ==========\\n\")\n",
        "\n",
        "    return downloaded\n"
      ],
      "metadata": {
        "id": "w5H_E3vXA1f5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_module2(top_n=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diLtZ-lhCpi8",
        "outputId": "c771ed1d-002d-468c-b65b-1fca1fed01f0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== MODULE 2 STARTED ==========\n",
            "\n",
            "üìÑ Latest search results auto-loaded:\n",
            "data/search_results/search_results_20251224_092539.json\n",
            " Loaded 16 papers on topic: AI system to automatically review and summarize research papers\n",
            "\n",
            "üìä Selecting top papers...\n",
            "\n",
            "\n",
            " PDF Check:\n",
            " ‚Ä¢ Total papers: 16\n",
            " ‚Ä¢ Papers with PDF URLs: 4\n",
            "\n",
            " Top 3 Selected Papers:\n",
            "\n",
            "1. Automatic assessment of text-based responses in post-secondary education: A systematic review\n",
            "   ‚Ü≥ Citations: 92, Year: 2023\n",
            "\n",
            "2. Editorial for Special Issue on Large-scale Pre-training: Data, Models, and Fine-tuning\n",
            "   ‚Ü≥ Citations: 2, Year: 2023\n",
            "\n",
            "3. Special issue on future hybrid artificial intelligence and machine learning for smart expert systems\n",
            "   ‚Ü≥ Citations: 0, Year: 2021\n",
            "\n",
            "üì• Downloading PDFs...\n",
            "\n",
            "\n",
            "[1/3] Downloading:\n",
            "  Automatic assessment of text-based responses in post-secondary education: A systematic review\n",
            "  Attempt 1/3...\n",
            "    ‚úÖ Downloaded: 962,703 bytes\n",
            "   ‚úÖ Success: downloads/Automaticassessmentoftextbased.pdf\n",
            "\n",
            "[2/3] Downloading:\n",
            "  Editorial for Special Issue on Large-scale Pre-training: Data, Models, and Fine-tuning\n",
            "  Attempt 1/3...\n",
            "    ‚úÖ Downloaded: 380,064 bytes\n",
            "   ‚úÖ Success: downloads/EditorialforSpecialIssueonLarg.pdf\n",
            "\n",
            "[3/3] Downloading:\n",
            "  Special issue on future hybrid artificial intelligence and machine learning for smart expert systems\n",
            "  Attempt 1/3...\n",
            "    HTTP 403 Forbidden. Retrying...\n",
            "  Attempt 2/3...\n",
            "    HTTP 403 Forbidden. Retrying...\n",
            "  Attempt 3/3...\n",
            "    HTTP 403 Forbidden. Retrying...\n",
            "   ‚ùå Failed\n",
            "\n",
            " Report saved to: data/reports/download_report_20251224_092540.json\n",
            "\n",
            "========== MODULE 2 COMPLETED ==========\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Automatic assessment of text-based responses in post-secondary education: A systematic review',\n",
              "  'authors': ['Rujun Gao',\n",
              "   'H. Merzdorf',\n",
              "   'S. Anwar',\n",
              "   'M. C. Hipwell',\n",
              "   'Arun Srinivasa'],\n",
              "  'year': 2023,\n",
              "  'pdf_url': 'https://arxiv.org/pdf/2308.16151',\n",
              "  'citationCount': 92,\n",
              "  'abstract': 'Text-based open-ended questions in academic formative and summative assessments help students become deep learners and prepare them to understand concepts for a subsequent conceptual assessment. However, grading text-based questions, especially in large courses, is tedious and time-consuming for instructors. Text processing models continue progressing with the rapid development of Artificial Intelligence (AI) tools and Natural Language Processing (NLP) algorithms. Especially after breakthroughs in Large Language Models (LLM), there is immense potential to automate rapid assessment and feedback of text-based responses in education. This systematic review adopts a scientific and reproducible literature search strategy based on the PRISMA process using explicit inclusion and exclusion criteria to study text-based automatic assessment systems in post-secondary education, screening 838 papers and synthesizing 93 studies. To understand how text-based automatic assessment systems have been developed and applied in education in recent years, three research questions are considered. All included studies are summarized and categorized according to a proposed comprehensive framework, including the input and output of the system, research motivation, and research outcomes, aiming to answer the research questions accordingly. Additionally, the typical studies of automated assessment systems, research methods, and application domains in these studies are investigated and summarized. This systematic review provides an overview of recent educational applications of text-based assessment systems for understanding the latest AI/NLP developments assisting in text-based assessments in higher education. Findings will particularly benefit researchers and educators incorporating LLMs such as ChatGPT into their educational activities.',\n",
              "  'url': 'https://www.semanticscholar.org/paper/2ced926ba4625fc08ac6685de16df2f142b3126f',\n",
              "  'local_file': 'downloads/Automaticassessmentoftextbased.pdf'},\n",
              " {'title': 'Editorial for Special Issue on Large-scale Pre-training: Data, Models, and Fine-tuning',\n",
              "  'authors': ['Jiying Wen', 'Zi-Hao Huang', 'Hanwang Zhang'],\n",
              "  'year': 2023,\n",
              "  'pdf_url': 'https://link.springer.com/content/pdf/10.1007/s11633-023-1431-y.pdf',\n",
              "  'citationCount': 2,\n",
              "  'abstract': None,\n",
              "  'url': 'https://www.semanticscholar.org/paper/72be13006e30a1601c0dfb0d3a2479006d1239c7',\n",
              "  'local_file': 'downloads/EditorialforSpecialIssueonLarg.pdf'}]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Milestone 2: PDF Text Extraction & Preprocessing\n",
        "\n",
        "Automatically generated by Colab.\n",
        "Enhanced version aligned with Infosys Springboard evaluation.\n",
        "\n",
        "Author: Intern\n",
        "Project: AI System to Automatically Review and Summarize Research Papers\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 0: INSTALL DEPENDENCIES\n",
        "# ============================================================\n",
        "\n",
        "!pip install pymupdf pymupdf4llm tqdm -q\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 1: IMPORTS & CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import fitz                    # PyMuPDF\n",
        "import pymupdf4llm             # Layout-aware extraction\n",
        "\n",
        "# Directory where PDFs were downloaded in Milestone 1\n",
        "PDF_DIR = \"downloads\"\n",
        "\n",
        "# Output directory for extracted structured data\n",
        "OUTPUT_DIR = \"data/extracted\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"üìÅ PDF Source Directory:\", PDF_DIR)\n",
        "print(\"üìÅ Output Directory:\", OUTPUT_DIR)\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 2: TEXT CLEANING UTILITIES\n",
        "# ============================================================\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans raw PDF text by:\n",
        "    - Removing extra spaces\n",
        "    - Fixing hyphenated line breaks\n",
        "    - Removing unreadable characters\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'-\\s+', '', text)\n",
        "    text = ''.join(c for c in text if ord(c) >= 32)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 3: PDF TEXT EXTRACTION\n",
        "# ============================================================\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text using two strategies:\n",
        "    1) Layout-aware extraction (preferred)\n",
        "    2) Standard PyMuPDF extraction (fallback)\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nüìÑ Processing: {pdf_path.name}\")\n",
        "\n",
        "    extracted_versions = []\n",
        "\n",
        "    try:\n",
        "        # Strategy 1: Layout-aware extraction\n",
        "        try:\n",
        "            layout_text = pymupdf4llm.to_markdown(str(pdf_path))\n",
        "            if layout_text and len(layout_text) > 1000:\n",
        "                extracted_versions.append(layout_text)\n",
        "                print(\"   ‚úî Layout-aware extraction used\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Strategy 2: Standard PyMuPDF extraction\n",
        "        doc = fitz.open(pdf_path)\n",
        "        raw_text = \"\"\n",
        "        for page in doc[:min(40, len(doc))]:\n",
        "            raw_text += page.get_text()\n",
        "        doc.close()\n",
        "\n",
        "        if len(raw_text) > 1000:\n",
        "            extracted_versions.append(raw_text)\n",
        "            print(\"   ‚úî Standard extraction used\")\n",
        "\n",
        "        if not extracted_versions:\n",
        "            print(\"   ‚ùå No usable text extracted\")\n",
        "            return None\n",
        "\n",
        "        best_text = max(extracted_versions, key=len)\n",
        "        return clean_text(best_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"   ‚ùå Extraction failed:\", e)\n",
        "        return None\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 4: SECTION-WISE EXTRACTION\n",
        "# ============================================================\n",
        "\n",
        "def extract_sections(text):\n",
        "    \"\"\"\n",
        "    Extract standard academic sections using regex.\n",
        "    \"\"\"\n",
        "\n",
        "    sections = {\n",
        "        \"title\": \"\",\n",
        "        \"abstract\": \"\",\n",
        "        \"introduction\": \"\",\n",
        "        \"methods\": \"\",\n",
        "        \"results\": \"\",\n",
        "        \"conclusion\": \"\"\n",
        "    }\n",
        "\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    patterns = {\n",
        "        \"abstract\": r\"\\babstract\\b\",\n",
        "        \"introduction\": r\"\\bintroduction\\b\",\n",
        "        \"methods\": r\"\\b(methods?|methodology|experiment)\\b\",\n",
        "        \"results\": r\"\\b(results?|findings)\\b\",\n",
        "        \"conclusion\": r\"\\b(conclusion|discussion)\\b\"\n",
        "    }\n",
        "\n",
        "    boundaries = {}\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                boundaries.setdefault(section, i)\n",
        "\n",
        "    sorted_sections = sorted(boundaries.items(), key=lambda x: x[1])\n",
        "\n",
        "    for idx, (section, start) in enumerate(sorted_sections):\n",
        "        end = sorted_sections[idx + 1][1] if idx + 1 < len(sorted_sections) else len(lines)\n",
        "        content = \" \".join(lines[start:end]).strip()\n",
        "        if len(content) > 200:\n",
        "            sections[section] = content[:5000]\n",
        "\n",
        "    # Extract title from first meaningful line\n",
        "    for line in lines[:10]:\n",
        "        if 20 < len(line) < 200:\n",
        "            sections[\"title\"] = line.strip()\n",
        "            break\n",
        "\n",
        "    return sections\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 5: KEY FINDINGS EXTRACTION\n",
        "# ============================================================\n",
        "\n",
        "def extract_key_findings(sections, max_findings=5):\n",
        "    \"\"\"\n",
        "    Extract key conclusions from Results and Conclusion sections.\n",
        "    \"\"\"\n",
        "\n",
        "    keywords = [\n",
        "        \"we found\", \"results show\", \"our results\",\n",
        "        \"significant\", \"improves\", \"outperforms\",\n",
        "        \"demonstrates\", \"indicates\", \"we conclude\"\n",
        "    ]\n",
        "\n",
        "    findings = []\n",
        "    combined_text = sections.get(\"results\", \"\") + \" \" + sections.get(\"conclusion\", \"\")\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', combined_text)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(k in sentence.lower() for k in keywords):\n",
        "            findings.append(sentence.strip())\n",
        "        if len(findings) >= max_findings:\n",
        "            break\n",
        "\n",
        "    return findings\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 6: VALIDATION CHECKS\n",
        "# ============================================================\n",
        "\n",
        "def validate_extraction(paper):\n",
        "    \"\"\"\n",
        "    Validate extracted content quality.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"has_abstract\": len(paper[\"sections\"].get(\"abstract\", \"\")) > 200,\n",
        "        \"has_multiple_sections\": sum(len(v) > 200 for v in paper[\"sections\"].values()) >= 2,\n",
        "        \"has_key_findings\": len(paper[\"key_findings\"]) > 0,\n",
        "        \"sufficient_text\": paper[\"stats\"][\"total_chars\"] > 1000\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 7: PROCESS SINGLE PDF\n",
        "# ============================================================\n",
        "\n",
        "def process_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    End-to-end PDF processing.\n",
        "    \"\"\"\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    sections = extract_sections(text)\n",
        "    key_findings = extract_key_findings(sections)\n",
        "\n",
        "    paper = {\n",
        "        \"paper_id\": pdf_path.stem,\n",
        "        \"filename\": pdf_path.name,\n",
        "        \"sections\": sections,\n",
        "        \"key_findings\": key_findings,\n",
        "        \"stats\": {\n",
        "            \"total_chars\": len(text),\n",
        "            \"sections_found\": sum(len(v) > 200 for v in sections.values())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    paper[\"validation\"] = validate_extraction(paper)\n",
        "    return paper\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 8: CROSS-PAPER COMPARISON\n",
        "# ============================================================\n",
        "\n",
        "def compare_papers(papers):\n",
        "    \"\"\"\n",
        "    Identify common themes across papers.\n",
        "    \"\"\"\n",
        "    all_findings = []\n",
        "    for paper in papers:\n",
        "        all_findings.extend(paper[\"key_findings\"])\n",
        "\n",
        "    words = []\n",
        "    for finding in all_findings:\n",
        "        words.extend(re.findall(r'\\b[a-zA-Z]{4,}\\b', finding.lower()))\n",
        "\n",
        "    return {\n",
        "        \"total_papers\": len(papers),\n",
        "        \"total_findings\": len(all_findings),\n",
        "        \"common_terms\": Counter(words).most_common(10)\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 9: RUN MILESTONE 2 PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "def run_milestone_2():\n",
        "    pdf_files = list(Path(PDF_DIR).glob(\"*.pdf\"))\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\" MILESTONE 2 PIPELINE STARTED \")\n",
        "    print(\"==============================\")\n",
        "    print(f\"üìÑ PDFs Found: {len(pdf_files)}\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"‚ùå No PDFs found. Run Milestone 1 first.\")\n",
        "        return\n",
        "\n",
        "    processed_papers = []\n",
        "\n",
        "    for pdf in tqdm(pdf_files):\n",
        "        paper = process_pdf(pdf)\n",
        "        if paper:\n",
        "            processed_papers.append(paper)\n",
        "            with open(f\"{OUTPUT_DIR}/{paper['paper_id']}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(paper, f, indent=2)\n",
        "\n",
        "    comparison = compare_papers(processed_papers)\n",
        "    with open(f\"{OUTPUT_DIR}/comparison_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(comparison, f, indent=2)\n",
        "\n",
        "    print(\"\\n‚úÖ MILESTONE 2 COMPLETED SUCCESSFULLY\")\n",
        "    print(f\"üìä Papers Processed: {len(processed_papers)}\")\n",
        "    print(\"üìÅ Output Saved To:\", OUTPUT_DIR)\n",
        "\n",
        "    return processed_papers, comparison\n",
        "\n",
        "# ============================================================\n",
        "# EXECUTION\n",
        "# ============================================================\n",
        "\n",
        "results, comparison = run_milestone_2()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqmLbx1orrnd",
        "outputId": "76c26bc2-3957-4193-cf1c-afdc65ed9109"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n",
            "üìÅ PDF Source Directory: downloads\n",
            "üìÅ Output Directory: data/extracted\n",
            "\n",
            "==============================\n",
            " MILESTONE 2 PIPELINE STARTED \n",
            "==============================\n",
            "üìÑ PDFs Found: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÑ Processing: Automaticassessmentoftextbased.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:21<00:21, 21.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úî Layout-aware extraction used\n",
            "   ‚úî Standard extraction used\n",
            "\n",
            "üìÑ Processing: EditorialforSpecialIssueonLarg.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:22<00:00, 11.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úî Layout-aware extraction used\n",
            "   ‚úî Standard extraction used\n",
            "\n",
            "‚úÖ MILESTONE 2 COMPLETED SUCCESSFULLY\n",
            "üìä Papers Processed: 2\n",
            "üìÅ Output Saved To: data/extracted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}