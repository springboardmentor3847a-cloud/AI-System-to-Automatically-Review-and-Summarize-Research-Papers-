{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPteDFDemZEm"
      },
      "source": [
        "AI System to Automatically Review and Summarize Research Papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LRZl39Ymanr"
      },
      "source": [
        "Module 1:Topic Input and Paper search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKdX4_N_mgh4"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "!pip install semanticscholar python-dotenv requests -q\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from semanticscholar import SemanticScholar\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsTv9fkcucu7"
      },
      "outputs": [],
      "source": [
        "#fallback papers(when api fails)\n",
        "FALLBACK_PAPERS = [\n",
        "    {\n",
        "        \"title\": \"Deep Learning\",\n",
        "        \"authors\": [\"LeCun\", \"Bengio\", \"Hinton\"],\n",
        "        \"year\": 2015,\n",
        "        \"paperId\": \"DL001\",\n",
        "        \"abstract\": \"Overview of deep learning...\",\n",
        "        \"citationCount\": 50000,\n",
        "        \"venue\": \"Nature\",\n",
        "        \"url\": \"https://arxiv.org/abs/1502.01852\",\n",
        "        \"pdf_url\": \"https://arxiv.org/pdf/1502.01852.pdf\",\n",
        "        \"has_pdf\": True\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Attention Is All You Need\",\n",
        "        \"authors\": [\"Vaswani\", \"Shazeer\"],\n",
        "        \"year\": 2017,\n",
        "        \"paperId\": \"DL002\",\n",
        "        \"abstract\": \"Transformer architecture...\",\n",
        "        \"citationCount\": 100000,\n",
        "        \"venue\": \"NeurIPS\",\n",
        "        \"url\": \"https://arxiv.org/abs/1706.03762\",\n",
        "        \"pdf_url\": \"https://arxiv.org/pdf/1706.03762.pdf\",\n",
        "        \"has_pdf\": True\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Machine Learning Foundations\",\n",
        "        \"authors\": [\"Mitchell\"],\n",
        "        \"year\": 1997,\n",
        "        \"paperId\": \"DL003\",\n",
        "        \"abstract\": \"Introduction to machine learning foundations...\",\n",
        "        \"citationCount\": 20000,\n",
        "        \"venue\": \"McGraw Hill\",\n",
        "        \"url\": None,\n",
        "        \"pdf_url\": None,\n",
        "        \"has_pdf\": False\n",
        "    }\n",
        "]\n",
        "# safe api initilaization\n",
        "def setup_api_key():\n",
        "    load_dotenv()\n",
        "    API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
        "\n",
        "    if not API_KEY:\n",
        "        print(\"No API key found. Running without API (fallback mode).\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        sch = SemanticScholar(api_key=API_KEY)\n",
        "        # Test request to validate key\n",
        "        sch.search_paper(\"test\", limit=1)\n",
        "        print(\"Semantic Scholar initialized with API key\")\n",
        "        return sch\n",
        "    except Exception as e:\n",
        "        print(f\"API key failed ({e}) → Using fallback mode.\")\n",
        "        return None\n",
        "# Buid result dictionary\n",
        "def build_result(topic, papers):\n",
        "    return {\n",
        "        \"topic\": topic,\n",
        "        \"search_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"total_results\": len(papers),\n",
        "        \"papers_with_pdf\": sum(p[\"has_pdf\"] for p in papers),\n",
        "        \"papers\": papers\n",
        "    }\n",
        "# search papers\n",
        "def search_papers(topic, limit=20):\n",
        "    print(f\"\\n Searching for papers on topic: '{topic}'\")\n",
        "\n",
        "    sch = setup_api_key()\n",
        "\n",
        "    # If API not available → fallback\n",
        "    if sch is None:\n",
        "        print(\" Using fallback sample dataset.\\n\")\n",
        "        return build_result(topic, FALLBACK_PAPERS)\n",
        "\n",
        "    try:\n",
        "        results = sch.search_paper(\n",
        "            query=topic,\n",
        "            limit=limit,\n",
        "            fields=[\"paperId\", \"title\", \"abstract\", \"year\", \"authors\",\n",
        "                    \"citationCount\", \"openAccessPdf\", \"url\", \"venue\"]\n",
        "        )\n",
        "\n",
        "        papers = []\n",
        "        for p in results:\n",
        "            papers.append({\n",
        "                \"title\": p.title,\n",
        "                \"authors\": [a[\"name\"] for a in p.authors] if p.authors else [],\n",
        "                \"year\": p.year,\n",
        "                \"paperId\": p.paperId,\n",
        "                \"abstract\": (p.abstract[:300] + \"...\") if p.abstract else \"No abstract\",\n",
        "                \"citationCount\": p.citationCount or 0,\n",
        "                \"venue\": getattr(p, \"venue\", None),\n",
        "                \"url\": p.url,\n",
        "                \"pdf_url\": p.openAccessPdf[\"url\"] if p.openAccessPdf else None,\n",
        "                \"has_pdf\": bool(p.openAccessPdf)\n",
        "            })\n",
        "\n",
        "        print(\"\\n Semantic Scholar search completed successfully!\")\n",
        "        return build_result(topic, papers)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Semantic Scholar search failed: {e}\")\n",
        "        print(\" Using fallback dataset.\\n\")\n",
        "        return build_result(topic, FALLBACK_PAPERS)\n",
        "# save search results\n",
        "def save_search_results(data):\n",
        "    os.makedirs(\"data/search_results\", exist_ok=True)\n",
        "    fname = f\"{data['topic'].replace(' ', '_')}_results.json\"\n",
        "    path = f\"data/search_results/{fname}\"\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "    print(f\"\\n Results saved to: {path}\")\n",
        "    return path\n",
        "# display results\n",
        "def display_search_results(data):\n",
        "    print(f\" SEARCH RESULTS FOR: {data['topic']}\")\n",
        "\n",
        "    print(f\"\\n Total papers found: {data['total_results']}\")\n",
        "    print(f\" Papers with PDF: {data['papers_with_pdf']}\")\n",
        "\n",
        "    print(\"\\n TOP PAPERS:\")\n",
        "\n",
        "    for i, p in enumerate(data[\"papers\"], start=1):\n",
        "        print(f\"\\n{i}. {p['title']}\")\n",
        "        print(f\"   Authors: {', '.join(p['authors'])}\")\n",
        "        print(f\"    Year: {p['year']}\")\n",
        "        print(f\"    Citations: {p['citationCount']}\")\n",
        "        print(f\"    PDF: {'YES' if p['has_pdf'] else 'NO'}\")\n",
        "#main function\n",
        "def main_search():\n",
        "    print(\" MODULE 1: TOPIC INPUT & PAPER SEARCH\")\n",
        "\n",
        "    topic = input(\"\\nEnter research topic: \").strip()\n",
        "    if not topic:\n",
        "        topic = \"machine learning\"\n",
        "\n",
        "    results = search_papers(topic)\n",
        "    path = save_search_results(results)\n",
        "    display_search_results(results)\n",
        "\n",
        "    print(\"\\n MODULE 1 COMPLETE!\")\n",
        "    print(f\" Proceed to Module 2\\n\")\n",
        "\n",
        "    return results, path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P18o-9I9A1w"
      },
      "source": [
        "Module 2:Paper selection and Pdf download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU22mcXWbp_m"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "os.makedirs(\"downloads\", exist_ok=True)\n",
        "def load_search_results(path):\n",
        "    print(\"\\n Loading Module 1 results...\")\n",
        "\n",
        "    if not path or not os.path.exists(path):\n",
        "        print(f\" ERROR: Cannot read results → {path}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        print(\"Results loaded.\\n\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\" JSON read error: {e}\")\n",
        "        return None\n",
        "\n",
        "def rank_papers(papers):\n",
        "    return sorted(\n",
        "        papers,\n",
        "        key=lambda p: ((p.get(\"citationCount\") or 0), (p.get(\"year\") or 0)),\n",
        "        reverse=True\n",
        "    )\n",
        "def download_pdf(url, title):\n",
        "    print(f\"\\n Downloading: {title}\")\n",
        "\n",
        "    if not url:\n",
        "        print(\"    No PDF URL available.\")\n",
        "        return False, \"no_url\"\n",
        "\n",
        "    # Safe standardized filename\n",
        "    safe_title = \"\".join(char if char.isalnum() or char in \" _-\" else \"_\" for char in title)[:50]\n",
        "    filename = f\"{safe_title}_{abs(hash(url)) % 99999}.pdf\"\n",
        "    filepath = os.path.join(\"downloads\", filename)\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, timeout=20)\n",
        "        if r.status_code != 200:\n",
        "            print(f\"   HTTP error: {r.status_code}\")\n",
        "            return False, f\"http_{r.status_code}\"\n",
        "\n",
        "        with open(filepath, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "\n",
        "        # Validate PDF\n",
        "        try:\n",
        "            doc = fitz.open(filepath)\n",
        "            if doc.page_count == 0:\n",
        "                os.remove(filepath)\n",
        "                print(\"    Corrupted PDF.\")\n",
        "                return False, \"empty_pdf\"\n",
        "        except:\n",
        "            os.remove(filepath)\n",
        "            print(\"    Could not open PDF.\")\n",
        "            return False, \"invalid_pdf\"\n",
        "\n",
        "        print(f\"    Saved at: {filepath}\")\n",
        "        return True, filepath\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    ERROR: {e}\")\n",
        "        return False, str(e)\n",
        "def ask_user_to_select(papers):\n",
        "\n",
        "    print(\"\\n AVAILABLE PAPERS WITH PDF:\")\n",
        "    for i, p in enumerate(papers, start=1):\n",
        "        print(f\"{i}. {p['title'][:60]}\")\n",
        "        print(f\"    Citations: {p['citationCount']} | Year: {p['year']}\")\n",
        "        print()\n",
        "\n",
        "    print(\"\\n Press ENTER to auto-select top papers\")\n",
        "    choice = input(\"Or enter paper numbers (comma separated): \").strip()\n",
        "\n",
        "    if choice == \"\":\n",
        "        print(\"\\n Auto-selecting top papers based on citation count.\\n\")\n",
        "        return None  # automatic mode\n",
        "\n",
        "    try:\n",
        "        indexes = [int(x.strip()) for x in choice.split(\",\")]\n",
        "        selected = [papers[i - 1] for i in indexes if 1 <= i <= len(papers)]\n",
        "        print(\"\\n User-selected papers:\")\n",
        "        return selected\n",
        "\n",
        "    except:\n",
        "        print(\"Invalid input → Using automatic selection.\")\n",
        "        return None\n",
        "\n",
        "def main_module_2(results_path):\n",
        "\n",
        "    print(\" MODULE 2: PDF DOWNLOAD WITH USER OR AUTO SELECTION\")\n",
        "    data = load_search_results(results_path)\n",
        "    if not data:\n",
        "        return\n",
        "\n",
        "    papers = data.get(\"papers\", [])\n",
        "    pdf_papers = [p for p in papers if p.get(\"has_pdf\")]\n",
        "\n",
        "    print(f\"Total papers: {len(papers)}\")\n",
        "    print(f\" Papers with PDF: {len(pdf_papers)}\")\n",
        "\n",
        "    if len(pdf_papers) == 0:\n",
        "        print(\"\\n⚠ No PDF papers found → Using fallback PDF paper.\")\n",
        "        pdf_papers = [{\n",
        "            \"title\": \"Deep Learning (Fallback PDF)\",\n",
        "            \"pdf_url\": \"https://arxiv.org/pdf/1502.01852.pdf\",\n",
        "            \"citationCount\": 50000,\n",
        "            \"year\": 2015\n",
        "        }]\n",
        "    selected = ask_user_to_select(pdf_papers)\n",
        "    if selected is None:\n",
        "        ranked = rank_papers(pdf_papers)\n",
        "        selected = ranked[:3]  # choose top 3 papers automatically\n",
        "\n",
        "    print(\"\\n SELECTED PAPERS FOR DOWNLOAD:\")\n",
        "    for p in selected:\n",
        "        print(f\" - {p['title']} (Citations: {p.get('citationCount',0)})\")\n",
        "    print(\"\\n STARTING DOWNLOADS...\\n\")\n",
        "\n",
        "    for p in selected:\n",
        "        success, info = download_pdf(p.get(\"pdf_url\"), p[\"title\"])\n",
        "        print(f\"  RESULT: {'SUCCESS' if success else 'FAILED'} ({info})\")\n",
        "\n",
        "    print(\"\\n MODULE 2 COMPLETE!\")\n",
        "    print(\" PDFs stored in: downloads/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6Nkzpshu84_",
        "outputId": "06afe30b-b0c7-455d-99cc-f7ea8cc86ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " MODULE 1: TOPIC INPUT & PAPER SEARCH\n",
            "\n",
            "Enter research topic: Machine Learning\n",
            "\n",
            " Searching for papers on topic: 'Machine Learning'\n",
            "No API key found. Running without API (fallback mode).\n",
            " Using fallback sample dataset.\n",
            "\n",
            "\n",
            " Results saved to: data/search_results/Machine_Learning_results.json\n",
            " SEARCH RESULTS FOR: Machine Learning\n",
            "\n",
            " Total papers found: 3\n",
            " Papers with PDF: 2\n",
            "\n",
            " TOP PAPERS:\n",
            "\n",
            "1. Deep Learning\n",
            "   Authors: LeCun, Bengio, Hinton\n",
            "    Year: 2015\n",
            "    Citations: 50000\n",
            "    PDF: YES\n",
            "\n",
            "2. Attention Is All You Need\n",
            "   Authors: Vaswani, Shazeer\n",
            "    Year: 2017\n",
            "    Citations: 100000\n",
            "    PDF: YES\n",
            "\n",
            "3. Machine Learning Foundations\n",
            "   Authors: Mitchell\n",
            "    Year: 1997\n",
            "    Citations: 20000\n",
            "    PDF: NO\n",
            "\n",
            " MODULE 1 COMPLETE!\n",
            " Proceed to Module 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results, path = main_search()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6O-00B2vvth",
        "outputId": "c5ff40da-af6a-42e7-ecfc-9d284e03f968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " MODULE 2: PDF DOWNLOAD WITH USER OR AUTO SELECTION\n",
            "\n",
            " Loading Module 1 results...\n",
            "Results loaded.\n",
            "\n",
            "Total papers: 3\n",
            " Papers with PDF: 2\n",
            "\n",
            " AVAILABLE PAPERS WITH PDF:\n",
            "1. Deep Learning\n",
            "    Citations: 50000 | Year: 2015\n",
            "\n",
            "2. Attention Is All You Need\n",
            "    Citations: 100000 | Year: 2017\n",
            "\n",
            "\n",
            " Press ENTER to auto-select top papers\n",
            "Or enter paper numbers (comma separated): \n",
            "\n",
            " Auto-selecting top papers based on citation count.\n",
            "\n",
            "\n",
            " SELECTED PAPERS FOR DOWNLOAD:\n",
            " - Attention Is All You Need (Citations: 100000)\n",
            " - Deep Learning (Citations: 50000)\n",
            "\n",
            " STARTING DOWNLOADS...\n",
            "\n",
            "\n",
            " Downloading: Attention Is All You Need\n",
            "    Saved at: downloads/Attention Is All You Need_27546.pdf\n",
            "  RESULT: SUCCESS (downloads/Attention Is All You Need_27546.pdf)\n",
            "\n",
            " Downloading: Deep Learning\n",
            "    Saved at: downloads/Deep Learning_62071.pdf\n",
            "  RESULT: SUCCESS (downloads/Deep Learning_62071.pdf)\n",
            "\n",
            " MODULE 2 COMPLETE!\n",
            " PDFs stored in: downloads/\n"
          ]
        }
      ],
      "source": [
        "main_module_2(path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijy7G_fncuBf"
      },
      "source": [
        "Module 3:PDF Text Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZEGF5MWc03T"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from datetime import datetime\n",
        "\n",
        "DOWNLOAD_DIR = \"downloads\"\n",
        "EXTRACT_DIR = \"data/extracted_text\"\n",
        "\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path, max_pages=20):\n",
        "    text = \"\"\n",
        "\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            pages_to_read = min(len(doc), max_pages)\n",
        "\n",
        "            for page_num in range(pages_to_read):\n",
        "                page = doc.load_page(page_num)\n",
        "                text += page.get_text()\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Failed to extract text from {pdf_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def run_module_3():\n",
        "    print(\"MODULE 3: PDF TEXT EXTRACTION\")\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.endswith(\".pdf\")]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"No PDF files found. Run Module 2 first.\")\n",
        "        return []\n",
        "\n",
        "    extracted_files = []\n",
        "\n",
        "    for pdf in pdf_files:\n",
        "        pdf_path = os.path.join(DOWNLOAD_DIR, pdf)\n",
        "        print(f\"\\n Processing: {pdf}\")\n",
        "\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        if not text:\n",
        "            print(\" No text extracted.\")\n",
        "            continue\n",
        "\n",
        "        base_name = os.path.splitext(pdf)[0]\n",
        "        text_path = os.path.join(EXTRACT_DIR, base_name + \".txt\")\n",
        "        meta_path = os.path.join(EXTRACT_DIR, base_name + \"_meta.json\")\n",
        "\n",
        "        # Save extracted text\n",
        "        with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "\n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            \"source_pdf\": pdf,\n",
        "            \"characters_extracted\": len(text),\n",
        "            \"extraction_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "\n",
        "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(metadata, f, indent=4)\n",
        "\n",
        "        extracted_files.append(text_path)\n",
        "\n",
        "        print(f\" Text extracted and saved: {text_path}\")\n",
        "\n",
        "    print(\"\\n MODULE 3 COMPLETE!\")\n",
        "    print(f\" Extracted files saved in: {EXTRACT_DIR}\")\n",
        "\n",
        "    return extracted_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1E_HvTlVVUX",
        "outputId": "7a3bd27e-02e8-4c1f-9a0d-c0f2356d1592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODULE 3: PDF TEXT EXTRACTION\n",
            "\n",
            " Processing: Attention Is All You Need_27546.pdf\n",
            " Text extracted and saved: data/extracted_text/Attention Is All You Need_27546.txt\n",
            "\n",
            " Processing: Deep Learning_62071.pdf\n",
            " Text extracted and saved: data/extracted_text/Deep Learning_62071.txt\n",
            "\n",
            " MODULE 3 COMPLETE!\n",
            " Extracted files saved in: data/extracted_text\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['data/extracted_text/Attention Is All You Need_27546.txt',\n",
              " 'data/extracted_text/Deep Learning_62071.txt']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run_module_3()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp80ioeaVlcK"
      },
      "source": [
        "Module 4:Paper Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsCJAymRVitt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "EXTRACT_DIR = \"data/extracted_text\"\n",
        "SUMMARY_DIR = \"data/summaries\"\n",
        "\n",
        "os.makedirs(SUMMARY_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def simple_summary(text, max_sentences=5):\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "    return \" \".join(sentences[:max_sentences]).strip()\n",
        "\n",
        "\n",
        "def run_module_4():\n",
        "    print(\" MODULE 4: PAPER SUMMARIZATION\")\n",
        "\n",
        "    if not os.path.exists(EXTRACT_DIR):\n",
        "        print(\" Extracted text folder missing. Run Module 3 first.\")\n",
        "        return\n",
        "\n",
        "    text_files = [f for f in os.listdir(EXTRACT_DIR) if f.endswith(\".txt\")]\n",
        "\n",
        "    print(f\" Text files found: {len(text_files)}\")\n",
        "\n",
        "    if not text_files:\n",
        "        print(\" No extracted text files found. Module 4 cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    for txt in text_files:\n",
        "        txt_path = os.path.join(EXTRACT_DIR, txt)\n",
        "        print(f\"\\n Generating summary for: {txt}\")\n",
        "\n",
        "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        if not text.strip():\n",
        "            print(\" Empty text. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        summary = simple_summary(text)\n",
        "\n",
        "        base = os.path.splitext(txt)[0]\n",
        "        summary_path = os.path.join(SUMMARY_DIR, base + \"_summary.txt\")\n",
        "        meta_path = os.path.join(SUMMARY_DIR, base + \"_summary_meta.json\")\n",
        "\n",
        "        with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(summary)\n",
        "\n",
        "        meta = {\n",
        "            \"source_text\": txt,\n",
        "            \"summary_method\": \"extractive_first_n_sentences\",\n",
        "            \"sentences_used\": 5,\n",
        "            \"generated_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "\n",
        "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, indent=4)\n",
        "\n",
        "        print(f\" Summary saved to: {summary_path}\")\n",
        "\n",
        "    print(\"\\n MODULE 4 COMPLETE!\")\n",
        "    print(f\" Summaries stored in: {SUMMARY_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AObbIA6NV-jG",
        "outputId": "c3bf5ef2-726f-4b06-aa8d-287d616e6a46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " MODULE 4: PAPER SUMMARIZATION\n",
            " Text files found: 2\n",
            "\n",
            " Generating summary for: Deep Learning_62071.txt\n",
            " Summary saved to: data/summaries/Deep Learning_62071_summary.txt\n",
            "\n",
            " Generating summary for: Attention Is All You Need_27546.txt\n",
            " Summary saved to: data/summaries/Attention Is All You Need_27546_summary.txt\n",
            "\n",
            " MODULE 4 COMPLETE!\n",
            " Summaries stored in: data/summaries\n"
          ]
        }
      ],
      "source": [
        "run_module_4()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdVSRMOVWiEx"
      },
      "source": [
        "Module 5:Knowledge Indexing & Question and answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-D16zNHXaIf"
      },
      "source": [
        "MODULE 6: TOPIC CLUSTERING & INSIGHTS GENERATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHowOzX1XgbK"
      },
      "source": [
        "1)Read summaries from Module 4\n",
        "\n",
        "2)Convert text into TF-IDF vectors\n",
        "\n",
        "3)Apply K-Means clustering\n",
        "\n",
        "4)Display top keywords per cluster\n",
        "\n",
        "5)Assign each paper to a cluster"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}