# -*- coding: utf-8 -*-
"""ai project milestone2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h-aSeNJ8XG9UK0JZq7P_b-jMV_LNoTpQ
"""

# ============================================
# MODULE 1: Topic Input & Paper Search (arXiv)
# ============================================

!pip install feedparser requests python-dotenv -q

import json
import os
import requests
import feedparser
from dotenv import load_dotenv

# ====================
# 1. SETUP
# ====================

def setup_arxiv():
    """Setup nothing, arXiv does not require authentication."""
    print("arXiv API initialized (no API key required)")
    return True


# ====================
# 2. PAPER SEARCH (arXiv)
# ====================

def search_papers(topic, limit=20):
    """
    Search arXiv for papers on a given topic.
    Returns: Dictionary with search results
    """

    print(f"\n Searching for papers on: '{topic}'")
    print(f"   Requesting {limit} papers from arXiv...")

    setup_arxiv()

    # Build arXiv API query
    base_url = "http://export.arxiv.org/api/query"
    query = f"search_query=all:{topic}&start=0&max_results={limit}"

    try:
        response = requests.get(f"{base_url}?{query}", timeout=20)

        if response.status_code != 200:
            print(" Error fetching arXiv results:", response.status_code)
            return None

        feed = feedparser.parse(response.text)

        papers = []

        for entry in feed.entries:

            pdf_link = None
            for link in entry.links:
                if link.rel == "alternate":
                    continue
                if link.type == "application/pdf":
                    pdf_link = link.href
                    break

            # Extract data
            paper_data = {
                "title": entry.title,
                "authors": [a.name for a in entry.authors],
                "year": entry.published[:4],
                "paperId": entry.id,                      # arXiv ID
                "abstract": entry.summary.replace("\n", " ").strip(),
                "citationCount": None,                    # arXiv does NOT provide citations
                "venue": "arXiv",
                "url": entry.id,
                "pdf_url": pdf_link,
                "has_pdf": pdf_link is not None
            }

            papers.append(paper_data)

        papers_with_pdf = sum(1 for p in papers if p["has_pdf"])

        print(" Search complete!")
        print(f"   Total papers found: {len(papers)}")
        print(f"   Papers with PDF available: {papers_with_pdf}")

        return {
            "topic": topic,
            "search_timestamp": "timestamp_placeholder",
            "total_results": len(papers),
            "papers_with_pdf": papers_with_pdf,
            "papers": papers
        }

    except Exception as e:
        print(f" Error searching arXiv: {e}")
        return None


# ====================
# 3. SAVE METADATA
# ====================

def save_search_results(data, filename=None):
    """
    Save search results to JSON file
    """
    if not filename:
        safe_topic = "".join(c for c in data["topic"] if c.isalnum() or c == " ").replace(" ", "_")
        filename = f"paper_search_results_{safe_topic}.json"

    os.makedirs("data/search_results", exist_ok=True)
    filepath = os.path.join("data/search_results", filename)

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

    print(f" Search results saved to: {filepath}")
    return filepath


# ====================
# 4. DISPLAY RESULTS
# ====================

def display_search_results(data, max_display=10):
    """
    Display search results in a readable format
    """
    if not data or "papers" not in data:
        print("No data to display")
        return

    papers = data["papers"]
    print("\n" + "="*80)
    print(f"SEARCH RESULTS: {data['topic']}")
    print("="*80)

    print(f"\nStatistics:")
    print(f"  • Total papers: {len(papers)}")
    print(f"  • Papers with PDF: {sum(1 for p in papers if p['has_pdf'])}")
    print(f"  • Papers without PDF: {sum(1 for p in papers if not p['has_pdf'])}")

    print(f"\n Top {min(max_display, len(papers))} Papers:")
    print("-"*80)

    for i, paper in enumerate(papers[:max_display]):
        print(f"\n{i+1}. {paper['title'][:80]}{'...' if len(paper['title']) > 80 else ''}")
        print(f"   Authors: {', '.join(paper['authors'][:3])}" +
              ("..." if len(paper['authors']) > 3 else ""))
        print(f"   Year: {paper['year']} | Citations: {paper['citationCount']}")
        print(f"   PDF Available: {'✅' if paper['has_pdf'] else '❌'}")
        print(f"   Abstract: {paper['abstract'][:100]}...")


# ====================
# 5. MAIN SEARCH FUNCTION
# ====================

def main_search():
    print("\n" + "="*80)
    print("MODULE 1: TOPIC INPUT & PAPER SEARCH (arXiv)")
    print("="*80)

    topic = input("\nEnter research topic: ").strip()
    if not topic:
        topic = "machine learning"

    results = search_papers(topic, limit=20)

    if results:
        save_path = save_search_results(results)
        display_search_results(results)

        print(f"\n Module 1 complete! Results saved to: {save_path}")
        print("   Proceed to Module 2 for paper selection and PDF download.")

        return results, save_path
    else:
        print(" No results found. Please try a different topic.")
        return None, None


if __name__ == "__main__":
    main_search()

# ============================================
# MODULE 2: Paper Selection & PDF Download (arXiv Version)
# ============================================

!pip install PyMuPDF requests -q

import json
import os
import requests
import fitz   # PyMuPDF
import hashlib
from datetime import datetime

# ============================================================
# 1. LOAD SEARCH RESULTS
# ============================================================

def load_search_results(filepath=None):
    """
    Load previously saved arXiv search results
    """
    if not filepath:
        results_dir = "data/search_results"
        if os.path.exists(results_dir):
            json_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
            if json_files:
                json_files.sort(key=lambda x: os.path.getmtime(os.path.join(results_dir, x)), reverse=True)
                filepath = os.path.join(results_dir, json_files[0])
                print(f" Loading latest search results: {json_files[0]}")
            else:
                print(" No search results found.")
                return None
        else:
            print(" Search results folder not found.")
            return None

    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)

        print(f" Loaded {len(data['papers'])} papers for topic '{data['topic']}'")
        return data

    except Exception as e:
        print(" Error loading:", e)
        return None


# ============================================================
# 2. FILTER & SELECT PAPERS
# ============================================================

def filter_papers_with_pdfs(papers):
    """
    arXiv always provides PDF links like:
    https://arxiv.org/pdf/XXXX.XXXX.pdf
    """
    papers_with_pdf = [p for p in papers if p.get("pdf_url")]

    print("\n PDF Availability:")
    print(f" • Total papers: {len(papers)}")
    print(f" • Papers with PDF: {len(papers_with_pdf)}")

    return papers_with_pdf


def rank_papers(papers):
    """
    Rank by:
    - newest year first
    - (optional future ranking: citations)
    """
    ranked = sorted(
        papers,
        key=lambda x: (x.get("year", 0)),
        reverse=True
    )
    return ranked


def select_top_papers(papers, count=3):
    papers_with_pdf = filter_papers_with_pdfs(papers)
    ranked = rank_papers(papers_with_pdf)

    selected = ranked[:count]

    print(f"\n Selected top {len(selected)} papers:")
    for i, p in enumerate(selected):
        print(f"\n{i+1}. {p['title'][:70]}...")
        print(f"   Year: {p['year']}")
        print(f"   Authors: {', '.join(p['authors'][:3])}")

    return selected


# ============================================================
# 3. DOWNLOAD PDF
# ============================================================

def download_pdf_with_verification(url, filename, max_retries=2):
    """
    Download PDF from arXiv with verification
    """
    headers = {"User-Agent": "Mozilla/5.0"}

    for attempt in range(max_retries):
        try:
            print(f"  Attempt {attempt+1}/{max_retries}")
            r = requests.get(url, headers=headers, timeout=30)

            if r.status_code != 200:
                print("   HTTP Error:", r.status_code)
                continue

            if not (r.content[:4] == b"%PDF"):
                print("   Not a PDF file")
                continue

            with open(filename, "wb") as f:
                f.write(r.content)

            if verify_pdf(filename):
                print("   PDF verified successfully.")
                return True
            else:
                print("   Invalid PDF, retrying...")
                os.remove(filename)

        except Exception as e:
            print("   Error:", e)

    return False


def verify_pdf(filepath):
    """
    Verify PDF integrity
    """
    try:
        if not os.path.exists(filepath):
            return False

        if os.path.getsize(filepath) < 2048:
            return False

        with fitz.open(filepath) as doc:
            return len(doc) > 0

    except:
        return False


def get_pdf_info(filepath):
    try:
        with fitz.open(filepath) as doc:
            return {
                "pages": len(doc),
                "size_mb": round(os.path.getsize(filepath) / (1024 * 1024), 2),
                "is_valid": True
            }
    except:
        return {"is_valid": False}


def download_selected_papers(selected, output_dir="downloads"):
    os.makedirs(output_dir, exist_ok=True)

    print("\n Starting Downloads...")
    print("-" * 60)

    downloaded = []

    for i, paper in enumerate(selected):
        print(f"\n[{i+1}/{len(selected)}] {paper['title'][:60]}...")

        safe_title = "".join(c for c in paper["title"] if c.isalnum() or c in " _-")
        safe_title = safe_title[:60]

        filename = f"{output_dir}/paper_{i+1}_{hashlib.md5(safe_title.encode()).hexdigest()[:8]}.pdf"

        success = download_pdf_with_verification(paper["pdf_url"], filename)

        if success:
            pdf_info = get_pdf_info(filename)
            paper["downloaded"] = True
            paper["local_path"] = filename
            paper["pdf_info"] = pdf_info
            paper["download_time"] = datetime.now().isoformat()

            downloaded.append(paper)
            print(f"   SUCCESS: {pdf_info['pages']} pages ({pdf_info['size_mb']} MB)")
        else:
            print("   FAILED to download")
            paper["downloaded"] = False

    return downloaded


# ============================================================
# 4. SAVE REPORT
# ============================================================

def save_download_report(downloaded, topic, output_dir="downloads"):
    report = {
        "topic": topic,
        "timestamp": datetime.now().isoformat(),
        "total_selected": len(downloaded),
        "successful": sum(1 for p in downloaded if p["downloaded"]),
        "failed": sum(1 for p in downloaded if not p["downloaded"]),
        "papers": downloaded
    }

    os.makedirs("data/reports", exist_ok=True)
    file = f"data/reports/download_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    with open(file, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=4)

    print(f"\n Report saved to: {file}")
    return file


# ============================================================
# 5. VERIFY DIRECTORY
# ============================================================

def verify_downloads(output_dir="downloads"):
    print("\n Verifying downloaded PDFs...")
    print("=" * 60)

    files = [f for f in os.listdir(output_dir) if f.endswith(".pdf")]
    valid = 0
    total_size = 0

    for f in files:
        path = os.path.join(output_dir, f)
        size = os.path.getsize(path)
        total_size += size

        if verify_pdf(path):
            valid += 1
            print(f" ✔ {f} ({size/1024/1024:.2f} MB)")
        else:
            print(f" ❌ INVALID: {f}")

    print("\n Summary:")
    print(f" • PDFs found: {len(files)}")
    print(f" • Valid PDFs: {valid}")
    print(f" • Total Size: {total_size/1024/1024:.2f} MB")

    return valid


# ============================================================
# 6. MAIN DRIVER
# ============================================================

def main_download(filepath=None, download_count=3):
    print("\n" + "="*80)
    print(" MODULE 2 — arXiv PDF DOWNLOAD")
    print("="*80)

    data = load_search_results(filepath)
    if not data:
        return None

    selected = select_top_papers(data["papers"], count=download_count)

    downloaded = download_selected_papers(selected)

    report_file = save_download_report(downloaded, data["topic"])

    verify_downloads()

    print("\n Module 2 Complete!")
    print(" Downloaded PDFs stored in: downloads/")
    print(" Report:", report_file)

    return downloaded


# Allow direct execution
if __name__ == "__main__":
    main_download(download_count=3)

!pip install PyMuPDF pymupdf4llm tqdm -q

print("Libraries PyMuPDF, pymupdf4llm, and tqdm installed successfully."

import sys

!{sys.executable} -m pip install PyMuPDF pymupdf4llm tqdm -q

print("Libraries PyMuPDF, pymupdf4llm, and tqdm installed successfully.")

import fitz # PyMuPDF
import pymupdf4llm
import os
import re
import json
from tqdm.auto import tqdm
from datetime import datetime

# ============================================================
# MODULE 3: PDF Parsing & Text Extraction
# ============================================================

def extract_text_from_pdf(pdf_path):
    """
    Extracts text from a PDF document using pymupdf4llm, with a fallback to PyMuPDF.
    Returns a dictionary with 'full_text' and 'pages' (list of page texts).
    """
    full_text = []
    pages = []
    try:
        # Try pymupdf4llm for robust extraction
        doc_text = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)
        for i, page_chunk in enumerate(doc_text):
            text = page_chunk['text'] # Corrected: access 'text' as a dictionary key
            pages.append(text)
            full_text.append(text)
        print(f"  [SUCCESS] Extracted {len(pages)} pages using pymupdf4llm.")
    except Exception as e_llm:
        print(f"  [WARNING] pymupdf4llm extraction failed: {e_llm}. Falling back to PyMuPDF.")
        try:
            # Fallback to standard PyMuPDF extraction if pymupdf4llm fails
            with fitz.open(pdf_path) as doc:
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)
                    text = page.get_text("text")
                    pages.append(text)
                    full_text.append(text)
            print(f"  [SUCCESS] Extracted {len(pages)} pages using PyMuPDF fallback.")
        except Exception as e_fitz:
            print(f"  [ERROR] PyMuPDF fallback extraction also failed: {e_fitz}")
            return {"full_text": "", "pages": []}

    return {"full_text": "\n\n".join(full_text), "pages": pages}

# ============================================================
# Text Cleaning
# ============================================================

def normalize_text(text):
    """
    Cleans and normalizes text by removing extra whitespace and special characters.
    """
    if not isinstance(text, str):
        return ""
    text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces/newlines with single space
    text = re.sub(r'[^a-zA-Z0-9.,\-:\s]', '', text) # Keep basic punctuation, letters, numbers
    text = text.strip()
    return text

# ============================================================
# Section Extraction
# ============================================================

SECTION_PATTERNS = {
    "abstract": r"^\s*(?:abstract|summary|synopsis|Abstract)\b",
    "introduction": r"^\s*(?:\d+\.?\s*)?(?:introduction|intro|I. INTRODUCTION)\b",
    "methods": r"^\s*(?:\d+\.?\s*)?(?:method|methodology|experiment|experimental design|experiments|experimental setup|II. METHODOLOGY|METHODOLOGY)\b",
    "results": r"^\s*(?:\d+\.?\s*)?(?:results|findings|experimental results|III. RESULTS)\b",
    "discussion": r"^\s*(?:\d+\.?\s*)?(?:discussion|IV. DISCUSSION)\b",
    "conclusion": r"^\s*(?:\d+\.?\s*)?(?:conclusion|conclusions|summary and conclusion|future work|V. CONCLUSION)\b",
    "references": r"^\s*(?:\d+\.?\s*)?(?:references|bibliography|works cited|acknowledgements|REFERENCES)\b",
    "keywords": r"^\s*(?:keywords|key words)\b"
}

def extract_sections(text, section_patterns=SECTION_PATTERNS, min_len=100):
    """
    Extracts sections from the text using regex patterns and a minimum length heuristic.
    Improved to handle common structural variations and ensure greedy extraction.
    More robust abstract extraction by considering text before the first major section.
    """
    sections = {}

    # Identify all section headers and their start/end positions
    header_matches = []
    for name, pattern_str in section_patterns.items():
        pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
        for match in pattern.finditer(text):
            header_matches.append((match.start(), match.end(), name))

    header_matches.sort(key=lambda x: x[0])

    # Add a sentinel for the end of the document to simplify content slicing
    processed_matches = []
    if header_matches:
        processed_matches.extend(header_matches)
        processed_matches.append((len(text), len(text), "END_DOC"))
    else: # No headers found, treat entire text as potentially preamble or single section
        if len(normalize_text(text)) > min_len:
            initial_paragraphs = "\n\n".join(text.split("\n\n")[:3])
            if len(normalize_text(initial_paragraphs)) > min_len/2 and re.search(r"abstract|summary", initial_paragraphs, re.IGNORECASE):
                sections["abstract"] = initial_paragraphs
            else:
                sections["full_text_as_section"] = text
        return sections

    # Process segments between identified headers
    for i in range(len(processed_matches) - 1):
        start_current_header, end_current_header, current_section_name = processed_matches[i]
        start_next_header = processed_matches[i+1][0]

        content = text[end_current_header:start_next_header].strip()

        if len(normalize_text(content)) > min_len:
            if current_section_name not in sections:
                sections[current_section_name] = content

    # Special handling for preamble before the first identified header (likely contains abstract)
    if processed_matches[0][0] > 0: # If there's text before the very first matched header
        preamble_text = text[0:processed_matches[0][0]].strip()
        normalized_preamble = normalize_text(preamble_text)

        if len(normalized_preamble) > min_len/2: # Preamble could contain abstract, which can be shorter
            # Try to infer abstract from preamble if 'abstract' isn't explicitly found in sections
            if 'abstract' not in sections:
                # Check for keywords 'abstract' or 'summary' in the preamble
                if re.search(r"abstract|summary", normalized_preamble, re.IGNORECASE):
                    sections['abstract'] = preamble_text
                # Fallback: if 'introduction' is the very first section found and abstract is still missing,
                # assume the preamble is the abstract if it's not excessively long and contains substantial text.
                elif processed_matches[0][2] == 'introduction' and len(normalized_preamble.split('\n\n')) <= 5:
                    sections['abstract'] = preamble_text

    return sections

# ============================================================
# Key Findings Extraction
# ============================================================
KEY_PHRASES = [
    "we propose", "we present", "our approach", "results show",
    "experimental results", "outperforms", "significant improvement",
    "we demonstrate", "our method", "key findings", "conclude that",
    "contributions include", "novel framework", "achieved state-of-the-art",
    "limitations include", "future work involves", "implications of this study",
    "architecture", "framework", "system", "implementation", "pipeline", "case study"
]

def extract_key_findings(text, phrases=KEY_PHRASES, window_size=200):
    """
    Extracts sentences containing key phrases from the text.
    """
    findings = []
    normalized_text = normalize_text(text)

    for phrase in phrases:
        for match in re.finditer(re.escape(phrase), normalized_text, re.IGNORECASE):
            start_index = max(0, match.start() - window_size)
            end_index = min(len(normalized_text), match.end() + window_size)
            context = normalized_text[start_index:end_index]

            sentence_match = re.search(r"[^.!?]*" + re.escape(phrase) + r"[^.!?]*[.!?]", context, re.IGNORECASE)
            if sentence_match:
                finding = sentence_match.group(0).strip()
            else:
                finding = context.strip()

            if finding and finding not in findings:
                findings.append(finding)
    return findings

# ============================================================
# Validation Module
# ============================================================
def validate_extraction(paper_data):
    """
    Validates that essential information has been extracted for a given paper.
    """
    is_valid = True
    missing_components = []

    if not paper_data.get("extracted_text", {}).get("full_text"):
        is_valid = False
        missing_components.append("full_text")
    if not paper_data.get("extracted_sections", {}).get("abstract"):
        is_valid = False
        missing_components.append("abstract_section")
    if not paper_data.get("key_findings"):
        is_valid = False
        missing_components.append("key_findings")

    if not is_valid:
        print(f"  [VALIDATION WARNING] Paper '{paper_data.get('title', 'N/A')}' is missing: {', '.join(missing_components)}")

    return is_valid

# ============================================================
# Cross-Paper Comparison
# ============================================================
def cross_paper_analysis(processed_papers):
    """
    Performs a simple cross-paper analysis to find common themes or compare findings.
    """
    analysis_results = {
        "common_themes": [],
        "summary_by_paper": []
    }

    all_findings_text = ""
    for paper in processed_papers:
        paper_summary = {
            "title": paper.get("title"),
            "paperId": paper.get("paperId"),
            "abstract_summary": "",
            "key_findings_summary": []
        }

        if "extracted_sections" in paper and "abstract" in paper["extracted_sections"]:
            abstract_content = paper["extracted_sections"]["abstract"]
            sentences = re.split(r'(?<=[.!?])\s+', abstract_content)
            paper_summary["abstract_summary"] = " ".join(sentences[:2]) + "..."

        if "key_findings" in paper:
            paper_summary["key_findings_summary"] = paper["key_findings"]
            all_findings_text += " ".join(paper["key_findings"]) + " "

        analysis_results["summary_by_paper"].append(paper_summary)

    if all_findings_text:
        words = re.findall(r'\b\w+\b', all_findings_text.lower())
        word_counts = {}
        for word in words:
            if len(word) > 3:
                word_counts[word] = word_counts.get(word, 0) + 1

        sorted_words = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)
        common_words = [word for word, count in sorted_words[:5]]
        analysis_results["common_themes"] = common_words

    print("Performed cross-paper analysis.")
    return analysis_results

# ============================================================
# Main Driver for Module 3
# ============================================================
def run_milestone_2(report_filepath=None, output_filename="final_results.json"):
    """
    Orchestrates the entire Module 3 process: PDF parsing, text extraction,
    sectioning, key finding extraction, and cross-paper analysis.
    """
    print("\n" + "="*80)
    print("MODULE 3: PDF PARSING & TEXT ANALYSIS")
    print("="*80)

    if not report_filepath:
        reports_dir = "data/reports"
        if os.path.exists(reports_dir):
            json_files = [f for f in os.listdir(reports_dir) if f.startswith('download_report_') and f.endswith('.json')]
            if json_files:
                json_files.sort(key=lambda x: os.path.getmtime(os.path.join(reports_dir, x)), reverse=True)
                report_filepath = os.path.join(reports_dir, json_files[0])
                print(f" Loading latest download report: {json_files[0]}")
            else:
                print(" No download reports found in data/reports.")
                return
        else:
            print(" Download reports folder 'data/reports' not found.")
            return

    try:
        with open(report_filepath, "r", encoding="utf-8") as f:
            download_report = json.load(f)
        downloaded_papers = download_report.get("papers", [])
        topic = download_report.get("topic", "")
        print(f" Successfully loaded {len(downloaded_papers)} downloaded papers for topic '{topic}'.")

    except Exception as e:
        print(f" [ERROR] Failed to load download report from {report_filepath}: {e}")
        return

    processed_papers = []
    print("\n Starting text extraction and analysis for each paper...")
    print("-"*80)

    for i, paper in tqdm(enumerate(downloaded_papers), total=len(downloaded_papers), desc="Processing Papers"):
        if not paper.get("downloaded") or not paper.get("local_path"):
            print(f"  [SKIP] Paper {i+1} '{paper.get('title', 'N/A')}' was not downloaded or path is missing.")
            continue

        pdf_path = paper["local_path"]
        print(f"\n [{i+1}/{len(downloaded_papers)}] Processing: {paper['title'][:70]}...")

        extracted_text = extract_text_from_pdf(pdf_path)
        paper["extracted_text"] = extracted_text

        if not extracted_text["full_text"]:
            print("  [WARNING] Could not extract any text from PDF.")
            paper["analysis_status"] = "failed_text_extraction"
            processed_papers.append(paper)
            continue

        normalized_full_text = normalize_text(extracted_text["full_text"])
        paper["normalized_full_text"] = normalized_full_text

        sections = extract_sections(normalized_full_text)
        paper["extracted_sections"] = sections

        key_findings_source = sections.get("abstract", "") + "\n\n" + \
                              sections.get("introduction", "") + "\n\n" + \
                              sections.get("results", "") + "\n\n" + \
                              sections.get("discussion", "") + "\n\n" + \
                              sections.get("conclusion", normalized_full_text)
        key_findings = extract_key_findings(key_findings_source)
        paper["key_findings"] = key_findings

        if not key_findings and sections.get("abstract"): # Fallback to abstract if no key findings were extracted
            abstract_sentences = re.split(r'(?<=[.!?])\s+', sections["abstract"])
            fallback_findings = [s.strip() for s in abstract_sentences[:3] if s.strip()] # Take first 3 sentences
            if fallback_findings:
                paper["key_findings"] = fallback_findings
                print("  [INFO] Fallback: Extracted key findings from abstract.")

        if validate_extraction(paper):
            paper["analysis_status"] = "success"
        else:
            paper["analysis_status"] = "partial_success_or_failure"

        processed_papers.append(paper)

    print("\n" + "="*80)
    print("Performing Cross-Paper Analysis...")
    print("="*80)
    analysis_output = cross_paper_analysis(processed_papers)

    final_results = {
        "topic": topic,
        "timestamp": datetime.now().isoformat(),
        "total_processed_papers": len(processed_papers),
        "cross_paper_analysis": analysis_output,
        "papers": processed_papers
    }

    os.makedirs("data/processed_results", exist_ok=True)
    output_filepath = os.path.join("data/processed_results", output_filename)
    with open(output_filepath, "w", encoding="utf-8") as f:
        json.dump(final_results, f, indent=4, ensure_ascii=False)

    print(f"\n Module 3 Complete! All analysis results saved to: {output_filepath}")
    return final_results

final_data = run_milestone_2()

print("Final analysis complete. Review 'data/processed_results/final_results.json' for details.")