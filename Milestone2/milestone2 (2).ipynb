{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQOg7FH0VZRL",
        "outputId": "d5d385eb-ffdf-4da1-92f4-e910a2b439c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing PyMuPDF...\n",
            "✓ PyMuPDF installed successfully\n",
            "\n",
            "================================================================================\n",
            "MODULE 3: TEXT EXTRACTION & ANALYSIS\n",
            "================================================================================\n",
            "\n",
            " Error: Directory 'downloads' not found!\n",
            "  Please run Module 2 first to download papers.\n",
            "\n",
            " No papers were successfully processed.\n"
          ]
        }
      ],
      "source": [
        "#Required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install required packages if not already installed\"\"\"\n",
        "    try:\n",
        "        import fitz\n",
        "        print(\"✓ PyMuPDF already installed\")\n",
        "    except ImportError:\n",
        "        print(\"Installing PyMuPDF...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"PyMuPDF\", \"-q\"])\n",
        "        print(\"✓ PyMuPDF installed successfully\")\n",
        "\n",
        "# Install dependencies\n",
        "install_requirements()\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "import hashlib\n",
        "\n",
        "# ====================\n",
        "# 1. PDF TEXT EXTRACTION\n",
        "# ====================\n",
        "\n",
        "class PDFTextExtractor:\n",
        "    \"\"\"\n",
        "    Handles extraction of text from PDF files with cleaning and preprocessing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pdf_path):\n",
        "        self.pdf_path = pdf_path\n",
        "        self.doc = None\n",
        "        self.raw_text = \"\"\n",
        "        self.cleaned_text = \"\"\n",
        "        self.metadata = {}\n",
        "\n",
        "    def open_pdf(self):\n",
        "        \"\"\"Open PDF file and extract metadata\"\"\"\n",
        "        try:\n",
        "            self.doc = fitz.open(self.pdf_path)\n",
        "            self.metadata = {\n",
        "                'page_count': len(self.doc),\n",
        "                'file_size_mb': os.path.getsize(self.pdf_path) / (1024 * 1024),\n",
        "                'file_name': os.path.basename(self.pdf_path)\n",
        "            }\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error opening PDF: {e}\")\n",
        "            return False\n",
        "\n",
        "    def extract_raw_text(self):\n",
        "        \"\"\"Extract raw text from all pages\"\"\"\n",
        "        if not self.doc:\n",
        "            return \"\"\n",
        "\n",
        "        text_blocks = []\n",
        "        for page_num in range(len(self.doc)):\n",
        "            page = self.doc[page_num]\n",
        "            text = page.get_text(\"text\")\n",
        "            text_blocks.append(text)\n",
        "\n",
        "        self.raw_text = \"\\n\".join(text_blocks)\n",
        "        return self.raw_text\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        Clean and preprocess extracted text\n",
        "        - Remove excessive whitespace\n",
        "        - Fix hyphenation\n",
        "        - Remove page numbers and headers/footers\n",
        "        - Normalize line breaks\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove page numbers (common patterns)\n",
        "        text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)\n",
        "        text = re.sub(r'\\n\\s*Page\\s+\\d+\\s*\\n', '\\n', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Fix hyphenation at line breaks\n",
        "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
        "        text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "        # Remove header/footer patterns (common patterns)\n",
        "        text = re.sub(r'\\n[A-Z][a-z\\s]+\\d{4}\\n', '\\n', text)\n",
        "\n",
        "        # Normalize line breaks for better readability\n",
        "        lines = text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                # If line is very short and next line exists, might be continuation\n",
        "                if len(line) < 80 and i + 1 < len(lines) and lines[i + 1].strip():\n",
        "                    next_line = lines[i + 1].strip()\n",
        "                    # Check if it looks like a continuation (starts with lowercase)\n",
        "                    if next_line and next_line[0].islower():\n",
        "                        line = line + ' '\n",
        "                cleaned_lines.append(line)\n",
        "\n",
        "        self.cleaned_text = '\\n'.join(cleaned_lines)\n",
        "        return self.cleaned_text\n",
        "\n",
        "    def extract_and_clean(self):\n",
        "        \"\"\"Main method: extract and clean text\"\"\"\n",
        "        if not self.open_pdf():\n",
        "            return None\n",
        "\n",
        "        self.extract_raw_text()\n",
        "        self.clean_text(self.raw_text)\n",
        "\n",
        "        return {\n",
        "            'raw_text': self.raw_text,\n",
        "            'cleaned_text': self.cleaned_text,\n",
        "            'metadata': self.metadata,\n",
        "            'char_count': len(self.cleaned_text),\n",
        "            'word_count': len(self.cleaned_text.split())\n",
        "        }\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close PDF document\"\"\"\n",
        "        if self.doc:\n",
        "            self.doc.close()\n",
        "\n",
        "# ====================\n",
        "# 2. SECTION DETECTION & EXTRACTION\n",
        "# ====================\n",
        "\n",
        "class SectionExtractor:\n",
        "    \"\"\"\n",
        "    Detects and extracts sections from research papers\n",
        "    \"\"\"\n",
        "\n",
        "    # Common section headers in research papers\n",
        "    SECTION_PATTERNS = [\n",
        "        r'^\\s*abstract\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*introduction\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*related\\s+work\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*background\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*literature\\s+review\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*methodology\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*methods\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*approach\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*experiments?\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*results?\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*evaluation\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*discussion\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*conclusion\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*future\\s+work\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*references?\\s*$',\n",
        "        r'^\\s*\\d*\\.?\\s*bibliography\\s*$',\n",
        "    ]\n",
        "\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "        self.sections = {}\n",
        "\n",
        "    def detect_sections(self):\n",
        "        \"\"\"\n",
        "        Detect section headers and boundaries in the text\n",
        "        \"\"\"\n",
        "        lines = self.text.split('\\n')\n",
        "        section_markers = []\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line_lower = line.lower().strip()\n",
        "\n",
        "            # Check against patterns\n",
        "            for pattern in self.SECTION_PATTERNS:\n",
        "                if re.match(pattern, line_lower):\n",
        "                    # Extract section name\n",
        "                    section_name = re.sub(r'^\\d+\\.?\\s*', '', line_lower).strip()\n",
        "                    section_markers.append({\n",
        "                        'line_num': i,\n",
        "                        'name': section_name,\n",
        "                        'original': line.strip()\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "        return section_markers\n",
        "\n",
        "    def extract_sections(self):\n",
        "        \"\"\"\n",
        "        Extract text for each detected section\n",
        "        \"\"\"\n",
        "        markers = self.detect_sections()\n",
        "\n",
        "        if not markers:\n",
        "            # If no sections found, return entire text as 'full_text'\n",
        "            self.sections = {'full_text': self.text}\n",
        "            return self.sections\n",
        "\n",
        "        lines = self.text.split('\\n')\n",
        "\n",
        "        for i, marker in enumerate(markers):\n",
        "            start_line = marker['line_num'] + 1  # Start after header\n",
        "\n",
        "            # End is either next section or end of document\n",
        "            if i + 1 < len(markers):\n",
        "                end_line = markers[i + 1]['line_num']\n",
        "            else:\n",
        "                end_line = len(lines)\n",
        "\n",
        "            # Extract section text\n",
        "            section_lines = lines[start_line:end_line]\n",
        "            section_text = '\\n'.join(section_lines).strip()\n",
        "\n",
        "            # Store section\n",
        "            section_name = marker['name']\n",
        "            self.sections[section_name] = section_text\n",
        "\n",
        "        return self.sections\n",
        "\n",
        "    def get_section(self, section_name):\n",
        "        \"\"\"Get specific section by name\"\"\"\n",
        "        section_name = section_name.lower().strip()\n",
        "        return self.sections.get(section_name, \"\")\n",
        "\n",
        "    def get_section_summary(self):\n",
        "        \"\"\"Get summary of detected sections\"\"\"\n",
        "        summary = {}\n",
        "        for name, text in self.sections.items():\n",
        "            summary[name] = {\n",
        "                'char_count': len(text),\n",
        "                'word_count': len(text.split()),\n",
        "                'line_count': len(text.split('\\n'))\n",
        "            }\n",
        "        return summary\n",
        "\n",
        "# ====================\n",
        "# 3. KEY FINDINGS EXTRACTION\n",
        "# ====================\n",
        "\n",
        "class KeyFindingsExtractor:\n",
        "    \"\"\"\n",
        "    Extracts key findings, contributions, and insights from papers\n",
        "    \"\"\"\n",
        "\n",
        "    # Patterns that often indicate key findings\n",
        "    FINDING_INDICATORS = [\n",
        "        r'we\\s+(demonstrate|show|find|observe|report|achieve|present|propose|introduce)',\n",
        "        r'our\\s+(results|findings|experiments|approach|method|work)\\s+(show|demonstrate|indicate|suggest|reveal)',\n",
        "        r'(significant|substantial|notable|important)\\s+(improvement|increase|decrease|reduction|performance)',\n",
        "        r'(outperforms?|better\\s+than|superior\\s+to|achieves?\\s+state-of-the-art)',\n",
        "        r'(contributes?|contribution|novel|novelty)',\n",
        "        r'(in\\s+conclusion|to\\s+summarize|in\\s+summary)',\n",
        "    ]\n",
        "\n",
        "    def __init__(self, sections_dict):\n",
        "        self.sections = sections_dict\n",
        "        self.findings = []\n",
        "\n",
        "    def extract_from_abstract(self):\n",
        "        \"\"\"Extract findings from abstract\"\"\"\n",
        "        abstract = self.sections.get('abstract', '')\n",
        "        if not abstract:\n",
        "            return []\n",
        "\n",
        "        # Split into sentences\n",
        "        sentences = re.split(r'[.!?]+', abstract)\n",
        "        findings = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if len(sentence.split()) < 5:  # Skip very short sentences\n",
        "                continue\n",
        "\n",
        "            # Check if sentence contains finding indicators\n",
        "            sentence_lower = sentence.lower()\n",
        "            for pattern in self.FINDING_INDICATORS:\n",
        "                if re.search(pattern, sentence_lower):\n",
        "                    findings.append({\n",
        "                        'text': sentence,\n",
        "                        'source': 'abstract',\n",
        "                        'type': 'key_finding'\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "        return findings\n",
        "\n",
        "    def extract_from_conclusion(self):\n",
        "        \"\"\"Extract findings from conclusion\"\"\"\n",
        "        conclusion = self.sections.get('conclusion', '')\n",
        "        if not conclusion:\n",
        "            return []\n",
        "\n",
        "        sentences = re.split(r'[.!?]+', conclusion)\n",
        "        findings = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if len(sentence.split()) < 5:\n",
        "                continue\n",
        "\n",
        "            sentence_lower = sentence.lower()\n",
        "            for pattern in self.FINDING_INDICATORS:\n",
        "                if re.search(pattern, sentence_lower):\n",
        "                    findings.append({\n",
        "                        'text': sentence,\n",
        "                        'source': 'conclusion',\n",
        "                        'type': 'key_finding'\n",
        "                    })\n",
        "                    break\n",
        "\n",
        "        return findings\n",
        "\n",
        "    def extract_numerical_results(self):\n",
        "        \"\"\"Extract sentences with numerical results/metrics\"\"\"\n",
        "        results_section = self.sections.get('results', '') or self.sections.get('experiments', '')\n",
        "        if not results_section:\n",
        "            return []\n",
        "\n",
        "        # Pattern for percentages, decimals, metrics\n",
        "        number_pattern = r'\\d+\\.?\\d*\\s*%|\\d+\\.?\\d*'\n",
        "\n",
        "        sentences = re.split(r'[.!?]+', results_section)\n",
        "        numerical_findings = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if len(sentence.split()) < 5:\n",
        "                continue\n",
        "\n",
        "            # Check if contains numbers\n",
        "            if re.search(number_pattern, sentence):\n",
        "                numerical_findings.append({\n",
        "                    'text': sentence,\n",
        "                    'source': 'results',\n",
        "                    'type': 'numerical_result'\n",
        "                })\n",
        "\n",
        "        return numerical_findings[:10]  # Limit to top 10\n",
        "\n",
        "    def extract_all_findings(self):\n",
        "        \"\"\"Extract all types of findings\"\"\"\n",
        "        self.findings = []\n",
        "\n",
        "        # Extract from different sections\n",
        "        self.findings.extend(self.extract_from_abstract())\n",
        "        self.findings.extend(self.extract_from_conclusion())\n",
        "        self.findings.extend(self.extract_numerical_results())\n",
        "\n",
        "        return self.findings\n",
        "\n",
        "    def get_top_findings(self, n=5):\n",
        "        \"\"\"Get top N findings\"\"\"\n",
        "        if not self.findings:\n",
        "            self.extract_all_findings()\n",
        "\n",
        "        # Prioritize: abstract > conclusion > numerical results\n",
        "        priority = {'abstract': 3, 'conclusion': 2, 'results': 1}\n",
        "\n",
        "        sorted_findings = sorted(\n",
        "            self.findings,\n",
        "            key=lambda x: (priority.get(x['source'], 0), len(x['text'].split())),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        return sorted_findings[:n]\n",
        "\n",
        "# ====================\n",
        "# 4. CROSS-PAPER COMPARISON\n",
        "# ====================\n",
        "\n",
        "class CrossPaperAnalyzer:\n",
        "    \"\"\"\n",
        "    Compares and analyzes multiple papers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, papers_data):\n",
        "        \"\"\"\n",
        "        papers_data: list of dicts with 'text', 'sections', 'findings', 'metadata'\n",
        "        \"\"\"\n",
        "        self.papers = papers_data\n",
        "\n",
        "    def extract_keywords(self, text, top_n=20):\n",
        "        \"\"\"Extract most common keywords from text\"\"\"\n",
        "        # Remove common stop words\n",
        "        stop_words = {\n",
        "            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
        "            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',\n",
        "            'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',\n",
        "            'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these',\n",
        "            'those', 'we', 'our', 'they', 'their', 'it', 'its'\n",
        "        }\n",
        "\n",
        "        # Extract words\n",
        "        words = re.findall(r'\\b[a-z]{4,}\\b', text.lower())\n",
        "\n",
        "        # Filter and count\n",
        "        filtered_words = [w for w in words if w not in stop_words]\n",
        "        word_counts = Counter(filtered_words)\n",
        "\n",
        "        return word_counts.most_common(top_n)\n",
        "\n",
        "    def find_common_themes(self):\n",
        "        \"\"\"Find common themes across papers\"\"\"\n",
        "        all_keywords = []\n",
        "\n",
        "        for paper in self.papers:\n",
        "            text = paper.get('cleaned_text', '')\n",
        "            keywords = self.extract_keywords(text, top_n=30)\n",
        "            all_keywords.extend([kw[0] for kw in keywords])\n",
        "\n",
        "        # Find keywords that appear in multiple papers\n",
        "        keyword_counts = Counter(all_keywords)\n",
        "        common_themes = [kw for kw, count in keyword_counts.items() if count >= len(self.papers) * 0.5]\n",
        "\n",
        "        return common_themes[:15]\n",
        "\n",
        "    def compare_methodologies(self):\n",
        "        \"\"\"Compare methodologies mentioned in papers\"\"\"\n",
        "        methodology_keywords = [\n",
        "            'neural network', 'deep learning', 'machine learning', 'algorithm',\n",
        "            'model', 'training', 'dataset', 'evaluation', 'baseline', 'transformer',\n",
        "            'attention', 'supervised', 'unsupervised', 'reinforcement'\n",
        "        ]\n",
        "\n",
        "        comparison = {}\n",
        "\n",
        "        for i, paper in enumerate(self.papers):\n",
        "            paper_name = paper.get('metadata', {}).get('file_name', f'Paper {i+1}')\n",
        "            methods_found = []\n",
        "\n",
        "            text_lower = paper.get('cleaned_text', '').lower()\n",
        "            for method in methodology_keywords:\n",
        "                if method in text_lower:\n",
        "                    # Count occurrences\n",
        "                    count = text_lower.count(method)\n",
        "                    methods_found.append((method, count))\n",
        "\n",
        "            comparison[paper_name] = sorted(methods_found, key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "        return comparison\n",
        "\n",
        "    def generate_comparison_report(self):\n",
        "        \"\"\"Generate comprehensive comparison report\"\"\"\n",
        "        report = {\n",
        "            'total_papers': len(self.papers),\n",
        "            'common_themes': self.find_common_themes(),\n",
        "            'methodology_comparison': self.compare_methodologies(),\n",
        "            'paper_summaries': []\n",
        "        }\n",
        "\n",
        "        for i, paper in enumerate(self.papers):\n",
        "            summary = {\n",
        "                'paper_id': i + 1,\n",
        "                'file_name': paper.get('metadata', {}).get('file_name', f'Paper {i+1}'),\n",
        "                'word_count': paper.get('word_count', 0),\n",
        "                'sections_found': list(paper.get('sections', {}).keys()),\n",
        "                'top_keywords': self.extract_keywords(paper.get('cleaned_text', ''), top_n=10),\n",
        "                'key_findings_count': len(paper.get('findings', []))\n",
        "            }\n",
        "            report['paper_summaries'].append(summary)\n",
        "\n",
        "        return report\n",
        "\n",
        "# ====================\n",
        "# 5. STORAGE & SAVING\n",
        "# ====================\n",
        "\n",
        "def save_extracted_data(paper_data, output_dir=\"data/extracted\"):\n",
        "    \"\"\"Save extracted text and analysis for a single paper\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Create filename based on original PDF\n",
        "    pdf_name = paper_data.get('metadata', {}).get('file_name', 'unknown.pdf')\n",
        "    base_name = os.path.splitext(pdf_name)[0]\n",
        "\n",
        "    # Save structured data as JSON\n",
        "    json_file = os.path.join(output_dir, f\"{base_name}_extracted.json\")\n",
        "    with open(json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(paper_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    # Save cleaned text separately\n",
        "    text_file = os.path.join(output_dir, f\"{base_name}_cleaned.txt\")\n",
        "    with open(text_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(paper_data.get('cleaned_text', ''))\n",
        "\n",
        "    return json_file, text_file\n",
        "\n",
        "def save_comparison_report(comparison_data, output_dir=\"data/reports\"):\n",
        "    \"\"\"Save cross-paper comparison report\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    report_file = os.path.join(output_dir, f\"comparison_report_{timestamp}.json\")\n",
        "\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(comparison_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return report_file\n",
        "\n",
        "# ====================\n",
        "# 6. MAIN PROCESSING PIPELINE\n",
        "# ====================\n",
        "\n",
        "def process_single_paper(pdf_path):\n",
        "    \"\"\"Process a single paper: extract, clean, analyze\"\"\"\n",
        "    print(f\"\\nProcessing: {os.path.basename(pdf_path)}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # 1. Extract text\n",
        "    print(\"  [1/4] Extracting text...\")\n",
        "    extractor = PDFTextExtractor(pdf_path)\n",
        "    extraction_result = extractor.extract_and_clean()\n",
        "    extractor.close()\n",
        "\n",
        "    if not extraction_result:\n",
        "        print(\"  Failed to extract text!\")\n",
        "        return None\n",
        "\n",
        "    print(f\"    Extracted {extraction_result['word_count']:,} words\")\n",
        "\n",
        "    # 2. Detect sections\n",
        "    print(\"  [2/4] Detecting sections...\")\n",
        "    section_extractor = SectionExtractor(extraction_result['cleaned_text'])\n",
        "    sections = section_extractor.extract_sections()\n",
        "    section_summary = section_extractor.get_section_summary()\n",
        "\n",
        "    print(f\"    Found {len(sections)} sections: {', '.join(sections.keys())}\")\n",
        "\n",
        "    # 3. Extract findings\n",
        "    print(\"  [3/4] Extracting key findings...\")\n",
        "    findings_extractor = KeyFindingsExtractor(sections)\n",
        "    findings = findings_extractor.extract_all_findings()\n",
        "    top_findings = findings_extractor.get_top_findings(n=5)\n",
        "\n",
        "    print(f\"    Extracted {len(findings)} findings\")\n",
        "\n",
        "    # 4. Compile results\n",
        "    print(\"  [4/4] Compiling results...\")\n",
        "    paper_data = {\n",
        "        'metadata': extraction_result['metadata'],\n",
        "        'raw_text': extraction_result['raw_text'][:1000] + '...',  # Truncate for storage\n",
        "        'cleaned_text': extraction_result['cleaned_text'],\n",
        "        'char_count': extraction_result['char_count'],\n",
        "        'word_count': extraction_result['word_count'],\n",
        "        'sections': sections,\n",
        "        'section_summary': section_summary,\n",
        "        'findings': findings,\n",
        "        'top_findings': top_findings,\n",
        "        'processing_timestamp': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    print(\"  ✓ Complete!\")\n",
        "    return paper_data\n",
        "\n",
        "def process_all_papers(download_dir=\"downloads\", output_dir=\"data/extracted\"):\n",
        "    \"\"\"Process all papers in download directory\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"MODULE 3: TEXT EXTRACTION & ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Find all PDFs\n",
        "    if not os.path.exists(download_dir):\n",
        "        print(f\"\\n Error: Directory '{download_dir}' not found!\")\n",
        "        print(\"  Please run Module 2 first to download papers.\")\n",
        "        return None\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(download_dir) if f.endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"\\n No PDF files found in '{download_dir}'\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n Found {len(pdf_files)} PDF files to process\")\n",
        "\n",
        "    # Process each paper\n",
        "    processed_papers = []\n",
        "\n",
        "    for i, pdf_file in enumerate(pdf_files):\n",
        "        pdf_path = os.path.join(download_dir, pdf_file)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Paper {i+1}/{len(pdf_files)}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        paper_data = process_single_paper(pdf_path)\n",
        "\n",
        "        if paper_data:\n",
        "            # Save individual paper data\n",
        "            json_file, text_file = save_extracted_data(paper_data, output_dir)\n",
        "            print(f\"\\n  Saved to:\")\n",
        "            print(f\"    JSON: {json_file}\")\n",
        "            print(f\"    Text: {text_file}\")\n",
        "\n",
        "            processed_papers.append(paper_data)\n",
        "        else:\n",
        "            print(f\"  Failed to process {pdf_file}\")\n",
        "\n",
        "    return processed_papers\n",
        "\n",
        "def perform_cross_paper_analysis(processed_papers, output_dir=\"data/reports\"):\n",
        "    \"\"\"Perform cross-paper comparison and analysis\"\"\"\n",
        "    if not processed_papers or len(processed_papers) < 2:\n",
        "        print(\"\\n Need at least 2 papers for cross-paper analysis\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CROSS-PAPER ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\n Comparing {len(processed_papers)} papers...\")\n",
        "\n",
        "    # Perform analysis\n",
        "    analyzer = CrossPaperAnalyzer(processed_papers)\n",
        "    comparison_report = analyzer.generate_comparison_report()\n",
        "\n",
        "    # Save report\n",
        "    report_file = save_comparison_report(comparison_report, output_dir)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n COMPARISON SUMMARY\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"\\n Common Themes:\")\n",
        "    for theme in comparison_report['common_themes'][:10]:\n",
        "        print(f\"   • {theme}\")\n",
        "\n",
        "    print(f\"\\n Paper Statistics:\")\n",
        "    for summary in comparison_report['paper_summaries']:\n",
        "        print(f\"\\n   {summary['file_name']}\")\n",
        "        print(f\"     Words: {summary['word_count']:,}\")\n",
        "        print(f\"     Sections: {len(summary['sections_found'])}\")\n",
        "        print(f\"     Key findings: {summary['key_findings_count']}\")\n",
        "\n",
        "    print(f\"\\n Report saved to: {report_file}\")\n",
        "\n",
        "    return comparison_report\n",
        "\n",
        "# ====================\n",
        "# 7. MAIN EXECUTION\n",
        "# ====================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function for Module 3\"\"\"\n",
        "\n",
        "    # Process all papers\n",
        "    processed_papers = process_all_papers(\n",
        "        download_dir=\"downloads\",\n",
        "        output_dir=\"data/extracted\"\n",
        "    )\n",
        "\n",
        "    if not processed_papers:\n",
        "        print(\"\\n No papers were successfully processed.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(f\" Successfully processed {len(processed_papers)} papers!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Perform cross-paper analysis\n",
        "    comparison_report = perform_cross_paper_analysis(\n",
        "        processed_papers,\n",
        "        output_dir=\"data/reports\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\" MODULE 3 COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\n Outputs:\")\n",
        "    print(\"   • Extracted texts: data/extracted/\")\n",
        "    print(\"   • Comparison report: data/reports/\")\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b278f663",
        "outputId": "4411a32d-10c1-4fb0-cc41-158a46894274"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the 'downloads' directory if it doesn't exist\n",
        "download_dir = \"downloads\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "print(f\"Created directory: {os.path.abspath(download_dir)}\")\n",
        "print(\"Please place your PDF files into this directory for processing.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/downloads\n",
            "Please place your PDF files into this directory for processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85b4f578",
        "outputId": "a9084274-eb68-43a1-bfdb-2a4424570fac"
      },
      "source": [
        "main()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MODULE 3: TEXT EXTRACTION & ANALYSIS\n",
            "================================================================================\n",
            "\n",
            " No PDF files found in 'downloads'\n",
            "\n",
            " No papers were successfully processed.\n"
          ]
        }
      ]
    }
  ]
}