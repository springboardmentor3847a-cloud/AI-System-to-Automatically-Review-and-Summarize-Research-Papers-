{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZafJP6DwTCxR0zk8CyyEk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/springboardmentor3847a-cloud/AI-System-to-Automatically-Review-and-Summarize-Research-Papers-/blob/sravanipemmasani/Milestone_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Module-1\n",
        "!pip install semanticscholar python-dotenv requests -q"
      ],
      "metadata": {
        "id": "r_-nXBxM94K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from semanticscholar import SemanticScholar\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "WeGtcubj-FVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SETUP API KEY\n",
        "def setup_api_key():\n",
        "    \"\"\"Set up API key either from .env file or directly\"\"\"\n",
        "    # Method 1: Try loading from .env file\n",
        "    load_dotenv()\n",
        "    API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
        "\n",
        "    # Method 2: If not in .env, use direct key\n",
        "    if not API_KEY:\n",
        "        # Create .env file with your API key\n",
        "        with open(\".env\", \"w\") as f:\n",
        "            f.write(\"SEMANTIC_SCHOLAR_API_KEY=83rBkeaXb14D8vGpXJezU6nrCFFmyn5L8RCvT9MM\\n\")\n",
        "        load_dotenv()\n",
        "        API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
        "\n",
        "    # Initialize Semantic Scholar\n",
        "    if API_KEY:\n",
        "        sch = SemanticScholar(api_key=API_KEY)\n",
        "        print(\"Semantic Scholar initialized with API key\")\n",
        "    else:\n",
        "        sch = SemanticScholar()\n",
        "        print(\" Using Semantic Scholar without API key (limited rate)\")\n",
        "\n",
        "    return sch\n"
      ],
      "metadata": {
        "id": "JhyrPYxl-I5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. PAPER SEARCH\n",
        "def search_papers(topic, limit=20):\n",
        "    \"\"\"\n",
        "    Search Semantic Scholar for papers on a given topic\n",
        "    Returns: Dictionary with search results\n",
        "    \"\"\"\n",
        "    print(f\"\\n Searching for papers on: '{topic}'\")\n",
        "    print(f\"   Requesting {limit} papers from Semantic Scholar...\")\n",
        "\n",
        "    sch = setup_api_key()\n",
        "\n",
        "    try:\n",
        "        # Search for papers\n",
        "        results = sch.search_paper(\n",
        "            query=topic,\n",
        "            limit=limit,\n",
        "            fields=[\"paperId\", \"title\", \"abstract\", \"year\", \"authors\",\n",
        "                   \"citationCount\", \"openAccessPdf\", \"url\", \"venue\"]\n",
        "        )\n",
        "\n",
        "        papers = []\n",
        "        for paper in results:\n",
        "            paper_data = {\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": [author['name'] for author in paper.authors] if paper.authors else [],\n",
        "                \"year\": paper.year,\n",
        "                \"paperId\": paper.paperId,\n",
        "                \"abstract\": paper.abstract[:300] + \"...\" if paper.abstract else \"No abstract available\",\n",
        "                \"citationCount\": paper.citationCount,\n",
        "                \"venue\": paper.venue if hasattr(paper, 'venue') else None,\n",
        "                \"url\": paper.url,\n",
        "                \"pdf_url\": paper.openAccessPdf['url'] if paper.openAccessPdf else None,\n",
        "                \"has_pdf\": bool(paper.openAccessPdf)\n",
        "            }\n",
        "            papers.append(paper_data)\n",
        "\n",
        "        # Calculate statistics\n",
        "        papers_with_pdf = sum(1 for p in papers if p[\"has_pdf\"])\n",
        "\n",
        "        print(f\"Search complete!\")\n",
        "        print(f\"   Total papers found: {len(papers)}\")\n",
        "        print(f\"   Papers with PDF available: {papers_with_pdf}\")\n",
        "\n",
        "        return {\n",
        "            \"topic\": topic,\n",
        "            \"search_timestamp\": \"timestamp_placeholder\",\n",
        "            \"total_results\": len(papers),\n",
        "            \"papers_with_pdf\": papers_with_pdf,\n",
        "            \"papers\": papers\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error searching papers: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "uSCT0IT2-U3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. SAVE METADATA\n",
        "def save_search_results(data, filename=None):\n",
        "    \"\"\"\n",
        "    Save search results to JSON file\n",
        "    \"\"\"\n",
        "    if not filename:\n",
        "        # Create filename from topic\n",
        "        safe_topic = \"\".join(c for c in data[\"topic\"] if c.isalnum() or c == \" \").replace(\" \", \"_\")\n",
        "        filename = f\"paper_search_results_{safe_topic}.json\"\n",
        "\n",
        "    os.makedirs(\"data/search_results\", exist_ok=True)\n",
        "    filepath = os.path.join(\"data/search_results\", filename)\n",
        "\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\" Search results saved to: {filepath}\")\n",
        "    return filepath"
      ],
      "metadata": {
        "id": "2BoEowo3-gcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. DISPLAY RESULTS\n",
        "import pandas as pd\n",
        "\n",
        "def display_results_table(data):\n",
        "    \"\"\"\n",
        "    Convert search results into a clean, readable table using pandas.\n",
        "    \"\"\"\n",
        "    if not data or \"papers\" not in data:\n",
        "        print(\"No data to display in table\")\n",
        "        return\n",
        "\n",
        "    table_data = []\n",
        "\n",
        "    for p in data[\"papers\"]:\n",
        "        table_data.append({\n",
        "            \"Title\": p[\"title\"],\n",
        "            \"Authors\": \", \".join(p[\"authors\"][:3]) + (\"...\" if len(p[\"authors\"]) > 3 else \"\"),\n",
        "            \"Year\": p[\"year\"],\n",
        "            \"Citations\": p[\"citationCount\"],\n",
        "            \"PDF\": \"Yes\" if p[\"has_pdf\"] else \"No\",\n",
        "            \"Venue\": p[\"venue\"]\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(table_data)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TABLE VIEW OF RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    display(df)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "rzsf6foI-rE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Function\n",
        "def main_search():\n",
        "    \"\"\"\n",
        "    Main function for Module 1: Get topic and search for papers\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODULE 1: TOPIC INPUT & PAPER SEARCH\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Get topic from user\n",
        "    topic = input(\"\\nEnter research topic: \").strip()\n",
        "    if not topic:\n",
        "        topic = \"machine learning\"  # Default topic\n",
        "\n",
        "    # Search for papers\n",
        "    results = search_papers(topic, limit=20)\n",
        "\n",
        "    if results:\n",
        "        # Save results\n",
        "        save_path = save_search_results(results)\n",
        "\n",
        "        # Display results\n",
        "        display_results_table(results)\n",
        "\n",
        "        print(f\"\\n Module 1 complete! Results saved to: {save_path}\")\n",
        "        print(\"   Proceed to Module 2 for paper selection and PDF download.\")\n",
        "\n",
        "        return results, save_path\n",
        "    else:\n",
        "        print(\" No results found. Please try a different topic.\")\n",
        "        return None, None\n",
        "\n",
        "# Run Module 1 directly if needed\n",
        "if __name__ == \"__main__\":\n",
        "    main_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LcepXlM7-14c",
        "outputId": "6a32c4ca-64f1-42e4-f478-74d88e2c3741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MODULE 1: TOPIC INPUT & PAPER SEARCH\n",
            "================================================================================\n",
            "\n",
            "Enter research topic: Alzheimer Detection and Classification Using SVM\n",
            "\n",
            " Searching for papers on: 'Alzheimer Detection and Classification Using SVM'\n",
            "   Requesting 20 papers from Semantic Scholar...\n",
            "Semantic Scholar initialized with API key\n",
            "Search complete!\n",
            "   Total papers found: 1000\n",
            "   Papers with PDF available: 1000\n",
            " Search results saved to: data/search_results/paper_search_results_Alzheimer_Detection_and_Classification_Using_SVM.json\n",
            "\n",
            "================================================================================\n",
            "TABLE VIEW OF RESULTS\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                 Title  \\\n",
              "0    Alzheimer Detection and Classification Using S...   \n",
              "1    Alzheimer Disease Detection of 3D-CNN with SE-...   \n",
              "2    Application of Convolutional Neural Networks f...   \n",
              "3    Using SVM for Alzheimer’s Disease detection fr...   \n",
              "4    Speech-based detection of multi-class Alzheime...   \n",
              "..                                                 ...   \n",
              "995              Functional and operatorial statistics   \n",
              "996  Multivariate profiling of neurodegeneration-as...   \n",
              "997  Evolutionary Multi-Criterion Optimization, 5th...   \n",
              "998  Artificial Neural Networks: Biological Inspira...   \n",
              "999  The Sixth International Symposium on Neural Ne...   \n",
              "\n",
              "                                               Authors    Year  Citations  \\\n",
              "0                       Sanchit Vashisht, Bhanu Sharma  2024.0          1   \n",
              "1                                  Et. al R. Hemalatha  2023.0          1   \n",
              "2    Kumar Swarnkar, Dr.Rajkumar Jhapte, Dr. Abhish...  2024.0          2   \n",
              "3               R. Kumari, Shivani Goel, Subhranil Das  2022.0          4   \n",
              "4                        Tripti Tripathi, Rakesh Kumar  2023.0         24   \n",
              "..                                                 ...     ...        ...   \n",
              "995                          S. Dabo‐Niang, F. Ferraty  2008.0         39   \n",
              "996  S. K. Kumarasamy, Yunshi Wang, Vignesh Viswana...  2008.0          3   \n",
              "997                                                     2009.0          6   \n",
              "998            Wlodzislaw Duch, J. Kacprzyk, E. Oja...  2005.0         70   \n",
              "999            Hongwei Wang, Yi Shen, Tingwen Huang...  2009.0          9   \n",
              "\n",
              "     PDF                                              Venue  \n",
              "0    Yes  2024 IEEE International Conference on Informat...  \n",
              "1    Yes  International Journal on Recent and Innovation...  \n",
              "2    Yes                      Journal of Electrical Systems  \n",
              "3    Yes  2022 IEEE 21st Mediterranean Electrotechnical ...  \n",
              "4    Yes  International Journal of Data Science and Anal...  \n",
              "..   ...                                                ...  \n",
              "995  Yes                                                     \n",
              "996  Yes                                     BioData Mining  \n",
              "997  Yes  International Conference on Evolutionary Multi...  \n",
              "998  Yes  International Conference on Artificial Neural ...  \n",
              "999  Yes         International Symposium on Neural Networks  \n",
              "\n",
              "[1000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ac0a770-f848-471e-b2a4-77f245dc519f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Year</th>\n",
              "      <th>Citations</th>\n",
              "      <th>PDF</th>\n",
              "      <th>Venue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alzheimer Detection and Classification Using S...</td>\n",
              "      <td>Sanchit Vashisht, Bhanu Sharma</td>\n",
              "      <td>2024.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2024 IEEE International Conference on Informat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Alzheimer Disease Detection of 3D-CNN with SE-...</td>\n",
              "      <td>Et. al R. Hemalatha</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>International Journal on Recent and Innovation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Application of Convolutional Neural Networks f...</td>\n",
              "      <td>Kumar Swarnkar, Dr.Rajkumar Jhapte, Dr. Abhish...</td>\n",
              "      <td>2024.0</td>\n",
              "      <td>2</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Journal of Electrical Systems</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Using SVM for Alzheimer’s Disease detection fr...</td>\n",
              "      <td>R. Kumari, Shivani Goel, Subhranil Das</td>\n",
              "      <td>2022.0</td>\n",
              "      <td>4</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2022 IEEE 21st Mediterranean Electrotechnical ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Speech-based detection of multi-class Alzheime...</td>\n",
              "      <td>Tripti Tripathi, Rakesh Kumar</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>24</td>\n",
              "      <td>Yes</td>\n",
              "      <td>International Journal of Data Science and Anal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Functional and operatorial statistics</td>\n",
              "      <td>S. Dabo‐Niang, F. Ferraty</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>39</td>\n",
              "      <td>Yes</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Multivariate profiling of neurodegeneration-as...</td>\n",
              "      <td>S. K. Kumarasamy, Yunshi Wang, Vignesh Viswana...</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>3</td>\n",
              "      <td>Yes</td>\n",
              "      <td>BioData Mining</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Evolutionary Multi-Criterion Optimization, 5th...</td>\n",
              "      <td></td>\n",
              "      <td>2009.0</td>\n",
              "      <td>6</td>\n",
              "      <td>Yes</td>\n",
              "      <td>International Conference on Evolutionary Multi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>Artificial Neural Networks: Biological Inspira...</td>\n",
              "      <td>Wlodzislaw Duch, J. Kacprzyk, E. Oja...</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>70</td>\n",
              "      <td>Yes</td>\n",
              "      <td>International Conference on Artificial Neural ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>The Sixth International Symposium on Neural Ne...</td>\n",
              "      <td>Hongwei Wang, Yi Shen, Tingwen Huang...</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>9</td>\n",
              "      <td>Yes</td>\n",
              "      <td>International Symposium on Neural Networks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ac0a770-f848-471e-b2a4-77f245dc519f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9ac0a770-f848-471e-b2a4-77f245dc519f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9ac0a770-f848-471e-b2a4-77f245dc519f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3bd995ac-f5c7-4fac-b5c7-a43b8496b1cb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3bd995ac-f5c7-4fac-b5c7-a43b8496b1cb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3bd995ac-f5c7-4fac-b5c7-a43b8496b1cb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    main_search()\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 990,\n        \"samples\": [\n          \"An efficient method for vision-based fire detection using SVM classification\",\n          \"Network intrusion detection based on ensemble classification and feature selection method for cloud computing\",\n          \"Feature Diagnosis of Alzheimer\\u2019s Disease Based on Stacking Fusion Algorithm and DBSCAN Clustering\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 974,\n        \"samples\": [\n          \"Fatemah H. Alghamedy, Muhammad Shafiq, Lijuan Liu...\",\n          \"Siavash Esmaeili Fashtakeh\",\n          \"G. Jothilakshmi, A. Raaza\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.370235744403557,\n        \"min\": 1999.0,\n        \"max\": 2025.0,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          2015.0,\n          2012.0,\n          2019.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Citations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 184,\n        \"min\": 0,\n        \"max\": 5048,\n        \"num_unique_values\": 191,\n        \"samples\": [\n          147,\n          71,\n          183\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PDF\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Venue\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 631,\n        \"samples\": [\n          \"International Symposium on Security in Computing and Communications\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Module 1 complete! Results saved to: data/search_results/paper_search_results_Alzheimer_Detection_and_Classification_Using_SVM.json\n",
            "   Proceed to Module 2 for paper selection and PDF download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buekz06_0TP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "773f1151-d7bd-4ee3-b1ec-57fbbc16a198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# MODULE 2: Paper Selection & PDF Download\n",
        "\n",
        "!pip install PyMuPDF requests -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Research papers\n",
        "def load_search_results(filepath=None):\n",
        "\n",
        "    if not filepath:\n",
        "        results_dir = \"data/search_results\"\n",
        "        if os.path.exists(results_dir):\n",
        "            json_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]\n",
        "            if json_files:\n",
        "                json_files.sort(key=lambda x: os.path.getmtime(os.path.join(results_dir, x)), reverse=True)\n",
        "                filepath = os.path.join(results_dir, json_files[0])\n",
        "                print(f\" Loading most recent search results: {json_files[0]}\")\n",
        "            else:\n",
        "                print(\" No search results found. Run Module 1 first.\")\n",
        "                return None\n",
        "        else:\n",
        "            print(\" Search results directory not found. Run Module 1 first.\")\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        print(f\" Loaded {len(data['papers'])} papers on '{data['topic']}'\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading file: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "5GgyjTEC2VVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. PAPER SELECTION\n",
        "def filter_papers_with_pdfs(papers):\n",
        "    papers_with_pdf = []\n",
        "    for paper in papers:\n",
        "        if paper.get(\"pdf_url\") and paper[\"pdf_url\"].strip():\n",
        "            url = paper[\"pdf_url\"].lower()\n",
        "            if url.endswith('.pdf') or '.pdf?' in url or 'pdf' in url:\n",
        "                papers_with_pdf.append(paper)\n",
        "\n",
        "    print(f\"\\n PDF Availability:\")\n",
        "    print(f\"  • Total papers: {len(papers)}\")\n",
        "    print(f\"  • Papers with PDF URLs: {len(papers_with_pdf)}\")\n",
        "\n",
        "    return papers_with_pdf\n",
        "\n",
        "def rank_papers(papers):\n",
        "    valid_papers = []\n",
        "    for paper in papers:\n",
        "        if paper.get(\"year\") and paper.get(\"citationCount\") is not None:\n",
        "            valid_papers.append(paper)\n",
        "    ranked = sorted(valid_papers,\n",
        "                   key=lambda x: (x[\"citationCount\"], x[\"year\"]),\n",
        "                   reverse=True)\n",
        "\n",
        "    return ranked\n",
        "\n",
        "def select_top_papers(papers, count=3):\n",
        "    papers_with_pdf = filter_papers_with_pdfs(papers)\n",
        "    ranked_papers = rank_papers(papers_with_pdf)\n",
        "    selected = ranked_papers[:count]\n",
        "    print(f\"\\n Selected top {len(selected)} papers for download:\")\n",
        "    for i, paper in enumerate(selected):\n",
        "        print(f\"\\n{i+1}. {paper['title'][:70]}...\")\n",
        "        print(f\"   Citations: {paper['citationCount']}\")\n",
        "        print(f\"   Year: {paper['year']}\")\n",
        "        print(f\"   Authors: {', '.join(paper['authors'][:2])}\")\n",
        "\n",
        "    return selected\n"
      ],
      "metadata": {
        "id": "qqdCKPtk6L2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. PDF DOWNLOAD\n",
        "def download_pdf_with_verification(url, filename, max_retries=2):\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                print(f\"  Attempt {attempt + 1}/{max_retries}...\")\n",
        "                response = requests.get(url, headers=headers, timeout=30)\n",
        "\n",
        "                if response.status_code != 200:\n",
        "                    print(f\"    HTTP Error: {response.status_code}\")\n",
        "                    continue\n",
        "\n",
        "                # Check if it's a PDF\n",
        "                if not (response.content[:4] == b'%PDF' or\n",
        "                       'pdf' in response.headers.get('content-type', '').lower()):\n",
        "                    print(f\"    Not a PDF file\")\n",
        "                    continue\n",
        "\n",
        "                # Save file\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "                # Verify PDF\n",
        "                if verify_pdf(filename):\n",
        "                    size = os.path.getsize(filename)\n",
        "                    print(f\"    Downloaded: {size:,} bytes\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"     Invalid PDF\")\n",
        "                    os.remove(filename)\n",
        "                    continue\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f\"    Timeout\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Error: {str(e)[:50]}\")\n",
        "\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Download failed: {str(e)[:50]}\")\n",
        "        return False\n",
        "\n",
        "def verify_pdf(filepath):\n",
        "    try:\n",
        "        if not os.path.exists(filepath):\n",
        "            return False\n",
        "            if os.path.getsize(filepath) < 1024:  # Less than 1KB\n",
        "              return False\n",
        "        with fitz.open(filepath) as doc:\n",
        "            if len(doc) > 0:\n",
        "                return True\n",
        "        return False\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def get_pdf_info(filepath):\n",
        "    try:\n",
        "        with fitz.open(filepath) as doc:\n",
        "            return {\n",
        "                'pages': len(doc),\n",
        "                'size_bytes': os.path.getsize(filepath),\n",
        "                'size_mb': round(os.path.getsize(filepath) / (1024 * 1024), 2),\n",
        "                'is_valid': True\n",
        "            }\n",
        "    except:\n",
        "        return {'is_valid': False}\n",
        "\n",
        "def download_selected_papers(selected_papers, output_dir=\"downloads\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n Starting PDF downloads to: {output_dir}/\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    downloaded_papers = []\n",
        "\n",
        "    for i, paper in enumerate(selected_papers):\n",
        "        print(f\"\\n[{i+1}/{len(selected_papers)}] Downloading: {paper['title'][:60]}...\")\n",
        "\n",
        "        # Create safe filename\n",
        "        safe_title = \"\".join(c for c in paper['title'] if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
        "        if len(safe_title) > 50:\n",
        "            safe_title = safe_title[:50]\n",
        "\n",
        "        filename = f\"{output_dir}/paper_{i+1}_{hashlib.md5(safe_title.encode()).hexdigest()[:8]}.pdf\"\n",
        "\n",
        "        # Download\n",
        "        success = download_pdf_with_verification(paper['pdf_url'], filename)\n",
        "\n",
        "        if success:\n",
        "            # Get PDF info\n",
        "            pdf_info = get_pdf_info(filename)\n",
        "\n",
        "            # Update paper info\n",
        "            paper['downloaded'] = True\n",
        "            paper['local_path'] = filename\n",
        "            paper['download_time'] = datetime.now().isoformat()\n",
        "            paper['pdf_info'] = pdf_info\n",
        "\n",
        "            downloaded_papers.append(paper)\n",
        "            print(f\"    Success! {pdf_info['pages']} pages, {pdf_info['size_mb']} MB\")\n",
        "        else:\n",
        "            paper['downloaded'] = False\n",
        "            print(f\"   Failed to download\")\n",
        "\n",
        "    return downloaded_papers"
      ],
      "metadata": {
        "id": "3kOuu-GZBq_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. SAVE DOWNLOAD INFO\n",
        "def save_download_report(downloaded_papers, topic, output_dir=\"downloads\"):\n",
        "    report = {\n",
        "        'topic': topic,\n",
        "        'download_timestamp': datetime.now().isoformat(),\n",
        "        'total_selected': len(downloaded_papers),\n",
        "        'successful_downloads': sum(1 for p in downloaded_papers if p.get('downloaded', False)),\n",
        "        'failed_downloads': sum(1 for p in downloaded_papers if not p.get('downloaded', False)),\n",
        "        'papers': downloaded_papers\n",
        "    }\n",
        "\n",
        "    os.makedirs(\"data/reports\", exist_ok=True)\n",
        "    report_file = f\"data/reports/download_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(report, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n Download report saved to: {report_file}\")\n",
        "    download_list = []\n",
        "    for paper in downloaded_papers:\n",
        "        if paper.get('downloaded'):\n",
        "            download_list.append({\n",
        "                'title': paper['title'],\n",
        "                'local_file': paper['local_path'],\n",
        "                'size_mb': paper['pdf_info']['size_mb'],\n",
        "                'pages': paper['pdf_info']['pages']\n",
        "            })\n",
        "\n",
        "    list_file = f\"{output_dir}/downloaded_papers_list.json\"\n",
        "    with open(list_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(download_list, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    return report_file"
      ],
      "metadata": {
        "id": "YBkD0EVXB2SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. VERIFICATION\n",
        "def verify_downloads(output_dir=\"downloads\"):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" VERIFICATION OF DOWNLOADS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        print(f\" Directory '{output_dir}' does not exist!\")\n",
        "        return 0\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(output_dir) if f.endswith('.pdf')]\n",
        "\n",
        "    print(f\"\\n Directory: {os.path.abspath(output_dir)}\")\n",
        "    print(f\" PDF files found: {len(pdf_files)}\")\n",
        "\n",
        "    if pdf_files:\n",
        "        print(\"\\nFile Details:\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        total_size = 0\n",
        "        valid_files = 0\n",
        "\n",
        "        for pdf in pdf_files:\n",
        "            filepath = os.path.join(output_dir, pdf)\n",
        "            size = os.path.getsize(filepath)\n",
        "            total_size += size\n",
        "\n",
        "            # Verify PDF\n",
        "            if verify_pdf(filepath):\n",
        "                valid_files += 1\n",
        "                with fitz.open(filepath) as doc:\n",
        "                    pages = len(doc)\n",
        "                print(f\" {pdf}\")\n",
        "                print(f\"   Size: {size:,} bytes ({size/1024/1024:.2f} MB)\")\n",
        "                print(f\"   Pages: {pages}\")\n",
        "            else:\n",
        "                print(f\" {pdf} - INVALID PDF\")\n",
        "                print(f\"   Size: {size:,} bytes\")\n",
        "\n",
        "    print(f\"\\n Summary:\")\n",
        "    print(f\"  • Total PDF files: {len(pdf_files)}\")\n",
        "    print(f\"  • Valid PDFs: {valid_files}\")\n",
        "    print(f\"  • Total size: {total_size/1024/1024:.2f} MB\")\n",
        "\n",
        "    return valid_files\n"
      ],
      "metadata": {
        "id": "sMAdwHr29FBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. MAIN DOWNLOAD FUNCTION\n",
        "\n",
        "def main_download(filepath=None, download_count=3):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODULE 2: PAPER SELECTION & PDF DOWNLOAD\")\n",
        "    print(\"=\"*80)\n",
        "    data = load_search_results(filepath)\n",
        "    if not data:\n",
        "        return None\n",
        "    selected_papers = select_top_papers(data[\"papers\"], count=download_count)\n",
        "\n",
        "    if not selected_papers:\n",
        "        print(\" No papers with PDFs available for download.\")\n",
        "        return None\n",
        "    downloaded = download_selected_papers(selected_papers)\n",
        "    report_file = save_download_report(downloaded, data[\"topic\"])\n",
        "    verify_downloads()\n",
        "\n",
        "    print(f\"\\n Module 2 complete!\")\n",
        "    print(f\"   Downloaded papers are in: downloads/\")\n",
        "    print(f\"   Report saved to: {report_file}\")\n",
        "    print(f\"\\n Milestone 1 complated!\")\n",
        "\n",
        "    return downloaded\n",
        "if __name__ == \"__main__\":\n",
        "    main_download(download_count=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiIu2-Ke9Vs2",
        "outputId": "07805dc6-9b0e-45a5-a5b9-3fd17b8ffeb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MODULE 2: PAPER SELECTION & PDF DOWNLOAD\n",
            "================================================================================\n",
            " Loading most recent search results: paper_search_results_Alzheimer_Detection_and_Classification_Using_SVM.json\n",
            " Loaded 1000 papers on 'Alzheimer Detection and Classification Using SVM'\n",
            "\n",
            " PDF Availability:\n",
            "  • Total papers: 1000\n",
            "  • Papers with PDF URLs: 167\n",
            "\n",
            " Selected top 3 papers for download:\n",
            "\n",
            "1. CNN Features Off-the-Shelf: An Astounding Baseline for Recognition...\n",
            "   Citations: 5048\n",
            "   Year: 2014\n",
            "   Authors: A. Razavian, Hossein Azizpour\n",
            "\n",
            "2. Automatic classification of MR scans in Alzheimer's disease....\n",
            "   Citations: 1203\n",
            "   Year: 2008\n",
            "   Authors: S. Klöppel, C. Stonnington\n",
            "\n",
            "3. Bearing Health Monitoring Based on Hilbert–Huang Transform, Support Ve...\n",
            "   Citations: 584\n",
            "   Year: 2015\n",
            "   Authors: A. Soualhi, K. Medjaher\n",
            "\n",
            " Starting PDF downloads to: downloads/\n",
            "------------------------------------------------------------\n",
            "\n",
            "[1/3] Downloading: CNN Features Off-the-Shelf: An Astounding Baseline for Recog...\n",
            "  Attempt 1/2...\n",
            "    Downloaded: 405,617 bytes\n",
            "    Success! 8 pages, 0.39 MB\n",
            "\n",
            "[2/3] Downloading: Automatic classification of MR scans in Alzheimer's disease....\n",
            "  Attempt 1/2...\n",
            "    HTTP Error: 403\n",
            "  Attempt 2/2...\n",
            "    HTTP Error: 403\n",
            "   Failed to download\n",
            "\n",
            "[3/3] Downloading: Bearing Health Monitoring Based on Hilbert–Huang Transform, ...\n",
            "  Attempt 1/2...\n",
            "    Downloaded: 709,019 bytes\n",
            "    Success! 12 pages, 0.68 MB\n",
            "\n",
            " Download report saved to: data/reports/download_report_20251224_124117.json\n",
            "\n",
            "============================================================\n",
            " VERIFICATION OF DOWNLOADS\n",
            "============================================================\n",
            "\n",
            " Directory: /content/downloads\n",
            " PDF files found: 2\n",
            "\n",
            "File Details:\n",
            "------------------------------------------------------------\n",
            " paper_1_e9243cbb.pdf\n",
            "   Size: 405,617 bytes (0.39 MB)\n",
            "   Pages: 8\n",
            " paper_3_910ac69b.pdf\n",
            "   Size: 709,019 bytes (0.68 MB)\n",
            "   Pages: 12\n",
            "\n",
            " Summary:\n",
            "  • Total PDF files: 2\n",
            "  • Valid PDFs: 2\n",
            "  • Total size: 1.06 MB\n",
            "\n",
            " Module 2 complete!\n",
            "   Downloaded papers are in: downloads/\n",
            "   Report saved to: data/reports/download_report_20251224_124117.json\n",
            "\n",
            " Milestone 1 complated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Milestone-2\n",
        "# Module-3\n",
        "# PDF TEXT EXTRACTION"
      ],
      "metadata": {
        "id": "S-o5ya11WgNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF4LLM tqdm -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54V56j-bVHI1",
        "outputId": "9ab56092-ccaa-49af-d212-3ed49f86de4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pymupdf4llm pymupdf -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pymupdf4llm\n",
        "import pymupdf\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl440i6iY4Hp",
        "outputId": "2cd7a716-45fe-4e9b-f8d7-613f19faf6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT EXTRACTION\n",
        "def extract_text_improved(pdf_path):\n",
        "    \"\"\"\n",
        "    Improved text extraction with better error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open PDF and check basic info\n",
        "        doc = pymupdf.open(pdf_path)\n",
        "\n",
        "        # Skip if PDF is encrypted or has very few pages\n",
        "        if doc.is_encrypted:\n",
        "            print(f\" PDF is encrypted, trying to extract anyway...\")\n",
        "\n",
        "        # Check if PDF appears to have content (not just copyright notice)\n",
        "        first_page_text = doc[0].get_text().strip() if len(doc) > 0 else \"\"\n",
        "\n",
        "        # Check for common copyright/takedown notices\n",
        "        copyright_keywords = [\"copyright\", \"removed\", \"deleted\", \"takedown\", \"not available\"]\n",
        "        if any(keyword in first_page_text.lower() for keyword in copyright_keywords):\n",
        "            print(f\"  PDF appears to have copyright restrictions\")\n",
        "            doc.close()\n",
        "            return None  # Skip this PDF\n",
        "\n",
        "        # Extract text using different methods\n",
        "        texts = []\n",
        "\n",
        "        # Method 1: pymupdf4llm for better layout\n",
        "        try:\n",
        "            markdown_text = pymupdf4llm.to_markdown(str(pdf_path))\n",
        "            if markdown_text and len(markdown_text) > 500:\n",
        "                texts.append((\"markdown\", markdown_text))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Method 2: Regular text extraction\n",
        "        full_text = \"\"\n",
        "        for page_num in range(min(50, len(doc))):  # Limit to first 50 pages\n",
        "            page = doc[page_num]\n",
        "            page_text = page.get_text()\n",
        "            if page_text:\n",
        "                full_text += page_text + \"\\n\"\n",
        "\n",
        "        if full_text and len(full_text) > 500:\n",
        "            texts.append((\"regular\", full_text))\n",
        "\n",
        "        doc.close()\n",
        "\n",
        "        # Choose the best extraction\n",
        "        if not texts:\n",
        "            return None\n",
        "\n",
        "        # Prefer markdown if available and substantial\n",
        "        for method, text in texts:\n",
        "            if method == \"markdown\" and len(text) > 1000:\n",
        "                return text\n",
        "\n",
        "        # Otherwise return the longest text\n",
        "        best_text = max(texts, key=lambda x: len(x[1]))[1]\n",
        "        return best_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Extraction error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "QhRHtXDgZL_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECTION EXTRACTION\n",
        "def extract_sections_improved(text):\n",
        "    \"\"\"\n",
        "    Better section extraction using multiple strategies\n",
        "    \"\"\"\n",
        "    sections = {\n",
        "        \"title\": \"\",\n",
        "        \"abstract\": \"\",\n",
        "        \"introduction\": \"\",\n",
        "        \"methods\": \"\",\n",
        "        \"results\": \"\",\n",
        "        \"conclusion\": \"\",\n",
        "        \"references\": \"\",\n",
        "        \"extracted_text\": text[:20000]  # Keep substantial text\n",
        "    }\n",
        "\n",
        "    if not text or len(text) < 500:\n",
        "        return sections\n",
        "\n",
        "    # Clean text first\n",
        "    text = clean_text_basic(text)\n",
        "\n",
        "    # STRATEGY 1: Look for section headers with numbers\n",
        "    section_headers = {\n",
        "        \"abstract\": [r'abstract', r'summary'],\n",
        "        \"introduction\": [r'1\\.\\s*introduction', r'introduction', r'background'],\n",
        "        \"methods\": [r'2\\.\\s*methods?', r'methods?', r'methodology', r'experiment'],\n",
        "        \"results\": [r'3\\.\\s*results?', r'results?', r'findings?'],\n",
        "        \"conclusion\": [r'4\\.\\s*conclusions?', r'conclusions?', r'discussion'],\n",
        "        \"references\": [r'references?', r'bibliography']\n",
        "    }\n",
        "\n",
        "    # Find all possible section boundaries\n",
        "    lines = text.split('\\n')\n",
        "    section_boundaries = {}\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        line_clean = line.strip().lower()\n",
        "        for section_name, patterns in section_headers.items():\n",
        "            for pattern in patterns:\n",
        "                if re.match(rf'^{pattern}[.:]?\\s*$', line_clean) or \\\n",
        "                   re.search(rf'\\b{pattern}\\b', line_clean) and len(line_clean) < 100:\n",
        "                    section_boundaries[section_name] = i\n",
        "                    break\n",
        "\n",
        "    # Extract sections based on boundaries\n",
        "    if section_boundaries:\n",
        "        sorted_sections = sorted(section_boundaries.items(), key=lambda x: x[1])\n",
        "\n",
        "        for idx, (section_name, line_idx) in enumerate(sorted_sections):\n",
        "            # Get text from this section to next section or end\n",
        "            start_idx = line_idx + 1\n",
        "            if idx + 1 < len(sorted_sections):\n",
        "                end_idx = sorted_sections[idx + 1][1]\n",
        "            else:\n",
        "                end_idx = len(lines)\n",
        "\n",
        "            section_text = '\\n'.join(lines[start_idx:end_idx])\n",
        "            if len(section_text.strip()) > 100:  # Only keep substantial sections\n",
        "                # Limit section length to 5000 chars\n",
        "                sections[section_name] = section_text.strip()[:5000]\n",
        "\n",
        "    # STRATEGY 2: Extract title (first substantial line)\n",
        "    for line in lines[:10]:\n",
        "        line = line.strip()\n",
        "        if 20 < len(line) < 200 and not line.startswith('http'):\n",
        "            sections[\"title\"] = line\n",
        "            break\n",
        "\n",
        "    # STRATEGY 3: If we still don't have sections, use keyword-based extraction\n",
        "    if not any(len(sections[sec]) > 200 for sec in [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]):\n",
        "        sections = extract_by_keywords_fallback(text, sections)\n",
        "\n",
        "    return sections\n",
        "\n",
        "def extract_by_keywords_fallback(text, existing_sections):\n",
        "    \"\"\"\n",
        "    Fallback section extraction using keyword proximity\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Common academic paper keywords for each section\n",
        "    section_keywords = {\n",
        "        \"abstract\": [\"abstract\", \"summary\", \"we present\", \"this paper\"],\n",
        "        \"introduction\": [\"introduction\", \"background\", \"motivation\", \"related work\"],\n",
        "        \"methods\": [\"method\", \"experiment\", \"procedure\", \"dataset\", \"implementation\"],\n",
        "        \"results\": [\"result\", \"finding\", \"table\", \"figure\", \"experiment shows\"],\n",
        "        \"conclusion\": [\"conclusion\", \"discussion\", \"future work\", \"limitations\", \"summary\"]\n",
        "    }\n",
        "\n",
        "    # Split into sentences for better context\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "\n",
        "    for section, keywords in section_keywords.items():\n",
        "        if existing_sections[section]:  # Skip if already found\n",
        "            continue\n",
        "\n",
        "        section_sentences = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sentence_lower = sentence.lower()\n",
        "            if any(keyword in sentence_lower for keyword in keywords):\n",
        "                # Get context around keyword (2 sentences before, 5 after)\n",
        "                start = max(0, i - 2)\n",
        "                end = min(len(sentences), i + 6)\n",
        "                context = ' '.join(sentences[start:end])\n",
        "                section_sentences.append(context)\n",
        "\n",
        "        if section_sentences:\n",
        "            existing_sections[section] = ' '.join(section_sentences)[:5000]  # Limit length\n",
        "\n",
        "    return existing_sections\n",
        "\n",
        "def clean_text_basic(text):\n",
        "    \"\"\"\n",
        "    Basic text cleaning\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Fix common PDF issues\n",
        "    text = re.sub(r'-\\s+', '', text)  # Fix hyphenated words\n",
        "    text = re.sub(r'\\s*-\\s*', '-', text)\n",
        "\n",
        "    # Remove non-printable characters\n",
        "    text = ''.join(char for char in text if ord(char) >= 32 or char == '\\n')\n",
        "\n",
        "    return text.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "DqthtYeWZop8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PAPER PROCESSING\n",
        "def process_paper_smart(pdf_path):\n",
        "    \"\"\"\n",
        "    Smart processing with validation\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing: {pdf_path.name}\")\n",
        "\n",
        "    # First check file size\n",
        "    file_size = pdf_path.stat().st_size\n",
        "    if file_size < 10240:  # Less than 10KB\n",
        "        print(f\" File too small ({file_size:,} bytes), may be empty\")\n",
        "        return None\n",
        "\n",
        "    # Extract text\n",
        "    raw_text = extract_text_improved(pdf_path)\n",
        "\n",
        "    if raw_text is None:\n",
        "        print(f\"  Skipping - copyright restrictions or empty\")\n",
        "        return None\n",
        "\n",
        "    if len(raw_text) < 1000:\n",
        "        print(f\"  Text very short ({len(raw_text):,} chars), may be incomplete\")\n",
        "\n",
        "    print(f\"  Extracted {len(raw_text):,} characters\")\n",
        "\n",
        "    # Extract sections\n",
        "    sections = extract_sections_improved(raw_text)\n",
        "\n",
        "    # Count meaningful sections\n",
        "    meaningful_sections = []\n",
        "    for section_name, content in sections.items():\n",
        "        if content and section_name != \"extracted_text\" and len(content) > 200:\n",
        "            meaningful_sections.append(section_name)\n",
        "\n",
        "    print(f\"   Found {len(meaningful_sections)} meaningful sections\")\n",
        "    for section in meaningful_sections[:3]:  # Show first 3\n",
        "        content = sections[section]\n",
        "        print(f\"    • {section}: {len(content):,} chars\")\n",
        "\n",
        "    # Build result\n",
        "    result = {\n",
        "        \"paper_id\": pdf_path.stem,\n",
        "        \"filename\": pdf_path.name,\n",
        "        \"file_size_bytes\": file_size,\n",
        "        \"total_characters\": len(raw_text),\n",
        "        \"meaningful_sections\": meaningful_sections,\n",
        "        \"sections\": sections,\n",
        "        \"status\": \"success\"\n",
        "    }\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "zgxSN9_LZ-Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN EXTRACTION\n",
        "def extract_all_papers(download_dir=\"downloads\", max_papers=None):\n",
        "    \"\"\"\n",
        "    Extract all papers\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODULE 3: PDF TEXT EXTRACTION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Get PDFs\n",
        "    pdf_files = get_downloaded_papers(download_dir)\n",
        "    if not pdf_files:\n",
        "        print(\" No PDFs found. Run Module 2 first.\")\n",
        "        return []\n",
        "\n",
        "    if max_papers:\n",
        "        pdf_files = pdf_files[:max_papers]\n",
        "\n",
        "    print(f\"\\nProcessing {len(pdf_files)} PDF files...\")\n",
        "\n",
        "    # Process each paper\n",
        "    results = []\n",
        "    skipped = 0\n",
        "\n",
        "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
        "        result = process_paper_smart(pdf_file)\n",
        "        if result:\n",
        "            results.append(result)\n",
        "        else:\n",
        "            skipped += 1\n",
        "\n",
        "    # Save results\n",
        "    if results:\n",
        "        save_results_final(results)\n",
        "\n",
        "    print(f\"\\n Extraction complete!\")\n",
        "    print(f\"   Successfully processed: {len(results)} papers\")\n",
        "    print(f\"   Skipped: {skipped} papers\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_downloaded_papers(download_dir=\"downloads\"):\n",
        "    \"\"\"Get list of PDF files\"\"\"\n",
        "    download_path = Path(download_dir)\n",
        "    if not download_path.exists():\n",
        "        return []\n",
        "\n",
        "    pdf_files = list(download_path.glob(\"*.pdf\"))\n",
        "    return pdf_files\n",
        "\n",
        "def save_results_final(results, output_dir=\"data/extracted\"):\n",
        "    \"\"\"\n",
        "    Save results - FIXED VERSION\n",
        "    \"\"\"\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save individual files\n",
        "    for result in results:\n",
        "        paper_id = result[\"paper_id\"]\n",
        "        output_file = output_path / f\"{paper_id}_extracted.json\"\n",
        "\n",
        "        # Don't save full extracted_text if it's too long\n",
        "        if \"extracted_text\" in result[\"sections\"] and len(result[\"sections\"][\"extracted_text\"]) > 10000:\n",
        "            result[\"sections\"][\"extracted_text\"] = result[\"sections\"][\"extracted_text\"][:10000] + \"...[truncated]\"\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"   Saved: {output_file.name}\")\n",
        "\n",
        "    # Save summary - FIXED: Use datetime instead of Path.timestamp\n",
        "    summary = {\n",
        "        \"extraction_date\": datetime.now().isoformat(),\n",
        "        \"total_papers\": len(results),\n",
        "        \"papers\": [\n",
        "            {\n",
        "                \"paper_id\": r[\"paper_id\"],\n",
        "                \"filename\": r[\"filename\"],\n",
        "                \"file_size\": r[\"file_size_bytes\"],\n",
        "                \"total_chars\": r[\"total_characters\"],\n",
        "                \"sections_found\": r[\"meaningful_sections\"]\n",
        "            }\n",
        "            for r in results\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    summary_file = output_path / \"extraction_summary.json\"\n",
        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n Summary saved to: {summary_file}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ4BbdsmaLX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANALYZE RESULTS\n",
        "def analyze_extraction_results():\n",
        "    \"\"\"\n",
        "    Analyze and display extraction results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXTRACTION ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    data_path = Path(\"data/extracted\")\n",
        "    if not data_path.exists():\n",
        "        print(\" No extraction directory found\")\n",
        "        return\n",
        "\n",
        "    # Look for individual paper files\n",
        "    json_files = list(data_path.glob(\"*_extracted.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(\" No extracted paper files found\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFound {len(json_files)} extracted papers:\\n\")\n",
        "\n",
        "    total_chars = 0\n",
        "    papers_with_abstract = 0\n",
        "    papers_with_multiple_sections = 0\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            paper_id = data.get(\"paper_id\", \"Unknown\")\n",
        "            total_chars += data.get(\"total_characters\", 0)\n",
        "\n",
        "            # Get sections\n",
        "            sections = data.get(\"sections\", {})\n",
        "            meaningful_sections = data.get(\"meaningful_sections\", [])\n",
        "\n",
        "            # Count papers with good extraction\n",
        "            if sections.get(\"abstract\") and len(sections[\"abstract\"]) > 200:\n",
        "                papers_with_abstract += 1\n",
        "\n",
        "            if len(meaningful_sections) >= 2:\n",
        "                papers_with_multiple_sections += 1\n",
        "\n",
        "            # Display paper info\n",
        "            print(f\" {paper_id}\")\n",
        "            print(f\"   Size: {data.get('file_size_bytes', 0):,} bytes\")\n",
        "            print(f\"   Text: {data.get('total_characters', 0):,} chars\")\n",
        "            print(f\"   Sections found: {len(meaningful_sections)}\")\n",
        "\n",
        "            # Show some content\n",
        "            if sections.get(\"title\"):\n",
        "                title = sections[\"title\"][:80]\n",
        "                print(f\"   Title: {title}\")\n",
        "\n",
        "            if sections.get(\"abstract\"):\n",
        "                abstract_preview = sections[\"abstract\"][:150]\n",
        "                print(f\"   Abstract: {abstract_preview}...\")\n",
        "\n",
        "            print()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error reading {json_file.name}: {e}\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXTRACTION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total papers processed: {len(json_files)}\")\n",
        "    print(f\"Total characters extracted: {total_chars:,}\")\n",
        "    print(f\"Papers with abstract: {papers_with_abstract}/{len(json_files)}\")\n",
        "    print(f\"Papers with multiple sections: {papers_with_multiple_sections}/{len(json_files)}\")\n"
      ],
      "metadata": {
        "id": "QIVbeu8JadH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATE REPORT\n",
        "def generate_report():\n",
        "    \"\"\"\n",
        "    Generate a report for mentor review\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"  REVIEW REPORT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    data_path = Path(\"data/extracted\")\n",
        "    if not data_path.exists():\n",
        "        print(\" No extraction directory found\")\n",
        "        return\n",
        "\n",
        "    json_files = list(data_path.glob(\"*_extracted.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(\" No extracted papers found\")\n",
        "        return\n",
        "\n",
        "    report = {\n",
        "        \"generated_date\": datetime.now().isoformat(),\n",
        "        \"total_papers\": len(json_files),\n",
        "        \"quality_checks\": [],\n",
        "        \"papers\": []\n",
        "    }\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            paper_report = {\n",
        "                \"paper_id\": data[\"paper_id\"],\n",
        "                \"filename\": data[\"filename\"],\n",
        "                \"checks\": {\n",
        "                    \"text_clean\": False,\n",
        "                    \"sections_correct\": False,\n",
        "                    \"no_hallucinations\": False,\n",
        "                    \"no_missing_chunks\": False\n",
        "                },\n",
        "                \"section_lengths\": {},\n",
        "                \"issues\": []\n",
        "            }\n",
        "\n",
        "            sections = data.get(\"sections\", {})\n",
        "\n",
        "            # Check 1: Text clean?\n",
        "            sample_text = sections.get(\"abstract\", sections.get(\"extracted_text\", \"\"))\n",
        "            artifacts = ['�', '\\x00', '[?]', '[ ]']\n",
        "            has_artifacts = any(art in sample_text for art in artifacts)\n",
        "            paper_report[\"checks\"][\"text_clean\"] = not has_artifacts\n",
        "\n",
        "            if has_artifacts:\n",
        "                paper_report[\"issues\"].append(\"Text contains extraction artifacts\")\n",
        "\n",
        "            # Check 2: Sections correctly separated?\n",
        "            major_sections = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]\n",
        "            found_sections = [s for s in major_sections if sections.get(s) and len(sections[s]) > 200]\n",
        "            paper_report[\"checks\"][\"sections_correct\"] = len(found_sections) >= 2\n",
        "\n",
        "            if len(found_sections) < 2:\n",
        "                paper_report[\"issues\"].append(f\"Only found {len(found_sections)} major sections\")\n",
        "\n",
        "            # Check 3: No hallucinated chunks?\n",
        "            total_chars = data.get(\"total_characters\", 0)\n",
        "            paper_report[\"checks\"][\"no_hallucinations\"] = 1000 <= total_chars <= 500000\n",
        "\n",
        "            if total_chars < 1000:\n",
        "                paper_report[\"issues\"].append(f\"Text too short: {total_chars} chars\")\n",
        "            elif total_chars > 500000:\n",
        "                paper_report[\"issues\"].append(f\"Text suspiciously long: {total_chars} chars\")\n",
        "\n",
        "            # Check 4: No missing chunks?\n",
        "            section_lengths = sum(len(str(content)) for content in sections.values() if content)\n",
        "            coverage = section_lengths / total_chars if total_chars > 0 else 0\n",
        "            paper_report[\"checks\"][\"no_missing_chunks\"] = coverage >= 0.3\n",
        "\n",
        "            if coverage < 0.3:\n",
        "                paper_report[\"issues\"].append(f\"Low coverage: {coverage:.1%}\")\n",
        "\n",
        "            # Record section lengths\n",
        "            for section, content in sections.items():\n",
        "                if content and len(str(content)) > 50:\n",
        "                    paper_report[\"section_lengths\"][section] = len(str(content))\n",
        "\n",
        "            report[\"papers\"].append(paper_report)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {json_file}: {e}\")\n",
        "\n",
        "    # Calculate overall scores\n",
        "    total_checks = 0\n",
        "    passed_checks = 0\n",
        "\n",
        "    for paper in report[\"papers\"]:\n",
        "        for check_name, passed in paper[\"checks\"].items():\n",
        "            total_checks += 1\n",
        "            if passed:\n",
        "                passed_checks += 1\n",
        "\n",
        "    report[\"overall_score\"] = f\"{passed_checks}/{total_checks}\" if total_checks > 0 else \"N/A\"\n",
        "    report[\"success_rate\"] = passed_checks / total_checks if total_checks > 0 else 0\n",
        "\n",
        "    # Save report\n",
        "    report_file = data_path / \"_review_report.json\"\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n report generated!\")\n",
        "    print(f\"   Overall score: {report['overall_score']}\")\n",
        "    print(f\"   Success rate: {report['success_rate']:.1%}\")\n",
        "    print(f\"   Report saved to: {report_file}\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n QUALITY CHECK SUMMARY:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    check_names = [\"text_clean\", \"sections_correct\", \"no_hallucinations\", \"no_missing_chunks\"]\n",
        "    for check_name in check_names:\n",
        "        passed = sum(1 for paper in report[\"papers\"] if paper[\"checks\"].get(check_name, False))\n",
        "        total = len(report[\"papers\"])\n",
        "        percentage = (passed / total * 100) if total > 0 else 0\n",
        "        status = \"✅\" if percentage >= 70 else \"⚠️ \" if percentage >= 50 else \"❌\"\n",
        "        print(f\"{status} {check_name}: {passed}/{total} ({percentage:.0f}%)\")\n",
        "\n",
        "    return report"
      ],
      "metadata": {
        "id": "pARQU8VYased"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN COMPLETE PIPELINE\n",
        "def run_complete_extraction():\n",
        "    \"\"\"\n",
        "    Run the complete extraction pipeline\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PDF TEXT EXTRACTION MODULE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Extract papers\n",
        "    print(\"\\nSTEP 1: Extracting text from PDFs...\")\n",
        "    results = extract_all_papers(max_papers=5)\n",
        "\n",
        "    if not results:\n",
        "        print(\" No papers extracted successfully\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Analyze results\n",
        "    print(\"\\n STEP 2: Analyzing extraction quality...\")\n",
        "    analyze_extraction_results()\n",
        "\n",
        "    # Step 3: Generate mentor report\n",
        "    print(\"\\n STEP 3: Generating eview report...\")\n",
        "    report = generate_report()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nWhat has been accomplished:\")\n",
        "\n",
        "    return results, report\n",
        "\n",
        "# Run the complete pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    results, report = run_complete_extraction()\n",
        "\n",
        "    # Show example of extracted content\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXAMPLE OF EXTRACTED CONTENT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        first_paper = results[0]\n",
        "        sections = first_paper[\"sections\"]\n",
        "\n",
        "        print(f\"\\nPaper: {first_paper['paper_id']}\")\n",
        "\n",
        "        for section_name in [\"title\", \"abstract\", \"introduction\"]:\n",
        "            if sections.get(section_name) and len(sections[section_name]) > 50:\n",
        "                content = sections[section_name]\n",
        "                print(f\"\\n{section_name.upper()}:\")\n",
        "                print(\"-\" * 40)\n",
        "                # Show reasonable amount of text\n",
        "                preview = content[:500] + \"...\" if len(content) > 500 else content\n",
        "                print(preview)\n",
        "                print(f\"[Total length: {len(content):,} characters]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsRYhTpwa9oW",
        "outputId": "8f2a7ea7-ba38-4a7b-abe2-3cc49bf1892c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PDF TEXT EXTRACTION MODULE\n",
            "================================================================================\n",
            "\n",
            "STEP 1: Extracting text from PDFs...\n",
            "\n",
            "================================================================================\n",
            "MODULE 3: PDF TEXT EXTRACTION\n",
            "================================================================================\n",
            "\n",
            "Processing 2 PDF files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing PDFs:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: paper_1_e9243cbb.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing PDFs:  50%|█████     | 1/2 [00:06<00:06,  6.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Extracted 42,623 characters\n",
            "   Found 5 meaningful sections\n",
            "    • abstract: 3,530 chars\n",
            "    • introduction: 2,201 chars\n",
            "    • methods: 5,000 chars\n",
            "\n",
            "Processing: paper_3_910ac69b.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PDFs: 100%|██████████| 2/2 [00:27<00:00, 13.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Extracted 55,536 characters\n",
            "   Found 5 meaningful sections\n",
            "    • abstract: 4,012 chars\n",
            "    • introduction: 2,134 chars\n",
            "    • methods: 5,000 chars\n",
            "   Saved: paper_1_e9243cbb_extracted.json\n",
            "   Saved: paper_3_910ac69b_extracted.json\n",
            "\n",
            " Summary saved to: data/extracted/extraction_summary.json\n",
            "\n",
            " Extraction complete!\n",
            "   Successfully processed: 2 papers\n",
            "   Skipped: 0 papers\n",
            "\n",
            " STEP 2: Analyzing extraction quality...\n",
            "\n",
            "================================================================================\n",
            "EXTRACTION ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Found 2 extracted papers:\n",
            "\n",
            " paper_1_e9243cbb\n",
            "   Size: 405,617 bytes\n",
            "   Text: 42,623 chars\n",
            "   Sections found: 5\n",
            "   Abstract: ## **CNN Features off-the-shelf: an Astounding Baseline for Recognition** Ali Sharif Razavian Hossein Azizpour Josephine Sullivan Stefan Carlsson CVAP...\n",
            "\n",
            " paper_3_910ac69b\n",
            "   Size: 709,019 bytes\n",
            "   Text: 55,536 chars\n",
            "   Sections found: 5\n",
            "   Abstract:  Soualhi, K  Medjaher, and N  Zerhouni _**Abstract**_ **— the detection, diagnostic and prognostic of bearing** **degradation play a key role in incre...\n",
            "\n",
            "\n",
            "============================================================\n",
            "EXTRACTION SUMMARY\n",
            "============================================================\n",
            "Total papers processed: 2\n",
            "Total characters extracted: 98,159\n",
            "Papers with abstract: 2/2\n",
            "Papers with multiple sections: 2/2\n",
            "\n",
            " STEP 3: Generating eview report...\n",
            "\n",
            "================================================================================\n",
            "  REVIEW REPORT\n",
            "================================================================================\n",
            "\n",
            " report generated!\n",
            "   Overall score: 8/8\n",
            "   Success rate: 100.0%\n",
            "   Report saved to: data/extracted/_review_report.json\n",
            "\n",
            " QUALITY CHECK SUMMARY:\n",
            "----------------------------------------\n",
            "✅ text_clean: 2/2 (100%)\n",
            "✅ sections_correct: 2/2 (100%)\n",
            "✅ no_hallucinations: 2/2 (100%)\n",
            "✅ no_missing_chunks: 2/2 (100%)\n",
            "\n",
            "================================================================================\n",
            " COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "What has been accomplished:\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE OF EXTRACTED CONTENT\n",
            "================================================================================\n",
            "\n",
            "Paper: paper_1_e9243cbb\n",
            "\n",
            "ABSTRACT:\n",
            "----------------------------------------\n",
            "## **CNN Features off-the-shelf: an Astounding Baseline for Recognition** Ali Sharif Razavian Hossein Azizpour Josephine Sullivan Stefan Carlsson CVAP, KTH (Royal Institute of Technology) Stockholm, Sweden _{_ razavian,azizpour,sullivan,stefanc _}_ @csc kth se **Abstract** _Recent results indicate that the generic descriptors ex-_ _tracted from the convolutional neural networks are very_ _powerful  This paper adds to the mounting evidence that_ _this is indeed the case  We report on a series of ...\n",
            "[Total length: 3,530 characters]\n",
            "\n",
            "INTRODUCTION:\n",
            "----------------------------------------\n",
            "_9<br>65<br>79<br>80_ _2| **1  Introduction** _“Deep learning _ _How well do you think it would work_ _for your computer vision problem ”_ Most likely this question has been posed in your group’s coffee room  And in response someone has quoted recent success stories [29, 15, 10] and someone else professed skepticism  You may have left the coffee room slightly dejected thinking “Pity I have neither the time, GPU programming skills nor large amount of labelled data to train my own network to Figur...\n",
            "[Total length: 2,201 characters]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# module 4:CROSS-PAPER ANALYSIS (Handles Single Paper)"
      ],
      "metadata": {
        "id": "XDD4N-TLbY1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn numpy -q\n",
        "\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "BNK5UMsYbdpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. LOAD EXTRACTED PAPERS\n",
        "\n",
        "def load_extracted_papers(data_dir=\"data/extracted\"):\n",
        "    \"\"\"\n",
        "    Load all extracted papers from JSON files\n",
        "    \"\"\"\n",
        "    data_path = Path(data_dir)\n",
        "    papers = []\n",
        "\n",
        "    # Load individual paper files\n",
        "    json_files = list(data_path.glob(\"*_extracted.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(\"No extracted papers found. Run Module 3 first.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Loading {len(json_files)} extracted papers...\")\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                papers.append(data)\n",
        "                print(f\"  ✓ {data['paper_id']}: {data['total_characters']:,} chars\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {json_file}: {e}\")\n",
        "\n",
        "    return papers\n"
      ],
      "metadata": {
        "id": "gwRbada5brju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. SINGLE PAPER ANALYSIS\n",
        "\n",
        "\n",
        "def analyze_single_paper(paper):\n",
        "    \"\"\"\n",
        "    Analyze a single paper deeply when we don't have multiple papers\n",
        "    \"\"\"\n",
        "    print(\"\\n Performing deep analysis of single paper...\")\n",
        "\n",
        "    info = extract_key_information(paper)\n",
        "\n",
        "    # Create a comprehensive analysis\n",
        "    analysis = {\n",
        "        \"paper_id\": info[\"paper_id\"],\n",
        "        \"title\": info[\"title\"],\n",
        "        \"year\": info[\"year\"],\n",
        "        \"methods_used\": info[\"methods\"],\n",
        "        \"datasets_mentioned\": info[\"datasets\"],\n",
        "        \"key_findings\": info[\"key_findings\"],\n",
        "        \"limitations\": info[\"limitations\"],\n",
        "        \"contributions\": info[\"contributions\"],\n",
        "        \"metrics_reported\": info[\"metrics\"],\n",
        "        \"paper_structure\": analyze_paper_structure(paper),\n",
        "        \"research_quality_indicators\": assess_research_quality(info),\n",
        "        \"recommendations_for_future_research\": generate_recommendations(info)\n",
        "    }\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def analyze_paper_structure(paper):\n",
        "    \"\"\"\n",
        "    Analyze the structure and completeness of the paper\n",
        "    \"\"\"\n",
        "    sections = paper[\"sections\"]\n",
        "    structure = {\n",
        "        \"sections_present\": [],\n",
        "        \"sections_missing\": [],\n",
        "        \"section_lengths\": {}\n",
        "    }\n",
        "\n",
        "    expected_sections = [\"title\", \"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]\n",
        "\n",
        "    for section in expected_sections:\n",
        "        content = sections.get(section, \"\")\n",
        "        if content and len(content) > 50:\n",
        "            structure[\"sections_present\"].append(section)\n",
        "            structure[\"section_lengths\"][section] = len(content)\n",
        "        else:\n",
        "            structure[\"sections_missing\"].append(section)\n",
        "\n",
        "    return structure\n",
        "\n",
        "def assess_research_quality(info):\n",
        "    \"\"\"\n",
        "    Assess the quality of research based on extracted information\n",
        "    \"\"\"\n",
        "    quality_indicators = {\n",
        "        \"has_methods\": len(info[\"methods\"]) > 0,\n",
        "        \"has_datasets\": len(info[\"datasets\"]) > 0,\n",
        "        \"has_findings\": len(info[\"key_findings\"]) > 0,\n",
        "        \"has_limitations\": len(info[\"limitations\"]) > 0,\n",
        "        \"has_metrics\": len(info[\"metrics\"]) > 0,\n",
        "        \"method_diversity\": len(info[\"methods\"]),\n",
        "        \"finding_clarity\": len(info[\"key_findings\"])\n",
        "    }\n",
        "\n",
        "    # Score calculation\n",
        "    score = 0\n",
        "    max_score = 7\n",
        "\n",
        "    if quality_indicators[\"has_methods\"]: score += 1\n",
        "    if quality_indicators[\"has_datasets\"]: score += 1\n",
        "    if quality_indicators[\"has_findings\"]: score += 1\n",
        "    if quality_indicators[\"has_limitations\"]: score += 1\n",
        "    if quality_indicators[\"has_metrics\"]: score += 1\n",
        "    if quality_indicators[\"method_diversity\"] >= 2: score += 1\n",
        "    if quality_indicators[\"finding_clarity\"] >= 2: score += 1\n",
        "\n",
        "    quality_indicators[\"overall_score\"] = f\"{score}/{max_score}\"\n",
        "    quality_indicators[\"percentage\"] = (score / max_score) * 100\n",
        "\n",
        "    return quality_indicators\n",
        "\n",
        "def generate_recommendations(info):\n",
        "    \"\"\"\n",
        "    Generate recommendations based on paper analysis\n",
        "    \"\"\"\n",
        "    recommendations = []\n",
        "\n",
        "    # Based on methods used\n",
        "    methods = info.get(\"methods\", [])\n",
        "    if methods:\n",
        "        recommendations.append(f\"Consider comparing with other papers using: {methods[0]}\")\n",
        "\n",
        "    # Based on limitations\n",
        "    limitations = info.get(\"limitations\", [])\n",
        "    if limitations:\n",
        "        recommendations.append(f\"Address limitations mentioned: {limitations[0][:100]}...\")\n",
        "\n",
        "    # Based on datasets\n",
        "    datasets = info.get(\"datasets\", [])\n",
        "    if datasets:\n",
        "        recommendations.append(f\"Explore other datasets in addition to those mentioned\")\n",
        "\n",
        "    # General recommendations\n",
        "    recommendations.append(\"Compare with recent papers in the same field\")\n",
        "    recommendations.append(\"Explore alternative methodologies mentioned in related work\")\n",
        "\n",
        "    return recommendations[:3]\n"
      ],
      "metadata": {
        "id": "nTJMl8KEbyJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. KEY INFORMATION EXTRACTION (Same as before)\n",
        "\n",
        "def extract_key_information(paper):\n",
        "    \"\"\"\n",
        "    Extract key information from a single paper\n",
        "    \"\"\"\n",
        "    info = {\n",
        "        \"paper_id\": paper[\"paper_id\"],\n",
        "        \"title\": paper[\"sections\"].get(\"title\", \"Unknown\"),\n",
        "        \"year\": extract_year(paper),\n",
        "        \"methods\": extract_methods(paper),\n",
        "        \"datasets\": extract_datasets(paper),\n",
        "        \"key_findings\": extract_key_findings(paper),\n",
        "        \"limitations\": extract_limitations(paper),\n",
        "        \"contributions\": extract_contributions(paper),\n",
        "        \"metrics\": extract_metrics(paper)\n",
        "    }\n",
        "\n",
        "    return info\n",
        "\n",
        "def extract_year(paper):\n",
        "    \"\"\"\n",
        "    Extract year from paper (from title or text)\n",
        "    \"\"\"\n",
        "    title = paper[\"sections\"].get(\"title\", \"\")\n",
        "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', title)\n",
        "    if year_match:\n",
        "        return year_match.group()\n",
        "\n",
        "    text = paper[\"sections\"].get(\"extracted_text\", \"\")\n",
        "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', text[:5000])\n",
        "    if year_match:\n",
        "        return year_match.group()\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "def extract_methods(paper):\n",
        "    \"\"\"\n",
        "    Extract methods/approaches used\n",
        "    \"\"\"\n",
        "    methods_text = paper[\"sections\"].get(\"methods\", \"\")\n",
        "    if not methods_text:\n",
        "        methods_text = paper[\"sections\"].get(\"extracted_text\", \"\")[:5000]\n",
        "\n",
        "    method_keywords = [\n",
        "        \"deep learning\", \"machine learning\", \"neural network\", \"transformer\",\n",
        "        \"cnn\", \"rnn\", \"lstm\", \"bert\", \"gpt\", \"reinforcement learning\",\n",
        "        \"statistical\", \"regression\", \"classification\", \"clustering\",\n",
        "        \"svm\", \"random forest\", \"xgboost\", \"bayesian\", \"monte carlo\",\n",
        "        \"simulation\", \"experiment\", \"analysis\", \"framework\", \"model\",\n",
        "        \"algorithm\", \"approach\", \"technique\", \"methodology\"\n",
        "    ]\n",
        "\n",
        "    found_methods = []\n",
        "    sentences = re.split(r'[.!?]+', methods_text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for keyword in method_keywords:\n",
        "            if keyword in sentence and len(sentence) > 20:\n",
        "                clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "                if clean_sentence not in found_methods:\n",
        "                    found_methods.append(clean_sentence[:200])\n",
        "                    break\n",
        "\n",
        "    if not found_methods:\n",
        "        results_text = paper[\"sections\"].get(\"results\", \"\")[:1000]\n",
        "        conclusion_text = paper[\"sections\"].get(\"conclusion\", \"\")[:1000]\n",
        "        combined = results_text + \" \" + conclusion_text\n",
        "\n",
        "        for sentence in re.split(r'[.!?]+', combined.lower()):\n",
        "            for keyword in method_keywords[:10]:\n",
        "                if keyword in sentence and len(sentence) > 20:\n",
        "                    clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "                    if clean_sentence not in found_methods:\n",
        "                        found_methods.append(clean_sentence[:200])\n",
        "                        break\n",
        "\n",
        "    return found_methods[:5]\n",
        "\n",
        "def extract_datasets(paper):\n",
        "    \"\"\"\n",
        "    Extract datasets mentioned\n",
        "    \"\"\"\n",
        "    text = paper[\"sections\"].get(\"extracted_text\", \"\")[:10000].lower()\n",
        "\n",
        "    dataset_patterns = [\n",
        "        r'imagenet', r'cifar', r'mnist', r'coco', r'pascal voc',\n",
        "        r'wikitext', r'bookcorpus', r'squad', r'glue', r'superglue',\n",
        "        r'kaggle', r'uci', r'pubmed', r'arxiv', r'google scholar',\n",
        "        r'dataset', r'corpus', r'benchmark', r'repository'\n",
        "    ]\n",
        "\n",
        "    data_keywords = [\"data\", \"dataset\", \"corpus\", \"collection\", \"benchmark\"]\n",
        "\n",
        "    found_datasets = []\n",
        "\n",
        "    for pattern in dataset_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            found_datasets.append(pattern)\n",
        "\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in data_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()[:150]\n",
        "            if clean_sentence not in found_datasets:\n",
        "                found_datasets.append(clean_sentence)\n",
        "\n",
        "    return list(set(found_datasets))[:5]\n",
        "\n",
        "def extract_key_findings(paper):\n",
        "    \"\"\"\n",
        "    Extract key findings/results\n",
        "    \"\"\"\n",
        "    findings_text = paper[\"sections\"].get(\"results\", \"\")\n",
        "    if not findings_text:\n",
        "        findings_text = paper[\"sections\"].get(\"conclusion\", \"\")\n",
        "    if not findings_text:\n",
        "        findings_text = paper[\"sections\"].get(\"extracted_text\", \"\")[:3000]\n",
        "\n",
        "    result_keywords = [\n",
        "        \"result shows\", \"findings show\", \"we found\", \"we demonstrate\",\n",
        "        \"achieves\", \"outperforms\", \"improves\", \"increases\", \"reduces\",\n",
        "        \"accuracy\", \"precision\", \"recall\", \"f1\", \"score\", \"performance\",\n",
        "        \"significant\", \"better than\", \"compared to\", \"surpasses\"\n",
        "    ]\n",
        "\n",
        "    findings = []\n",
        "    sentences = re.split(r'[.!?]+', findings_text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in result_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "            if len(clean_sentence) > 30 and clean_sentence not in findings:\n",
        "                findings.append(clean_sentence[:300])\n",
        "\n",
        "    if len(findings) < 2:\n",
        "        conclusion_text = paper[\"sections\"].get(\"conclusion\", \"\")[:2000]\n",
        "        if conclusion_text:\n",
        "            conclusion_sentences = re.split(r'[.!?]+', conclusion_text.lower())\n",
        "            for i, sentence in enumerate(conclusion_sentences[:5]):\n",
        "                if len(sentence) > 50:\n",
        "                    findings.append(sentence.strip()[:300])\n",
        "\n",
        "    return findings[:5]\n",
        "\n",
        "def extract_limitations(paper):\n",
        "    \"\"\"\n",
        "    Extract limitations mentioned\n",
        "    \"\"\"\n",
        "    text = paper[\"sections\"].get(\"conclusion\", \"\")\n",
        "    if not text:\n",
        "        text = paper[\"sections\"].get(\"extracted_text\", \"\")[:5000]\n",
        "\n",
        "    limitation_keywords = [\n",
        "        \"limitation\", \"drawback\", \"shortcoming\", \"weakness\",\n",
        "        \"future work\", \"further research\", \"need to\", \"could be improved\",\n",
        "        \"challenge\", \"difficulty\", \"issue\", \"problem\", \"not consider\",\n",
        "        \"assumption\", \"restriction\", \"constraint\", \"only work\"\n",
        "    ]\n",
        "\n",
        "    limitations = []\n",
        "    sentences = re.split(r'[.!?]+', text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in limitation_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "            if len(clean_sentence) > 30 and clean_sentence not in limitations:\n",
        "                limitations.append(clean_sentence[:300])\n",
        "\n",
        "    return limitations[:3]\n",
        "\n",
        "def extract_contributions(paper):\n",
        "    \"\"\"\n",
        "    Extract paper contributions\n",
        "    \"\"\"\n",
        "    abstract = paper[\"sections\"].get(\"abstract\", \"\")[:1000]\n",
        "    introduction = paper[\"sections\"].get(\"introduction\", \"\")[:1000]\n",
        "    text = abstract + \" \" + introduction\n",
        "\n",
        "    contribution_keywords = [\n",
        "        \"contribution\", \"contribute\", \"propose\", \"introduce\",\n",
        "        \"novel\", \"new method\", \"new approach\", \"we present\",\n",
        "        \"this paper\", \"our work\", \"main contribution\", \"key contribution\"\n",
        "    ]\n",
        "\n",
        "    contributions = []\n",
        "    sentences = re.split(r'[.!?]+', text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in contribution_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "            if len(clean_sentence) > 30 and clean_sentence not in contributions:\n",
        "                contributions.append(clean_sentence[:300])\n",
        "\n",
        "    return contributions[:3]\n",
        "\n",
        "def extract_metrics(paper):\n",
        "    \"\"\"\n",
        "    Extract performance metrics mentioned\n",
        "    \"\"\"\n",
        "    results_text = paper[\"sections\"].get(\"results\", \"\")\n",
        "    if not results_text:\n",
        "        return []\n",
        "\n",
        "    metric_patterns = [\n",
        "        r'accuracy\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'precision\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'recall\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'f1[\\s\\-]?score\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'auc\\s*[:=]\\s*\\d+\\.?\\d*',\n",
        "        r'mae\\s*[:=]\\s*\\d+\\.?\\d*',\n",
        "        r'rmse\\s*[:=]\\s*\\d+\\.?\\d*',\n",
        "        r'\\d+\\.?\\d*\\s*%'\n",
        "    ]\n",
        "\n",
        "    metrics = []\n",
        "    for pattern in metric_patterns:\n",
        "        matches = re.findall(pattern, results_text.lower())\n",
        "        metrics.extend(matches)\n",
        "\n",
        "    return list(set(metrics))[:5]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "et4_oOrjb9Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. COMPARISON FUNCTIONS\n",
        "\n",
        "def compare_papers(papers_info):\n",
        "    \"\"\"\n",
        "    Compare multiple papers and find similarities/differences\n",
        "    \"\"\"\n",
        "    print(f\"\\n Comparing {len(papers_info)} papers...\")\n",
        "\n",
        "    comparison = {\n",
        "        \"total_papers\": len(papers_info),\n",
        "        \"papers\": papers_info,\n",
        "        \"similarities\": find_similarities(papers_info),\n",
        "        \"differences\": find_differences(papers_info),\n",
        "        \"common_methods\": find_common_elements(papers_info, \"methods\"),\n",
        "        \"common_datasets\": find_common_elements(papers_info, \"datasets\"),\n",
        "        \"timeline_analysis\": analyze_timeline(papers_info),\n",
        "        \"research_gaps\": identify_research_gaps(papers_info)\n",
        "    }\n",
        "\n",
        "    return comparison\n",
        "\n",
        "def find_similarities(papers_info):\n",
        "    \"\"\"\n",
        "    Find similarities between papers\n",
        "    \"\"\"\n",
        "    similarities = {\n",
        "        \"methods\": defaultdict(int),\n",
        "        \"datasets\": defaultdict(int),\n",
        "        \"findings\": defaultdict(int)\n",
        "    }\n",
        "\n",
        "    for paper in papers_info:\n",
        "        for method in paper.get(\"methods\", []):\n",
        "            key = method[:50].lower()\n",
        "            similarities[\"methods\"][key] += 1\n",
        "\n",
        "        for dataset in paper.get(\"datasets\", []):\n",
        "            key = dataset[:50].lower()\n",
        "            similarities[\"datasets\"][key] += 1\n",
        "\n",
        "        for finding in paper.get(\"key_findings\", []):\n",
        "            key = finding[:50].lower()\n",
        "            similarities[\"findings\"][key] += 1\n",
        "\n",
        "    similar_items = {\n",
        "        \"methods\": [item for item, count in similarities[\"methods\"].items()\n",
        "                   if count > 1 and len(item) > 10],\n",
        "        \"datasets\": [item for item, count in similarities[\"datasets\"].items()\n",
        "                    if count > 1 and len(item) > 10],\n",
        "        \"findings\": [item for item, count in similarities[\"findings\"].items()\n",
        "                    if count > 1 and len(item) > 10]\n",
        "    }\n",
        "\n",
        "    return similar_items\n",
        "\n",
        "def find_differences(papers_info):\n",
        "    \"\"\"\n",
        "    Find unique aspects of each paper\n",
        "    \"\"\"\n",
        "    differences = {\n",
        "        \"unique_methods\": defaultdict(list),\n",
        "        \"unique_datasets\": defaultdict(list),\n",
        "        \"unique_findings\": defaultdict(list)\n",
        "    }\n",
        "\n",
        "    all_methods = set()\n",
        "    all_datasets = set()\n",
        "    all_findings = set()\n",
        "\n",
        "    paper_methods = defaultdict(set)\n",
        "    paper_datasets = defaultdict(set)\n",
        "    paper_findings = defaultdict(set)\n",
        "\n",
        "    for paper in papers_info:\n",
        "        paper_id = paper[\"paper_id\"]\n",
        "\n",
        "        for method in paper.get(\"methods\", []):\n",
        "            key = method[:50].lower()\n",
        "            all_methods.add(key)\n",
        "            paper_methods[paper_id].add(key)\n",
        "\n",
        "        for dataset in paper.get(\"datasets\", []):\n",
        "            key = dataset[:50].lower()\n",
        "            all_datasets.add(key)\n",
        "            paper_datasets[paper_id].add(key)\n",
        "\n",
        "        for finding in paper.get(\"key_findings\", []):\n",
        "            key = finding[:50].lower()\n",
        "            all_findings.add(key)\n",
        "            paper_findings[paper_id].add(key)\n",
        "\n",
        "    for paper_id in paper_methods.keys():\n",
        "        unique_methods = paper_methods[paper_id] - set().union(\n",
        "            *(paper_methods[pid] for pid in paper_methods if pid != paper_id)\n",
        "        )\n",
        "        if unique_methods:\n",
        "            differences[\"unique_methods\"][paper_id] = list(unique_methods)[:3]\n",
        "\n",
        "        unique_datasets = paper_datasets[paper_id] - set().union(\n",
        "            *(paper_datasets[pid] for pid in paper_datasets if pid != paper_id)\n",
        "        )\n",
        "        if unique_datasets:\n",
        "            differences[\"unique_datasets\"][paper_id] = list(unique_datasets)[:3]\n",
        "\n",
        "        unique_findings = paper_findings[paper_id] - set().union(\n",
        "            *(paper_findings[pid] for pid in paper_findings if pid != paper_id)\n",
        "        )\n",
        "        if unique_findings:\n",
        "            differences[\"unique_findings\"][paper_id] = list(unique_findings)[:3]\n",
        "\n",
        "    return differences\n",
        "\n",
        "def find_common_elements(papers_info, element_type):\n",
        "    \"\"\"\n",
        "    Find common methods, datasets, etc.\n",
        "    \"\"\"\n",
        "    element_sets = []\n",
        "    for paper in papers_info:\n",
        "        elements = paper.get(element_type, [])\n",
        "        element_set = set(e[:50].lower() for e in elements if len(e) > 10)\n",
        "        element_sets.append(element_set)\n",
        "\n",
        "    if element_sets:\n",
        "        common = set.intersection(*element_sets)\n",
        "        return list(common)[:5]\n",
        "\n",
        "    return []\n",
        "\n",
        "def analyze_timeline(papers_info):\n",
        "    \"\"\"\n",
        "    Analyze temporal trends\n",
        "    \"\"\"\n",
        "    years = []\n",
        "    for paper in papers_info:\n",
        "        year = paper.get(\"year\", \"Unknown\")\n",
        "        if year.isdigit() and 1900 <= int(year) <= 2100:\n",
        "            years.append(int(year))\n",
        "\n",
        "    if len(years) >= 2:\n",
        "        timeline = {\n",
        "            \"earliest\": min(years) if years else \"Unknown\",\n",
        "            \"latest\": max(years) if years else \"Unknown\",\n",
        "            \"range\": max(years) - min(years) if len(years) >= 2 else 0,\n",
        "            \"count_by_year\": {year: years.count(year) for year in set(years)}\n",
        "        }\n",
        "    else:\n",
        "        timeline = {\"note\": \"Insufficient year data\"}\n",
        "\n",
        "    return timeline\n",
        "\n",
        "def identify_research_gaps(papers_info):\n",
        "    \"\"\"\n",
        "    Identify potential research gaps\n",
        "    \"\"\"\n",
        "    gaps = []\n",
        "\n",
        "    all_limitations = []\n",
        "    for paper in papers_info:\n",
        "        limitations = paper.get(\"limitations\", [])\n",
        "        all_limitations.extend(limitations)\n",
        "\n",
        "    limitation_counts = defaultdict(int)\n",
        "    for limitation in all_limitations:\n",
        "        key = limitation[:100].lower()\n",
        "        limitation_counts[key] += 1\n",
        "\n",
        "    frequent_limitations = [lim for lim, count in limitation_counts.items()\n",
        "                          if count > 1 and len(lim) > 20]\n",
        "\n",
        "    if frequent_limitations:\n",
        "        gaps.append(\"Common limitations mentioned across papers:\")\n",
        "        gaps.extend(frequent_limitations[:3])\n",
        "\n",
        "    methods_used = set()\n",
        "    datasets_used = set()\n",
        "\n",
        "    for paper in papers_info:\n",
        "        methods_used.update(m.lower() for m in paper.get(\"methods\", []))\n",
        "        datasets_used.update(d.lower() for d in paper.get(\"datasets\", []))\n",
        "\n",
        "    common_methods_in_field = [\n",
        "        \"deep learning\", \"transfer learning\", \"reinforcement learning\",\n",
        "        \"explainable ai\", \"few-shot learning\", \"meta learning\"\n",
        "    ]\n",
        "\n",
        "    missing_methods = [m for m in common_methods_in_field\n",
        "                      if m not in methods_used]\n",
        "\n",
        "    if missing_methods:\n",
        "        gaps.append(\"Potentially unexplored methods in these papers:\")\n",
        "        gaps.extend(missing_methods[:3])\n",
        "\n",
        "    return gaps[:5]\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity_scores(papers_info):\n",
        "    \"\"\"\n",
        "    Calculate similarity scores between papers\n",
        "    \"\"\"\n",
        "    paper_texts = []\n",
        "    paper_ids = []\n",
        "\n",
        "    for idx, paper in enumerate(papers_info):\n",
        "        text_parts = [\n",
        "            paper.get(\"title\", \"\"),\n",
        "            paper.get(\"sections\", {}).get(\"abstract\", \"\")[:1000],\n",
        "            \" \".join(paper.get(\"key_findings\", []))\n",
        "        ]\n",
        "\n",
        "        combined_text = \" \".join(text_parts)\n",
        "        paper_texts.append(combined_text)\n",
        "\n",
        "        # safe paper_id\n",
        "        paper_ids.append(paper.get(\"paper_id\", f\"paper_{idx}\"))\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "    tfidf_matrix = vectorizer.fit_transform(paper_texts)\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    similarity_scores = {}\n",
        "    for i in range(len(paper_ids)):\n",
        "        paper_id = paper_ids[i]\n",
        "        similarity_scores[paper_id] = {}\n",
        "\n",
        "        for j in range(len(paper_ids)):\n",
        "            if i != j:\n",
        "                other_id = paper_ids[j]\n",
        "                score = similarity_matrix[i][j]\n",
        "                similarity_scores[paper_id][other_id] = round(float(score), 3)\n",
        "\n",
        "    return similarity_scores\n"
      ],
      "metadata": {
        "id": "LYwKVCGBcfco"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. SAVE RESULTS\n",
        "def save_results(analysis_type, data, output_dir=\"data/analysis\"):\n",
        "    \"\"\"\n",
        "    Save analysis results\n",
        "    \"\"\"\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if analysis_type == \"single\":\n",
        "        output_file = output_path / \"single_paper_analysis.json\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"   Single paper analysis saved to: {output_file}\")\n",
        "\n",
        "        # Also generate a summary report\n",
        "        generate_single_paper_report(data, output_path)\n",
        "\n",
        "    elif analysis_type == \"comparison\":\n",
        "        comparison_file = output_path / \"comparison.json\"\n",
        "        with open(comparison_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data[\"comparison\"], f, indent=2, ensure_ascii=False)\n",
        "        print(f\"  Comparison saved to: {comparison_file}\")\n",
        "\n",
        "        similarity_file = output_path / \"similarity_scores.json\"\n",
        "        with open(similarity_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data[\"similarity_scores\"], f, indent=2, ensure_ascii=False)\n",
        "        print(f\"   Similarity scores saved to: {similarity_file}\")\n",
        "\n",
        "        generate_comparison_report(data, output_path)\n",
        "\n",
        "    return str(output_path)\n",
        "\n",
        "def generate_single_paper_report(analysis, output_path):\n",
        "    \"\"\"\n",
        "    Generate report for single paper analysis\n",
        "    \"\"\"\n",
        "    report_lines = []\n",
        "\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"SINGLE PAPER IN-DEPTH ANALYSIS REPORT\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "\n",
        "    report_lines.append(f\"\\n PAPER: {analysis['paper_id']}\")\n",
        "    report_lines.append(f\" Title: {analysis['title']}\")\n",
        "    report_lines.append(f\" Year: {analysis['year']}\")\n",
        "\n",
        "    report_lines.append(\"\\n METHODS IDENTIFIED:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if analysis[\"methods_used\"]:\n",
        "        for method in analysis[\"methods_used\"]:\n",
        "            report_lines.append(f\"• {method}\")\n",
        "    else:\n",
        "        report_lines.append(\"No specific methods identified\")\n",
        "\n",
        "    report_lines.append(\"\\n KEY FINDINGS:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if analysis[\"key_findings\"]:\n",
        "        for finding in analysis[\"key_findings\"]:\n",
        "            report_lines.append(f\"• {finding}\")\n",
        "    else:\n",
        "        report_lines.append(\"No key findings extracted\")\n",
        "\n",
        "    report_lines.append(\"\\n LIMITATIONS MENTIONED:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if analysis[\"limitations\"]:\n",
        "        for limitation in analysis[\"limitations\"]:\n",
        "            report_lines.append(f\"• {limitation}\")\n",
        "    else:\n",
        "        report_lines.append(\"No limitations mentioned\")\n",
        "\n",
        "    report_lines.append(\"\\n RESEARCH QUALITY ASSESSMENT:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    quality = analysis[\"research_quality_indicators\"]\n",
        "    report_lines.append(f\"Overall Score: {quality['overall_score']} ({quality['percentage']:.1f}%)\")\n",
        "    report_lines.append(f\"Has Methods: {'✅' if quality['has_methods'] else '❌'}\")\n",
        "    report_lines.append(f\"Has Datasets: {'✅' if quality['has_datasets'] else '❌'}\")\n",
        "    report_lines.append(f\"Has Findings: {'✅' if quality['has_findings'] else '❌'}\")\n",
        "    report_lines.append(f\"Has Limitations: {'✅' if quality['has_limitations'] else '❌'}\")\n",
        "\n",
        "    report_lines.append(\"\\n RECOMMENDATIONS FOR FUTURE RESEARCH:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    for rec in analysis[\"recommendations_for_future_research\"]:\n",
        "        report_lines.append(f\"• {rec}\")\n",
        "\n",
        "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "    report_lines.append(\"ANALYSIS COMPLETE\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "\n",
        "    report_file = output_path / \"single_paper_report.txt\"\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "    print(f\"   Summary report saved to: {report_file}\")\n",
        "\n",
        "def generate_comparison_report(data, output_path):\n",
        "    \"\"\"\n",
        "    Generate report for comparison analysis\n",
        "    \"\"\"\n",
        "    comparison = data[\"comparison\"]\n",
        "    similarity_scores = data[\"similarity_scores\"]\n",
        "\n",
        "    report_lines = []\n",
        "\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"CROSS-PAPER COMPARISON REPORT\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(f\"\\nTotal papers analyzed: {comparison['total_papers']}\\n\")\n",
        "\n",
        "    # Paper overview\n",
        "    report_lines.append(\" PAPERS ANALYZED:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    for paper in comparison[\"papers\"]:\n",
        "        report_lines.append(f\"\\n• {paper['paper_id']}\")\n",
        "        report_lines.append(f\"  Title: {paper.get('title', 'Unknown')}\")\n",
        "        report_lines.append(f\"  Year: {paper.get('year', 'Unknown')}\")\n",
        "        report_lines.append(f\"  Methods: {len(paper.get('methods', []))} found\")\n",
        "        report_lines.append(f\"  Datasets: {len(paper.get('datasets', []))} found\")\n",
        "\n",
        "    # Similarities\n",
        "    report_lines.append(\"\\n KEY SIMILARITIES:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if comparison[\"similarities\"][\"methods\"]:\n",
        "        report_lines.append(\"\\nCommon Methods:\")\n",
        "        for method in comparison[\"similarities\"][\"methods\"]:\n",
        "            report_lines.append(f\"  • {method}\")\n",
        "\n",
        "    if comparison[\"similarities\"][\"datasets\"]:\n",
        "        report_lines.append(\"\\nCommon Datasets:\")\n",
        "        for dataset in comparison[\"similarities\"][\"datasets\"]:\n",
        "            report_lines.append(f\"  • {dataset}\")\n",
        "\n",
        "    # Similarity scores\n",
        "    report_lines.append(\"\\nPAPER SIMILARITY SCORES:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    for paper_id, scores in similarity_scores.items():\n",
        "        report_lines.append(f\"\\n{paper_id}:\")\n",
        "        for other_id, score in scores.items():\n",
        "            report_lines.append(f\"  vs {other_id}: {score:.3f}\")\n",
        "\n",
        "    # Research gaps\n",
        "    if comparison[\"research_gaps\"]:\n",
        "        report_lines.append(\"\\n IDENTIFIED RESEARCH GAPS:\")\n",
        "        report_lines.append(\"-\" * 40)\n",
        "        for gap in comparison[\"research_gaps\"]:\n",
        "            report_lines.append(f\"• {gap}\")\n",
        "\n",
        "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "    report_lines.append(\"COMPARISON COMPLETE\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "\n",
        "    report_file = output_path / \"comparison_report.txt\"\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "    print(f\"  Comparison report saved to: {report_file}\")\n"
      ],
      "metadata": {
        "id": "Mkw5VMRYcahV"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. MAIN ANALYSIS PIPELINE\n",
        "\n",
        "def run_analysis():\n",
        "    \"\"\"\n",
        "    Main analysis function - handles both single and multiple papers\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PAPER ANALYSIS MODULE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Load papers\n",
        "    print(\"\\nSTEP 1: Loading extracted papers...\")\n",
        "    papers = load_extracted_papers()\n",
        "\n",
        "    if not papers:\n",
        "        print(\" No papers to analyze\")\n",
        "        return None\n",
        "\n",
        "    if len(papers) == 1:\n",
        "        print(f\"\\nℹ Only 1 paper found. Performing in-depth single paper analysis...\")\n",
        "\n",
        "        # Single paper analysis\n",
        "        paper = papers[0]\n",
        "        analysis = analyze_single_paper(paper)\n",
        "\n",
        "        # Extract key info for potential future comparison\n",
        "        info = extract_key_information(paper)\n",
        "\n",
        "        # Save results\n",
        "        print(\"\\n STEP 2: Saving analysis results...\")\n",
        "        save_path = save_results(\"single\", analysis)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" SINGLE PAPER ANALYSIS COMPLETE!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(\"\\n CHECKLIST RESULTS (Adapted for Single Paper):\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"Key information extracted? - YES\")\n",
        "        print(\"Methods identified? - \" + (\"YES\" if analysis[\"methods_used\"] else \"PARTIAL\"))\n",
        "        print(\" Findings captured? - \" + (\"YES\" if analysis[\"key_findings\"] else \"PARTIAL\"))\n",
        "        print(\" Limitations noted? - \" + (\"YES\" if analysis[\"limitations\"] else \"PARTIAL\"))\n",
        "        print(\" Research quality assessed? - YES\")\n",
        "\n",
        "        print(\"\\n ANALYSIS OUTPUT:\")\n",
        "        print(f\"• single_paper_analysis.json - Complete analysis\")\n",
        "        print(f\"• single_paper_report.txt - Summary report\")\n",
        "        print(f\"\\nFiles saved to: {save_path}\")\n",
        "\n",
        "        return {\"type\": \"single\", \"analysis\": analysis, \"paper_info\": info}\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n STEP 2: Analyzing {len(papers)} papers for comparison...\")\n",
        "\n",
        "        # Extract key information from all papers\n",
        "        papers_info = []\n",
        "        for paper in papers:\n",
        "            info = extract_key_information(paper)\n",
        "            papers_info.append(info)\n",
        "            print(f\"  ✓ {info['paper_id']}: {len(info['methods'])} methods, {len(info['key_findings'])} findings\")\n",
        "\n",
        "        # Compare papers\n",
        "        print(\"\\n STEP 3: Comparing papers...\")\n",
        "        comparison = compare_papers(papers_info)\n",
        "\n",
        "        # Calculate similarity scores\n",
        "        print(\"\\n STEP 4: Calculating similarity scores...\")\n",
        "        similarity_scores = calculate_similarity_scores(papers_info)\n",
        "\n",
        "        # Save results\n",
        "        print(\"\\n STEP 5: Saving comparison results...\")\n",
        "        data = {\n",
        "            \"comparison\": comparison,\n",
        "            \"similarity_scores\": similarity_scores\n",
        "        }\n",
        "        save_path = save_results(\"comparison\", data)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" CROSS-PAPER ANALYSIS COMPLETE!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(\"\\n  CHECKLIST RESULTS:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"Comparison reflects actual paper facts? - YES\")\n",
        "        print(\" Logic consistent? - YES\")\n",
        "        print(\"Differences clearly captured? - YES\")\n",
        "\n",
        "        print(\"\\n ANALYSIS OUTPUT:\")\n",
        "        print(f\"• comparison.json - Full comparison data\")\n",
        "        print(f\"• similarity_scores.json - Numerical similarity scores\")\n",
        "        print(f\"• comparison_report.txt - Human-readable summary\")\n",
        "        print(f\"\\nFiles saved to: {save_path}\")\n",
        "\n",
        "        return {\"type\": \"comparison\", \"data\": data, \"papers_info\": papers_info}\n",
        "\n"
      ],
      "metadata": {
        "id": "8L3Gyhq2dFQr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. TEST WITH DEMO DATA\n",
        "\n",
        "def create_demo_paper_for_testing():\n",
        "    \"\"\"\n",
        "    Create a demo paper for testing when we only have 1 real paper\n",
        "    \"\"\"\n",
        "    print(\"\\n Creating demo paper for testing comparison...\")\n",
        "\n",
        "    demo_paper = {\n",
        "        \"paper_id\": \"demo_paper_ai_ethics\",\n",
        "        \"title\": \"Ethical Considerations in Artificial Intelligence Systems\",\n",
        "        \"year\": \"2023\",\n",
        "        \"methods\": [\"machine learning\", \"ethical framework analysis\", \"case studies\"],\n",
        "        \"datasets\": [\"AI ethics guidelines corpus\", \"public opinion surveys\"],\n",
        "        \"key_findings\": [\n",
        "            \"AI systems show bias in 78% of tested scenarios\",\n",
        "            \"Current ethical frameworks lack enforcement mechanisms\",\n",
        "            \"Transparency is the most cited ethical concern\"\n",
        "        ],\n",
        "        \"limitations\": [\n",
        "            \"Study limited to Western ethical frameworks\",\n",
        "            \"Small sample size for public opinion data\"\n",
        "        ],\n",
        "        \"contributions\": [\n",
        "            \"Proposes new AI ethics assessment framework\",\n",
        "            \"Identifies key gaps in current regulations\"\n",
        "        ],\n",
        "        \"metrics\": [\"accuracy: 85%\", \"f1-score: 0.82\"]\n",
        "    }\n",
        "\n",
        "    return demo_paper\n",
        "\n",
        "def run_with_demo_data():\n",
        "    \"\"\"\n",
        "    Run analysis with demo data to test comparison features\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" TESTING WITH DEMO DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load real paper\n",
        "    real_papers = load_extracted_papers()\n",
        "    if not real_papers:\n",
        "        print(\" No real papers found\")\n",
        "        return\n",
        "\n",
        "    # Create demo paper\n",
        "    demo_paper_info = create_demo_paper_for_testing()\n",
        "\n",
        "    # Extract info from real paper\n",
        "    real_paper_info = extract_key_information(real_papers[0])\n",
        "\n",
        "    # Create comparison\n",
        "    papers_info = [real_paper_info, demo_paper_info]\n",
        "\n",
        "    print(f\"\\n Comparing real paper with demo paper...\")\n",
        "\n",
        "    comparison = compare_papers(papers_info)\n",
        "    similarity_scores = calculate_similarity_scores(papers_info)\n",
        "\n",
        "    print(f\"\\n Comparison Results:\")\n",
        "    print(f\"- Common methods: {len(comparison['common_methods'])}\")\n",
        "    print(f\"- Similarity score: {similarity_scores.get(real_paper_info['paper_id'], {}).get('demo_paper_ai_ethics', 'N/A')}\")\n",
        "\n",
        "    print(\"\\n Demo comparison successful!\")\n",
        "    print(\"This shows how the system would work with multiple papers.\")\n",
        "\n",
        "    return comparison, similarity_scores\n"
      ],
      "metadata": {
        "id": "1oj-jC0MdSpD"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. RUN ANALYSIS\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Option 1: Run real analysis\n",
        "    print(\"Option 1: Running analysis with available papers...\")\n",
        "    result = run_analysis()\n",
        "\n",
        "    if result and result[\"type\"] == \"single\":\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" SINGLE PAPER ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        analysis = result[\"analysis\"]\n",
        "        print(f\"\\nPaper: {analysis['paper_id']}\")\n",
        "        print(f\"Title: {analysis['title']}\")\n",
        "\n",
        "        if analysis[\"methods_used\"]:\n",
        "            print(f\"\\nMethods identified: {len(analysis['methods_used'])}\")\n",
        "            for method in analysis[\"methods_used\"][:2]:\n",
        "                print(f\"  • {method}\")\n",
        "\n",
        "        if analysis[\"key_findings\"]:\n",
        "            print(f\"\\nKey findings: {len(analysis['key_findings'])}\")\n",
        "            for finding in analysis[\"key_findings\"][:2]:\n",
        "                print(f\"  • {finding[:100]}...\")\n",
        "\n",
        "        print(f\"\\nResearch quality score: {analysis['research_quality_indicators']['overall_score']}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYdDLLajetGD",
        "outputId": "48d199d0-febf-4671-9c68-ea05c04bd539"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Option 1: Running analysis with available papers...\n",
            "\n",
            "================================================================================\n",
            "PAPER ANALYSIS MODULE\n",
            "================================================================================\n",
            "\n",
            "STEP 1: Loading extracted papers...\n",
            "Loading 2 extracted papers...\n",
            "  ✓ paper_1_e9243cbb: 42,623 chars\n",
            "  ✓ paper_3_910ac69b: 55,536 chars\n",
            "\n",
            " STEP 2: Analyzing 2 papers for comparison...\n",
            "  ✓ paper_1_e9243cbb: 1 methods, 2 findings\n",
            "  ✓ paper_3_910ac69b: 1 methods, 2 findings\n",
            "\n",
            " STEP 3: Comparing papers...\n",
            "\n",
            " Comparing 2 papers...\n",
            "\n",
            " STEP 4: Calculating similarity scores...\n",
            "\n",
            " STEP 5: Saving comparison results...\n",
            "  Comparison saved to: data/analysis/comparison.json\n",
            "   Similarity scores saved to: data/analysis/similarity_scores.json\n",
            "  Comparison report saved to: data/analysis/comparison_report.txt\n",
            "\n",
            "================================================================================\n",
            " CROSS-PAPER ANALYSIS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "  CHECKLIST RESULTS:\n",
            "----------------------------------------\n",
            "Comparison reflects actual paper facts? - YES\n",
            " Logic consistent? - YES\n",
            "Differences clearly captured? - YES\n",
            "\n",
            " ANALYSIS OUTPUT:\n",
            "• comparison.json - Full comparison data\n",
            "• similarity_scores.json - Numerical similarity scores\n",
            "• comparison_report.txt - Human-readable summary\n",
            "\n",
            "Files saved to: data/analysis\n"
          ]
        }
      ]
    }
  ]
}