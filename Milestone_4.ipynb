{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe6VywIJ5rNp5zP34rTNZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/springboardmentor3847a-cloud/AI-System-to-Automatically-Review-and-Summarize-Research-Papers-/blob/sravanipemmasani/Milestone_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Milestone-1\n",
        "\n",
        "Module-1\n",
        "\n",
        "Week-1&2"
      ],
      "metadata": {
        "id": "hdm7_kDezafL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Access Semantic Scholar API using Python libs\n"
      ],
      "metadata": {
        "id": "rQiqvCPazh2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install semanticscholar python-dotenv requests -q\n",
        "import json\n",
        "import os\n",
        "from semanticscholar import SemanticScholar\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "ZPYPaR38zdKO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function loads or creates a Semantic Scholar API key and initializes the API client with authenticated or limited access."
      ],
      "metadata": {
        "id": "kR1nOPwuzsRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SETUP API KEY\n",
        "def setup_api_key():\n",
        "    \"\"\"Set up API key either from .env file or directly\"\"\"\n",
        "    # Method 1: Try loading from .env file\n",
        "    load_dotenv()\n",
        "    API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
        "\n",
        "    # Method 2: If not in .env, use direct key\n",
        "    if not API_KEY:\n",
        "        # Create .env file with your API key\n",
        "        with open(\".env\", \"w\") as f:\n",
        "            f.write(\"SEMANTIC_SCHOLAR_API_KEY=83rBkeaXb14D8vGpXJezU6nrCFFmyn5L8RCvT9MM\\n\")\n",
        "        load_dotenv()\n",
        "        API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
        "\n",
        "    # Initialize Semantic Scholar\n",
        "    if API_KEY:\n",
        "        sch = SemanticScholar(api_key=API_KEY)\n",
        "        print(\"Semantic Scholar initialized with API key\")\n",
        "    else:\n",
        "        sch = SemanticScholar()\n",
        "        print(\" Using Semantic Scholar without API key (limited rate)\")\n",
        "\n",
        "    return sch\n"
      ],
      "metadata": {
        "id": "eWRij90yznh4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function searches Semantic Scholar for papers on a given topic, extracts key metadata, counts available PDFs, and returns the results with summary statistics.\n"
      ],
      "metadata": {
        "id": "SpIkE-EZz0Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. PAPER SEARCH\n",
        "def search_papers(topic, limit=20):\n",
        "    \"\"\"\n",
        "    Search Semantic Scholar for papers on a given topic\n",
        "    Returns: Dictionary with search results\n",
        "    \"\"\"\n",
        "    print(f\"\\n Searching for papers on: '{topic}'\")\n",
        "    print(f\"   Requesting {limit} papers from Semantic Scholar...\")\n",
        "\n",
        "    sch = setup_api_key()\n",
        "\n",
        "    try:\n",
        "        # Search for papers\n",
        "        results = sch.search_paper(\n",
        "            query=topic,\n",
        "            limit=limit,\n",
        "            fields=[\"paperId\", \"title\", \"abstract\", \"year\", \"authors\",\n",
        "                   \"citationCount\", \"openAccessPdf\", \"url\", \"venue\"]\n",
        "        )\n",
        "\n",
        "        papers = []\n",
        "        for paper in results:\n",
        "            paper_data = {\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": [author['name'] for author in paper.authors] if paper.authors else [],\n",
        "                \"year\": paper.year,\n",
        "                \"paperId\": paper.paperId,\n",
        "                \"abstract\": paper.abstract[:300] + \"...\" if paper.abstract else \"No abstract available\",\n",
        "                \"citationCount\": paper.citationCount,\n",
        "                \"venue\": paper.venue if hasattr(paper, 'venue') else None,\n",
        "                \"url\": paper.url,\n",
        "                \"pdf_url\": paper.openAccessPdf['url'] if paper.openAccessPdf else None,\n",
        "                \"has_pdf\": bool(paper.openAccessPdf)\n",
        "            }\n",
        "            papers.append(paper_data)\n",
        "\n",
        "        # Calculate statistics\n",
        "        papers_with_pdf = sum(1 for p in papers if p[\"has_pdf\"])\n",
        "\n",
        "        print(f\"Search complete!\")\n",
        "        print(f\"   Total papers found: {len(papers)}\")\n",
        "        print(f\"   Papers with PDF available: {papers_with_pdf}\")\n",
        "\n",
        "        return {\n",
        "            \"topic\": topic,\n",
        "            \"search_timestamp\": \"timestamp_placeholder\",\n",
        "            \"total_results\": len(papers),\n",
        "            \"papers_with_pdf\": papers_with_pdf,\n",
        "            \"papers\": papers\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error searching papers: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "u-Nj5NQCzy8l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function saves paper search results as a JSON file with a topic-based filename inside the `data/search_results` directory.\n"
      ],
      "metadata": {
        "id": "j5Lqh2F2z9pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. SAVE METADATA\n",
        "def save_search_results(data, filename=None):\n",
        "    \"\"\"\n",
        "    Save search results to JSON file\n",
        "    \"\"\"\n",
        "    if not filename:\n",
        "        # Create filename from topic\n",
        "        safe_topic = \"\".join(c for c in data[\"topic\"] if c.isalnum() or c == \" \").replace(\" \", \"_\")\n",
        "        filename = f\"paper_search_results_{safe_topic}.json\"\n",
        "\n",
        "    os.makedirs(\"data/search_results\", exist_ok=True)\n",
        "    filepath = os.path.join(\"data/search_results\", filename)\n",
        "\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\" Search results saved to: {filepath}\")\n",
        "    return filepath"
      ],
      "metadata": {
        "id": "qqr52s120B-5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function converts paper search results into a pandas DataFrame and displays them as a clean, readable table.\n"
      ],
      "metadata": {
        "id": "rPbLrq_p0F3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. DISPLAY RESULTS\n",
        "import pandas as pd\n",
        "\n",
        "def display_results_table(data):\n",
        "    \"\"\"\n",
        "    Convert search results into a clean, readable table using pandas.\n",
        "    \"\"\"\n",
        "    if not data or \"papers\" not in data:\n",
        "        print(\"No data to display in table\")\n",
        "        return\n",
        "\n",
        "    table_data = []\n",
        "\n",
        "    for p in data[\"papers\"]:\n",
        "        table_data.append({\n",
        "            \"Title\": p[\"title\"],\n",
        "            \"Authors\": \", \".join(p[\"authors\"][:3]) + (\"...\" if len(p[\"authors\"]) > 3 else \"\"),\n",
        "            \"Year\": p[\"year\"],\n",
        "            \"Citations\": p[\"citationCount\"],\n",
        "            \"PDF\": \"Yes\" if p[\"has_pdf\"] else \"No\",\n",
        "            \"Venue\": p[\"venue\"]\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(table_data)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TABLE VIEW OF RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    display(df)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "PW_msAAy0Kqp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This main function takes a research topic, searches for relevant papers, saves the results, displays them in a table, and completes Module 1 of the workflow.\n"
      ],
      "metadata": {
        "id": "0QQxTwni0Pn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Function\n",
        "def main_search():\n",
        "    \"\"\"\n",
        "    Main function for Module 1: Get topic and search for papers\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODULE 1: TOPIC INPUT & PAPER SEARCH\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Get topic from user\n",
        "    topic = input(\"\\nEnter research topic: \").strip()\n",
        "    if not topic:\n",
        "        topic = \"machine learning\"  # Default topic\n",
        "\n",
        "    # Search for papers\n",
        "    results = search_papers(topic, limit=20)\n",
        "\n",
        "    if results:\n",
        "        # Save results\n",
        "        save_path = save_search_results(results)\n",
        "\n",
        "        # Display results\n",
        "        display_results_table(results)\n",
        "\n",
        "        print(f\"\\n Module 1 complete! Results saved to: {save_path}\")\n",
        "        print(\"   Proceed to Module 2 for paper selection and PDF download.\")\n",
        "\n",
        "        return results, save_path\n",
        "    else:\n",
        "        print(\" No results found. Please try a different topic.\")\n",
        "        return None, None\n",
        "\n",
        "# Run Module 1 directly if needed\n",
        "if __name__ == \"__main__\":\n",
        "    main_search()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "1UmrNr4p0TNb",
        "outputId": "68622d2a-d48b-4f63-aaee-94581519e6db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MODULE 1: TOPIC INPUT & PAPER SEARCH\n",
            "================================================================================\n",
            "\n",
            "Enter research topic: 'Alzheimer Detection and Classification Using SVM\n",
            "\n",
            " Searching for papers on: ''Alzheimer Detection and Classification Using SVM'\n",
            "   Requesting 20 papers from Semantic Scholar...\n",
            "Semantic Scholar initialized with API key\n",
            "Search complete!\n",
            "   Total papers found: 1000\n",
            "   Papers with PDF available: 1000\n",
            " Search results saved to: data/search_results/paper_search_results_Alzheimer_Detection_and_Classification_Using_SVM.json\n",
            "\n",
            "================================================================================\n",
            "TABLE VIEW OF RESULTS\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                 Title  \\\n",
              "0    Alzheimer Detection and Classification Using S...   \n",
              "1    MRI-Based Biomarkers for Early Detection and C...   \n",
              "2    Detection and Classification of Alzheimer’s Di...   \n",
              "3    Advancing Alzheimer’s Diagnosis Through Machin...   \n",
              "4    Alzheimer Disease Detection of 3D-CNN with SE-...   \n",
              "..                                                 ...   \n",
              "995              Functional and operatorial statistics   \n",
              "996  Artificial Neural Networks: Biological Inspira...   \n",
              "997  Evolutionary Multi-Criterion Optimization, 5th...   \n",
              "998  Multivariate profiling of neurodegeneration-as...   \n",
              "999  The Sixth International Symposium on Neural Ne...   \n",
              "\n",
              "                                               Authors    Year  Citations  \\\n",
              "0                       Sanchit Vashisht, Bhanu Sharma  2024.0          1   \n",
              "1    Karpagam M, V.R. Rishendra, Rangineni Yukthamukhi  2025.0          0   \n",
              "2      Muhammad Zaeem Khalid, Nida Iqbal, Babar Ali...  2025.0          0   \n",
              "3    L. Jabasheela, A.Nancy Jenifer, Pravin R. Kshi...  2025.0          0   \n",
              "4                                  Et. al R. Hemalatha  2023.0          1   \n",
              "..                                                 ...     ...        ...   \n",
              "995                          S. Dabo‐Niang, F. Ferraty  2008.0         39   \n",
              "996            Wlodzislaw Duch, J. Kacprzyk, E. Oja...  2005.0         70   \n",
              "997                                                     2009.0          6   \n",
              "998  S. K. Kumarasamy, Yunshi Wang, Vignesh Viswana...  2008.0          3   \n",
              "999            Hongwei Wang, Yi Shen, Tingwen Huang...  2009.0          9   \n",
              "\n",
              "     PDF                                              Venue  \n",
              "0    Yes  2024 IEEE International Conference on Informat...  \n",
              "1    Yes  Proceedings of the 4th International Conferenc...  \n",
              "2    Yes                                         Tomography  \n",
              "3    Yes  2025 2nd International Conference on Research ...  \n",
              "4    Yes  International Journal on Recent and Innovation...  \n",
              "..   ...                                                ...  \n",
              "995  Yes                                                     \n",
              "996  Yes  International Conference on Artificial Neural ...  \n",
              "997  Yes  International Conference on Evolutionary Multi...  \n",
              "998  Yes                                     BioData Mining  \n",
              "999  Yes         International Symposium on Neural Networks  \n",
              "\n",
              "[1000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ef8c4aa-2adb-4519-b731-145a5a7a45b3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Year</th>\n",
              "      <th>Citations</th>\n",
              "      <th>PDF</th>\n",
              "      <th>Venue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alzheimer Detection and Classification Using S...</td>\n",
              "      <td>Sanchit Vashisht, Bhanu Sharma</td>\n",
              "      <td>2024.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2024 IEEE International Conference on Informat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MRI-Based Biomarkers for Early Detection and C...</td>\n",
              "      <td>Karpagam M, V.R. Rishendra, Rangineni Yukthamukhi</td>\n",
              "      <td>2025.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Proceedings of the 4th International Conferenc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Detection and Classification of Alzheimer’s Di...</td>\n",
              "      <td>Muhammad Zaeem Khalid, Nida Iqbal, Babar Ali...</td>\n",
              "      <td>2025.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Tomography</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Advancing Alzheimer’s Diagnosis Through Machin...</td>\n",
              "      <td>L. Jabasheela, A.Nancy Jenifer, Pravin R. Kshi...</td>\n",
              "      <td>2025.0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2025 2nd International Conference on Research ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Alzheimer Disease Detection of 3D-CNN with SE-...</td>\n",
              "      <td>Et. al R. Hemalatha</td>\n",
              "      <td>2023.0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>International Journal on Recent and Innovation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Functional and operatorial statistics</td>\n",
              "      <td>S. Dabo‐Niang, F. Ferraty</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>39</td>\n",
              "      <td>Yes</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Artificial Neural Networks: Biological Inspira...</td>\n",
              "      <td>Wlodzislaw Duch, J. Kacprzyk, E. Oja...</td>\n",
              "      <td>2005.0</td>\n",
              "      <td>70</td>\n",
              "      <td>Yes</td>\n",
              "      <td>International Conference on Artificial Neural ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Evolutionary Multi-Criterion Optimization, 5th...</td>\n",
              "      <td></td>\n",
              "      <td>2009.0</td>\n",
              "      <td>6</td>\n",
              "      <td>Yes</td>\n",
              "      <td>International Conference on Evolutionary Multi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>Multivariate profiling of neurodegeneration-as...</td>\n",
              "      <td>S. K. Kumarasamy, Yunshi Wang, Vignesh Viswana...</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>3</td>\n",
              "      <td>Yes</td>\n",
              "      <td>BioData Mining</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>The Sixth International Symposium on Neural Ne...</td>\n",
              "      <td>Hongwei Wang, Yi Shen, Tingwen Huang...</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>9</td>\n",
              "      <td>Yes</td>\n",
              "      <td>International Symposium on Neural Networks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ef8c4aa-2adb-4519-b731-145a5a7a45b3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0ef8c4aa-2adb-4519-b731-145a5a7a45b3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0ef8c4aa-2adb-4519-b731-145a5a7a45b3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c94af740-83f5-4edf-8a93-272b1cf0584b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c94af740-83f5-4edf-8a93-272b1cf0584b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c94af740-83f5-4edf-8a93-272b1cf0584b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    main_search()\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 995,\n        \"samples\": [\n          \"Gender identification using frontal facial images\",\n          \"Wavelet De-Noising and Genetic Algorithm-Based Least Squares Twin SVM for Classification of Arrhythmias\",\n          \"Data Classification using Support Vector Machines\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 979,\n        \"samples\": [\n          \"H. Kim, H. Lee, Sangwon Lee...\",\n          \"Siavash Esmaeili Fashtakeh\",\n          \"H. Chethan, G. Kumar, Ramachandra Raghavendra\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.403263686528824,\n        \"min\": 1999.0,\n        \"max\": 2025.0,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          2015.0,\n          2012.0,\n          2009.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Citations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 187,\n        \"min\": 0,\n        \"max\": 5053,\n        \"num_unique_values\": 193,\n        \"samples\": [\n          186,\n          243,\n          80\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PDF\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Venue\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 634,\n        \"samples\": [\n          \"American Control Conference\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Module 1 complete! Results saved to: data/search_results/paper_search_results_Alzheimer_Detection_and_Classification_Using_SVM.json\n",
            "   Proceed to Module 2 for paper selection and PDF download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " Module: 2 Paper Selection & PDF Download"
      ],
      "metadata": {
        "id": "G5T7yJAs0xXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install PyMuPDF requests -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import fitz  # PyMuPDF\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qZY-y7E02OU",
        "outputId": "7ab05297-ad4d-4220-8866-5943be3fee4c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function loads the most recent or specified research paper search results from a JSON file for further processing.\n"
      ],
      "metadata": {
        "id": "Vs_DT6X006F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Research papers\n",
        "def load_search_results(filepath=None):\n",
        "\n",
        "    if not filepath:\n",
        "        results_dir = \"data/search_results\"\n",
        "        if os.path.exists(results_dir):\n",
        "            json_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]\n",
        "            if json_files:\n",
        "                json_files.sort(key=lambda x: os.path.getmtime(os.path.join(results_dir, x)), reverse=True)\n",
        "                filepath = os.path.join(results_dir, json_files[0])\n",
        "                print(f\" Loading most recent search results: {json_files[0]}\")\n",
        "            else:\n",
        "                print(\" No search results found. Run Module 1 first.\")\n",
        "                return None\n",
        "        else:\n",
        "            print(\" Search results directory not found. Run Module 1 first.\")\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        print(f\" Loaded {len(data['papers'])} papers on '{data['topic']}'\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading file: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "2mF4ldgS09xl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions filter papers with valid PDFs, rank them by citations and year, and select the top papers for download.\n"
      ],
      "metadata": {
        "id": "G25cpelm1C1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. PAPER SELECTION\n",
        "def filter_papers_with_pdfs(papers):\n",
        "    papers_with_pdf = []\n",
        "    for paper in papers:\n",
        "        if paper.get(\"pdf_url\") and paper[\"pdf_url\"].strip():\n",
        "            url = paper[\"pdf_url\"].lower()\n",
        "            if url.endswith('.pdf') or '.pdf?' in url or 'pdf' in url:\n",
        "                papers_with_pdf.append(paper)\n",
        "\n",
        "    print(f\"\\n PDF Availability:\")\n",
        "    print(f\"  • Total papers: {len(papers)}\")\n",
        "    print(f\"  • Papers with PDF URLs: {len(papers_with_pdf)}\")\n",
        "\n",
        "    return papers_with_pdf\n",
        "\n",
        "def rank_papers(papers):\n",
        "    valid_papers = []\n",
        "    for paper in papers:\n",
        "        if paper.get(\"year\") and paper.get(\"citationCount\") is not None:\n",
        "            valid_papers.append(paper)\n",
        "    ranked = sorted(valid_papers,\n",
        "                   key=lambda x: (x[\"citationCount\"], x[\"year\"]),\n",
        "                   reverse=True)\n",
        "\n",
        "    return ranked\n",
        "\n",
        "def select_top_papers(papers, count=3):\n",
        "    papers_with_pdf = filter_papers_with_pdfs(papers)\n",
        "    ranked_papers = rank_papers(papers_with_pdf)\n",
        "    selected = ranked_papers[:count]\n",
        "    print(f\"\\n Selected top {len(selected)} papers for download:\")\n",
        "    for i, paper in enumerate(selected):\n",
        "        print(f\"\\n{i+1}. {paper['title'][:70]}...\")\n",
        "        print(f\"   Citations: {paper['citationCount']}\")\n",
        "        print(f\"   Year: {paper['year']}\")\n",
        "        print(f\"   Authors: {', '.join(paper['authors'][:2])}\")\n",
        "\n",
        "    return selected\n"
      ],
      "metadata": {
        "id": "ZYly3ToU1PyH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions securely download selected paper PDFs, verify their validity, extract file details, and store the successfully downloaded papers locally.\n"
      ],
      "metadata": {
        "id": "_ecX2gT_1aqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. PDF DOWNLOAD\n",
        "def download_pdf_with_verification(url, filename, max_retries=2):\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                print(f\"  Attempt {attempt + 1}/{max_retries}...\")\n",
        "                response = requests.get(url, headers=headers, timeout=30)\n",
        "\n",
        "                if response.status_code != 200:\n",
        "                    print(f\"    HTTP Error: {response.status_code}\")\n",
        "                    continue\n",
        "\n",
        "                # Check if it's a PDF\n",
        "                if not (response.content[:4] == b'%PDF' or\n",
        "                       'pdf' in response.headers.get('content-type', '').lower()):\n",
        "                    print(f\"    Not a PDF file\")\n",
        "                    continue\n",
        "\n",
        "                # Save file\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "                # Verify PDF\n",
        "                if verify_pdf(filename):\n",
        "                    size = os.path.getsize(filename)\n",
        "                    print(f\"    Downloaded: {size:,} bytes\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"     Invalid PDF\")\n",
        "                    os.remove(filename)\n",
        "                    continue\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f\"    Timeout\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Error: {str(e)[:50]}\")\n",
        "\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Download failed: {str(e)[:50]}\")\n",
        "        return False\n",
        "\n",
        "def verify_pdf(filepath):\n",
        "    try:\n",
        "        if not os.path.exists(filepath):\n",
        "            return False\n",
        "            if os.path.getsize(filepath) < 1024:  # Less than 1KB\n",
        "              return False\n",
        "        with fitz.open(filepath) as doc:\n",
        "            if len(doc) > 0:\n",
        "                return True\n",
        "        return False\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def get_pdf_info(filepath):\n",
        "    try:\n",
        "        with fitz.open(filepath) as doc:\n",
        "            return {\n",
        "                'pages': len(doc),\n",
        "                'size_bytes': os.path.getsize(filepath),\n",
        "                'size_mb': round(os.path.getsize(filepath) / (1024 * 1024), 2),\n",
        "                'is_valid': True\n",
        "            }\n",
        "    except:\n",
        "        return {'is_valid': False}\n",
        "\n",
        "def download_selected_papers(selected_papers, output_dir=\"downloads\"):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n Starting PDF downloads to: {output_dir}/\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    downloaded_papers = []\n",
        "\n",
        "    for i, paper in enumerate(selected_papers):\n",
        "        print(f\"\\n[{i+1}/{len(selected_papers)}] Downloading: {paper['title'][:60]}...\")\n",
        "\n",
        "        # Create safe filename\n",
        "        safe_title = \"\".join(c for c in paper['title'] if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
        "        if len(safe_title) > 50:\n",
        "            safe_title = safe_title[:50]\n",
        "\n",
        "        filename = f\"{output_dir}/paper_{i+1}_{hashlib.md5(safe_title.encode()).hexdigest()[:8]}.pdf\"\n",
        "\n",
        "        # Download\n",
        "        success = download_pdf_with_verification(paper['pdf_url'], filename)\n",
        "\n",
        "        if success:\n",
        "            # Get PDF info\n",
        "            pdf_info = get_pdf_info(filename)\n",
        "\n",
        "            # Update paper info\n",
        "            paper['downloaded'] = True\n",
        "            paper['local_path'] = filename\n",
        "            paper['download_time'] = datetime.now().isoformat()\n",
        "            paper['pdf_info'] = pdf_info\n",
        "\n",
        "            downloaded_papers.append(paper)\n",
        "            print(f\"    Success! {pdf_info['pages']} pages, {pdf_info['size_mb']} MB\")\n",
        "        else:\n",
        "            paper['downloaded'] = False\n",
        "            print(f\"   Failed to download\")\n",
        "\n",
        "    return downloaded_papers"
      ],
      "metadata": {
        "id": "AXs1TBih1d8R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function saves a detailed JSON report of PDF download results and generates a summary list of successfully downloaded papers.\n"
      ],
      "metadata": {
        "id": "-RJPw2Xf1hdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. SAVE DOWNLOAD INFO\n",
        "def save_download_report(downloaded_papers, topic, output_dir=\"downloads\"):\n",
        "    report = {\n",
        "        'topic': topic,\n",
        "        'download_timestamp': datetime.now().isoformat(),\n",
        "        'total_selected': len(downloaded_papers),\n",
        "        'successful_downloads': sum(1 for p in downloaded_papers if p.get('downloaded', False)),\n",
        "        'failed_downloads': sum(1 for p in downloaded_papers if not p.get('downloaded', False)),\n",
        "        'papers': downloaded_papers\n",
        "    }\n",
        "\n",
        "    os.makedirs(\"data/reports\", exist_ok=True)\n",
        "    report_file = f\"data/reports/download_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(report, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n Download report saved to: {report_file}\")\n",
        "    download_list = []\n",
        "    for paper in downloaded_papers:\n",
        "        if paper.get('downloaded'):\n",
        "            download_list.append({\n",
        "                'title': paper['title'],\n",
        "                'local_file': paper['local_path'],\n",
        "                'size_mb': paper['pdf_info']['size_mb'],\n",
        "                'pages': paper['pdf_info']['pages']\n",
        "            })\n",
        "\n",
        "    list_file = f\"{output_dir}/downloaded_papers_list.json\"\n",
        "    with open(list_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(download_list, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    return report_file"
      ],
      "metadata": {
        "id": "gem7yqh71ks5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function verifies downloaded PDFs by checking their validity, page count, file size, and provides a summary of the download directory.\n"
      ],
      "metadata": {
        "id": "fKYKThHR1oPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. VERIFICATION\n",
        "def verify_downloads(output_dir=\"downloads\"):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" VERIFICATION OF DOWNLOADS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        print(f\" Directory '{output_dir}' does not exist!\")\n",
        "        return 0\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(output_dir) if f.endswith('.pdf')]\n",
        "\n",
        "    print(f\"\\n Directory: {os.path.abspath(output_dir)}\")\n",
        "    print(f\" PDF files found: {len(pdf_files)}\")\n",
        "\n",
        "    if pdf_files:\n",
        "        print(\"\\nFile Details:\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        total_size = 0\n",
        "        valid_files = 0\n",
        "\n",
        "        for pdf in pdf_files:\n",
        "            filepath = os.path.join(output_dir, pdf)\n",
        "            size = os.path.getsize(filepath)\n",
        "            total_size += size\n",
        "\n",
        "            # Verify PDF\n",
        "            if verify_pdf(filepath):\n",
        "                valid_files += 1\n",
        "                with fitz.open(filepath) as doc:\n",
        "                    pages = len(doc)\n",
        "                print(f\" {pdf}\")\n",
        "                print(f\"   Size: {size:,} bytes ({size/1024/1024:.2f} MB)\")\n",
        "                print(f\"   Pages: {pages}\")\n",
        "            else:\n",
        "                print(f\" {pdf} - INVALID PDF\")\n",
        "                print(f\"   Size: {size:,} bytes\")\n",
        "\n",
        "    print(f\"\\n Summary:\")\n",
        "    print(f\"  • Total PDF files: {len(pdf_files)}\")\n",
        "    print(f\"  • Valid PDFs: {valid_files}\")\n",
        "    print(f\"  • Total size: {total_size/1024/1024:.2f} MB\")\n",
        "\n",
        "    return valid_files\n"
      ],
      "metadata": {
        "id": "u2Jy9S3q1sVm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This main function loads search results, selects top papers with PDFs, downloads them, saves a report, verifies the files, and completes Module 2.\n"
      ],
      "metadata": {
        "id": "JpJmKHTo1yLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. MAIN DOWNLOAD FUNCTION\n",
        "\n",
        "def main_download(filepath=None, download_count=3):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODULE 2: PAPER SELECTION & PDF DOWNLOAD\")\n",
        "    print(\"=\"*80)\n",
        "    data = load_search_results(filepath)\n",
        "    if not data:\n",
        "        return None\n",
        "    selected_papers = select_top_papers(data[\"papers\"], count=download_count)\n",
        "\n",
        "    if not selected_papers:\n",
        "        print(\" No papers with PDFs available for download.\")\n",
        "        return None\n",
        "    downloaded = download_selected_papers(selected_papers)\n",
        "    report_file = save_download_report(downloaded, data[\"topic\"])\n",
        "    verify_downloads()\n",
        "\n",
        "    print(f\"\\n Module 2 complete!\")\n",
        "    print(f\"   Downloaded papers are in: downloads/\")\n",
        "    print(f\"   Report saved to: {report_file}\")\n",
        "    print(f\"\\n Milestone 1 complated!\")\n",
        "\n",
        "    return downloaded\n",
        "if __name__ == \"__main__\":\n",
        "    main_download(download_count=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEl2f9R5117T",
        "outputId": "1694c65f-ad0b-480a-b1c7-c24d9e925e5b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MODULE 2: PAPER SELECTION & PDF DOWNLOAD\n",
            "================================================================================\n",
            " Loading most recent search results: paper_search_results_Alzheimer_Detection_and_Classification_Using_SVM.json\n",
            " Loaded 1000 papers on ''Alzheimer Detection and Classification Using SVM'\n",
            "\n",
            " PDF Availability:\n",
            "  • Total papers: 1000\n",
            "  • Papers with PDF URLs: 165\n",
            "\n",
            " Selected top 3 papers for download:\n",
            "\n",
            "1. CNN Features Off-the-Shelf: An Astounding Baseline for Recognition...\n",
            "   Citations: 5053\n",
            "   Year: 2014\n",
            "   Authors: A. Razavian, Hossein Azizpour\n",
            "\n",
            "2. Automatic classification of MR scans in Alzheimer's disease....\n",
            "   Citations: 1205\n",
            "   Year: 2008\n",
            "   Authors: S. Klöppel, C. Stonnington\n",
            "\n",
            "3. Bearing Health Monitoring Based on Hilbert–Huang Transform, Support Ve...\n",
            "   Citations: 584\n",
            "   Year: 2015\n",
            "   Authors: A. Soualhi, K. Medjaher\n",
            "\n",
            " Starting PDF downloads to: downloads/\n",
            "------------------------------------------------------------\n",
            "\n",
            "[1/3] Downloading: CNN Features Off-the-Shelf: An Astounding Baseline for Recog...\n",
            "  Attempt 1/2...\n",
            "    Downloaded: 405,617 bytes\n",
            "    Success! 8 pages, 0.39 MB\n",
            "\n",
            "[2/3] Downloading: Automatic classification of MR scans in Alzheimer's disease....\n",
            "  Attempt 1/2...\n",
            "    HTTP Error: 403\n",
            "  Attempt 2/2...\n",
            "    HTTP Error: 403\n",
            "   Failed to download\n",
            "\n",
            "[3/3] Downloading: Bearing Health Monitoring Based on Hilbert–Huang Transform, ...\n",
            "  Attempt 1/2...\n",
            "    Downloaded: 709,019 bytes\n",
            "    Success! 12 pages, 0.68 MB\n",
            "\n",
            " Download report saved to: data/reports/download_report_20260111_063344.json\n",
            "\n",
            "============================================================\n",
            " VERIFICATION OF DOWNLOADS\n",
            "============================================================\n",
            "\n",
            " Directory: /content/downloads\n",
            " PDF files found: 2\n",
            "\n",
            "File Details:\n",
            "------------------------------------------------------------\n",
            " paper_1_e9243cbb.pdf\n",
            "   Size: 405,617 bytes (0.39 MB)\n",
            "   Pages: 8\n",
            " paper_3_910ac69b.pdf\n",
            "   Size: 709,019 bytes (0.68 MB)\n",
            "   Pages: 12\n",
            "\n",
            " Summary:\n",
            "  • Total PDF files: 2\n",
            "  • Valid PDFs: 2\n",
            "  • Total size: 1.06 MB\n",
            "\n",
            " Module 2 complete!\n",
            "   Downloaded papers are in: downloads/\n",
            "   Report saved to: data/reports/download_report_20260111_063344.json\n",
            "\n",
            " Milestone 1 complated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Milestone-2\n",
        "\n",
        "Module-3\n",
        "\n",
        "PDF TEXT EXTRACTION"
      ],
      "metadata": {
        "id": "fyDRqh5319OA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pymupdf4llm pymupdf -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pymupdf4llm\n",
        "import pymupdf\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj_dONIU2BO8",
        "outputId": "98e36172-b3f6-468e-a54b-eedcfcd123f4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/72.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hConsider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Extracts meaningful text from a PDF using multiple methods with error handling, content checks, and returns the best available text.\n"
      ],
      "metadata": {
        "id": "FDB3mY-a2G0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT EXTRACTION\n",
        "def extract_text_improved(pdf_path):\n",
        "    \"\"\"\n",
        "    Improved text extraction with better error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open PDF and check basic info\n",
        "        doc = pymupdf.open(pdf_path)\n",
        "\n",
        "        # Skip if PDF is encrypted or has very few pages\n",
        "        if doc.is_encrypted:\n",
        "            print(f\" PDF is encrypted, trying to extract anyway...\")\n",
        "\n",
        "        # Check if PDF appears to have content (not just copyright notice)\n",
        "        first_page_text = doc[0].get_text().strip() if len(doc) > 0 else \"\"\n",
        "\n",
        "        # Check for common copyright/takedown notices\n",
        "        copyright_keywords = [\"copyright\", \"removed\", \"deleted\", \"takedown\", \"not available\"]\n",
        "        if any(keyword in first_page_text.lower() for keyword in copyright_keywords):\n",
        "            print(f\"  PDF appears to have copyright restrictions\")\n",
        "            doc.close()\n",
        "            return None  # Skip this PDF\n",
        "\n",
        "        # Extract text using different methods\n",
        "        texts = []\n",
        "\n",
        "        # Method 1: pymupdf4llm for better layout\n",
        "        try:\n",
        "            markdown_text = pymupdf4llm.to_markdown(str(pdf_path))\n",
        "            if markdown_text and len(markdown_text) > 500:\n",
        "                texts.append((\"markdown\", markdown_text))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Method 2: Regular text extraction\n",
        "        full_text = \"\"\n",
        "        for page_num in range(min(50, len(doc))):  # Limit to first 50 pages\n",
        "            page = doc[page_num]\n",
        "            page_text = page.get_text()\n",
        "            if page_text:\n",
        "                full_text += page_text + \"\\n\"\n",
        "\n",
        "        if full_text and len(full_text) > 500:\n",
        "            texts.append((\"regular\", full_text))\n",
        "\n",
        "        doc.close()\n",
        "\n",
        "        # Choose the best extraction\n",
        "        if not texts:\n",
        "            return None\n",
        "\n",
        "        # Prefer markdown if available and substantial\n",
        "        for method, text in texts:\n",
        "            if method == \"markdown\" and len(text) > 1000:\n",
        "                return text\n",
        "\n",
        "        # Otherwise return the longest text\n",
        "        best_text = max(texts, key=lambda x: len(x[1]))[1]\n",
        "        return best_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Extraction error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "yZx_CZsr2Kks"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracts and cleans structured sections (title, abstract, methods, results, etc.) from academic text using header detection, fallback keywords, and robustness checks.\n"
      ],
      "metadata": {
        "id": "VA8UyQa42Peg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SECTION EXTRACTION\n",
        "def extract_sections_improved(text):\n",
        "    \"\"\"\n",
        "    Better section extraction using multiple strategies\n",
        "    \"\"\"\n",
        "    sections = {\n",
        "        \"title\": \"\",\n",
        "        \"abstract\": \"\",\n",
        "        \"introduction\": \"\",\n",
        "        \"methods\": \"\",\n",
        "        \"results\": \"\",\n",
        "        \"conclusion\": \"\",\n",
        "        \"references\": \"\",\n",
        "        \"extracted_text\": text[:20000]  # Keep substantial text\n",
        "    }\n",
        "\n",
        "    if not text or len(text) < 500:\n",
        "        return sections\n",
        "\n",
        "    # Clean text first\n",
        "    text = clean_text_basic(text)\n",
        "\n",
        "    # STRATEGY 1: Look for section headers with numbers\n",
        "    section_headers = {\n",
        "        \"abstract\": [r'abstract', r'summary'],\n",
        "        \"introduction\": [r'1\\.\\s*introduction', r'introduction', r'background'],\n",
        "        \"methods\": [r'2\\.\\s*methods?', r'methods?', r'methodology', r'experiment'],\n",
        "        \"results\": [r'3\\.\\s*results?', r'results?', r'findings?'],\n",
        "        \"conclusion\": [r'4\\.\\s*conclusions?', r'conclusions?', r'discussion'],\n",
        "        \"references\": [r'references?', r'bibliography']\n",
        "    }\n",
        "\n",
        "    # Find all possible section boundaries\n",
        "    lines = text.split('\\n')\n",
        "    section_boundaries = {}\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        line_clean = line.strip().lower()\n",
        "        for section_name, patterns in section_headers.items():\n",
        "            for pattern in patterns:\n",
        "                if re.match(rf'^{pattern}[.:]?\\s*$', line_clean) or \\\n",
        "                   re.search(rf'\\b{pattern}\\b', line_clean) and len(line_clean) < 100:\n",
        "                    section_boundaries[section_name] = i\n",
        "                    break\n",
        "\n",
        "    # Extract sections based on boundaries\n",
        "    if section_boundaries:\n",
        "        sorted_sections = sorted(section_boundaries.items(), key=lambda x: x[1])\n",
        "\n",
        "        for idx, (section_name, line_idx) in enumerate(sorted_sections):\n",
        "            # Get text from this section to next section or end\n",
        "            start_idx = line_idx + 1\n",
        "            if idx + 1 < len(sorted_sections):\n",
        "                end_idx = sorted_sections[idx + 1][1]\n",
        "            else:\n",
        "                end_idx = len(lines)\n",
        "\n",
        "            section_text = '\\n'.join(lines[start_idx:end_idx])\n",
        "            if len(section_text.strip()) > 100:  # Only keep substantial sections\n",
        "                # Limit section length to 5000 chars\n",
        "                sections[section_name] = section_text.strip()[:5000]\n",
        "\n",
        "    # STRATEGY 2: Extract title (first substantial line)\n",
        "    for line in lines[:10]:\n",
        "        line = line.strip()\n",
        "        if 20 < len(line) < 200 and not line.startswith('http'):\n",
        "            sections[\"title\"] = line\n",
        "            break\n",
        "\n",
        "    # STRATEGY 3: If we still don't have sections, use keyword-based extraction\n",
        "    if not any(len(sections[sec]) > 200 for sec in [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]):\n",
        "        sections = extract_by_keywords_fallback(text, sections)\n",
        "\n",
        "    return sections\n",
        "\n",
        "def extract_by_keywords_fallback(text, existing_sections):\n",
        "    \"\"\"\n",
        "    Fallback section extraction using keyword proximity\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Common academic paper keywords for each section\n",
        "    section_keywords = {\n",
        "        \"abstract\": [\"abstract\", \"summary\", \"we present\", \"this paper\"],\n",
        "        \"introduction\": [\"introduction\", \"background\", \"motivation\", \"related work\"],\n",
        "        \"methods\": [\"method\", \"experiment\", \"procedure\", \"dataset\", \"implementation\"],\n",
        "        \"results\": [\"result\", \"finding\", \"table\", \"figure\", \"experiment shows\"],\n",
        "        \"conclusion\": [\"conclusion\", \"discussion\", \"future work\", \"limitations\", \"summary\"]\n",
        "    }\n",
        "\n",
        "    # Split into sentences for better context\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "\n",
        "    for section, keywords in section_keywords.items():\n",
        "        if existing_sections[section]:  # Skip if already found\n",
        "            continue\n",
        "\n",
        "        section_sentences = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sentence_lower = sentence.lower()\n",
        "            if any(keyword in sentence_lower for keyword in keywords):\n",
        "                # Get context around keyword (2 sentences before, 5 after)\n",
        "                start = max(0, i - 2)\n",
        "                end = min(len(sentences), i + 6)\n",
        "                context = ' '.join(sentences[start:end])\n",
        "                section_sentences.append(context)\n",
        "\n",
        "        if section_sentences:\n",
        "            existing_sections[section] = ' '.join(section_sentences)[:5000]  # Limit length\n",
        "\n",
        "    return existing_sections\n",
        "\n",
        "def clean_text_basic(text):\n",
        "    \"\"\"\n",
        "    Basic text cleaning\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Fix common PDF issues\n",
        "    text = re.sub(r'-\\s+', '', text)  # Fix hyphenated words\n",
        "    text = re.sub(r'\\s*-\\s*', '-', text)\n",
        "\n",
        "    # Remove non-printable characters\n",
        "    text = ''.join(char for char in text if ord(char) >= 32 or char == '\\n')\n",
        "\n",
        "    return text.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "jpCBGJDr2S_A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processes a research paper PDF by validating file size, extracting text and sections, assessing content quality, and returning a structured result with status.\n",
        "\n"
      ],
      "metadata": {
        "id": "rcC8IbDh2b-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PAPER PROCESSING\n",
        "def process_paper_smart(pdf_path):\n",
        "    \"\"\"\n",
        "    Smart processing with validation\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing: {pdf_path.name}\")\n",
        "\n",
        "    # First check file size\n",
        "    file_size = pdf_path.stat().st_size\n",
        "    if file_size < 10240:  # Less than 10KB\n",
        "        print(f\" File too small ({file_size:,} bytes), may be empty\")\n",
        "        return None\n",
        "\n",
        "    # Extract text\n",
        "    raw_text = extract_text_improved(pdf_path)\n",
        "\n",
        "    if raw_text is None:\n",
        "        print(f\"  Skipping - copyright restrictions or empty\")\n",
        "        return None\n",
        "\n",
        "    if len(raw_text) < 1000:\n",
        "        print(f\"  Text very short ({len(raw_text):,} chars), may be incomplete\")\n",
        "\n",
        "    print(f\"  Extracted {len(raw_text):,} characters\")\n",
        "\n",
        "    # Extract sections\n",
        "    sections = extract_sections_improved(raw_text)\n",
        "\n",
        "    # Count meaningful sections\n",
        "    meaningful_sections = []\n",
        "    for section_name, content in sections.items():\n",
        "        if content and section_name != \"extracted_text\" and len(content) > 200:\n",
        "            meaningful_sections.append(section_name)\n",
        "\n",
        "    print(f\"   Found {len(meaningful_sections)} meaningful sections\")\n",
        "    for section in meaningful_sections[:3]:  # Show first 3\n",
        "        content = sections[section]\n",
        "        print(f\"    • {section}: {len(content):,} chars\")\n",
        "\n",
        "    # Build result\n",
        "    result = {\n",
        "        \"paper_id\": pdf_path.stem,\n",
        "        \"filename\": pdf_path.name,\n",
        "        \"file_size_bytes\": file_size,\n",
        "        \"total_characters\": len(raw_text),\n",
        "        \"meaningful_sections\": meaningful_sections,\n",
        "        \"sections\": sections,\n",
        "        \"status\": \"success\"\n",
        "    }\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "SbaP34NX2f2e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs end-to-end PDF extraction by processing all downloaded papers, saving structured section data and a summary while reporting progress and skips.\n"
      ],
      "metadata": {
        "id": "MB9d08Oq2jUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN EXTRACTION\n",
        "def extract_all_papers(download_dir=\"downloads\", max_papers=None):\n",
        "    \"\"\"\n",
        "    Extract all papers\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODULE 3: PDF TEXT EXTRACTION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Get PDFs\n",
        "    pdf_files = get_downloaded_papers(download_dir)\n",
        "    if not pdf_files:\n",
        "        print(\" No PDFs found. Run Module 2 first.\")\n",
        "        return []\n",
        "\n",
        "    if max_papers:\n",
        "        pdf_files = pdf_files[:max_papers]\n",
        "\n",
        "    print(f\"\\nProcessing {len(pdf_files)} PDF files...\")\n",
        "\n",
        "    # Process each paper\n",
        "    results = []\n",
        "    skipped = 0\n",
        "\n",
        "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
        "        result = process_paper_smart(pdf_file)\n",
        "        if result:\n",
        "            results.append(result)\n",
        "        else:\n",
        "            skipped += 1\n",
        "\n",
        "    # Save results\n",
        "    if results:\n",
        "        save_results_final(results)\n",
        "\n",
        "    print(f\"\\n Extraction complete!\")\n",
        "    print(f\"   Successfully processed: {len(results)} papers\")\n",
        "    print(f\"   Skipped: {skipped} papers\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_downloaded_papers(download_dir=\"downloads\"):\n",
        "    \"\"\"Get list of PDF files\"\"\"\n",
        "    download_path = Path(download_dir)\n",
        "    if not download_path.exists():\n",
        "        return []\n",
        "\n",
        "    pdf_files = list(download_path.glob(\"*.pdf\"))\n",
        "    return pdf_files\n",
        "\n",
        "def save_results_final(results, output_dir=\"data/extracted\"):\n",
        "    \"\"\"\n",
        "    Save results - FIXED VERSION\n",
        "    \"\"\"\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save individual files\n",
        "    for result in results:\n",
        "        paper_id = result[\"paper_id\"]\n",
        "        output_file = output_path / f\"{paper_id}_extracted.json\"\n",
        "\n",
        "        # Don't save full extracted_text if it's too long\n",
        "        if \"extracted_text\" in result[\"sections\"] and len(result[\"sections\"][\"extracted_text\"]) > 10000:\n",
        "            result[\"sections\"][\"extracted_text\"] = result[\"sections\"][\"extracted_text\"][:10000] + \"...[truncated]\"\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"   Saved: {output_file.name}\")\n",
        "\n",
        "    # Save summary - FIXED: Use datetime instead of Path.timestamp\n",
        "    summary = {\n",
        "        \"extraction_date\": datetime.now().isoformat(),\n",
        "        \"total_papers\": len(results),\n",
        "        \"papers\": [\n",
        "            {\n",
        "                \"paper_id\": r[\"paper_id\"],\n",
        "                \"filename\": r[\"filename\"],\n",
        "                \"file_size\": r[\"file_size_bytes\"],\n",
        "                \"total_chars\": r[\"total_characters\"],\n",
        "                \"sections_found\": r[\"meaningful_sections\"]\n",
        "            }\n",
        "            for r in results\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    summary_file = output_path / \"extraction_summary.json\"\n",
        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n Summary saved to: {summary_file}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pPAmCJK92qag"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzes extracted paper JSON files to report section quality, text volume, and overall extraction success statistics.\n"
      ],
      "metadata": {
        "id": "y70gLlsI2uGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANALYZE RESULTS\n",
        "def analyze_extraction_results():\n",
        "    \"\"\"\n",
        "    Analyze and display extraction results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXTRACTION ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    data_path = Path(\"data/extracted\")\n",
        "    if not data_path.exists():\n",
        "        print(\" No extraction directory found\")\n",
        "        return\n",
        "\n",
        "    # Look for individual paper files\n",
        "    json_files = list(data_path.glob(\"*_extracted.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(\" No extracted paper files found\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFound {len(json_files)} extracted papers:\\n\")\n",
        "\n",
        "    total_chars = 0\n",
        "    papers_with_abstract = 0\n",
        "    papers_with_multiple_sections = 0\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            paper_id = data.get(\"paper_id\", \"Unknown\")\n",
        "            total_chars += data.get(\"total_characters\", 0)\n",
        "\n",
        "            # Get sections\n",
        "            sections = data.get(\"sections\", {})\n",
        "            meaningful_sections = data.get(\"meaningful_sections\", [])\n",
        "\n",
        "            # Count papers with good extraction\n",
        "            if sections.get(\"abstract\") and len(sections[\"abstract\"]) > 200:\n",
        "                papers_with_abstract += 1\n",
        "\n",
        "            if len(meaningful_sections) >= 2:\n",
        "                papers_with_multiple_sections += 1\n",
        "\n",
        "            # Display paper info\n",
        "            print(f\" {paper_id}\")\n",
        "            print(f\"   Size: {data.get('file_size_bytes', 0):,} bytes\")\n",
        "            print(f\"   Text: {data.get('total_characters', 0):,} chars\")\n",
        "            print(f\"   Sections found: {len(meaningful_sections)}\")\n",
        "\n",
        "            # Show some content\n",
        "            if sections.get(\"title\"):\n",
        "                title = sections[\"title\"][:80]\n",
        "                print(f\"   Title: {title}\")\n",
        "\n",
        "            if sections.get(\"abstract\"):\n",
        "                abstract_preview = sections[\"abstract\"][:150]\n",
        "                print(f\"   Abstract: {abstract_preview}...\")\n",
        "\n",
        "            print()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error reading {json_file.name}: {e}\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXTRACTION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total papers processed: {len(json_files)}\")\n",
        "    print(f\"Total characters extracted: {total_chars:,}\")\n",
        "    print(f\"Papers with abstract: {papers_with_abstract}/{len(json_files)}\")\n",
        "    print(f\"Papers with multiple sections: {papers_with_multiple_sections}/{len(json_files)}\")\n"
      ],
      "metadata": {
        "id": "kaOZLuIr2y89"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generates a comprehensive quality review report for extracted papers by validating text cleanliness, section accuracy, coverage, and saving overall success metrics.\n"
      ],
      "metadata": {
        "id": "5gpIjpVk234Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATE REPORT\n",
        "def generate_report():\n",
        "    \"\"\"\n",
        "    Generate a report for mentor review\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"  REVIEW REPORT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    data_path = Path(\"data/extracted\")\n",
        "    if not data_path.exists():\n",
        "        print(\" No extraction directory found\")\n",
        "        return\n",
        "\n",
        "    json_files = list(data_path.glob(\"*_extracted.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(\" No extracted papers found\")\n",
        "        return\n",
        "\n",
        "    report = {\n",
        "        \"generated_date\": datetime.now().isoformat(),\n",
        "        \"total_papers\": len(json_files),\n",
        "        \"quality_checks\": [],\n",
        "        \"papers\": []\n",
        "    }\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            paper_report = {\n",
        "                \"paper_id\": data[\"paper_id\"],\n",
        "                \"filename\": data[\"filename\"],\n",
        "                \"checks\": {\n",
        "                    \"text_clean\": False,\n",
        "                    \"sections_correct\": False,\n",
        "                    \"no_hallucinations\": False,\n",
        "                    \"no_missing_chunks\": False\n",
        "                },\n",
        "                \"section_lengths\": {},\n",
        "                \"issues\": []\n",
        "            }\n",
        "\n",
        "            sections = data.get(\"sections\", {})\n",
        "\n",
        "            # Check 1: Text clean?\n",
        "            sample_text = sections.get(\"abstract\", sections.get(\"extracted_text\", \"\"))\n",
        "            artifacts = ['�', '\\x00', '[?]', '[ ]']\n",
        "            has_artifacts = any(art in sample_text for art in artifacts)\n",
        "            paper_report[\"checks\"][\"text_clean\"] = not has_artifacts\n",
        "\n",
        "            if has_artifacts:\n",
        "                paper_report[\"issues\"].append(\"Text contains extraction artifacts\")\n",
        "\n",
        "            # Check 2: Sections correctly separated?\n",
        "            major_sections = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]\n",
        "            found_sections = [s for s in major_sections if sections.get(s) and len(sections[s]) > 200]\n",
        "            paper_report[\"checks\"][\"sections_correct\"] = len(found_sections) >= 2\n",
        "\n",
        "            if len(found_sections) < 2:\n",
        "                paper_report[\"issues\"].append(f\"Only found {len(found_sections)} major sections\")\n",
        "\n",
        "            # Check 3: No hallucinated chunks?\n",
        "            total_chars = data.get(\"total_characters\", 0)\n",
        "            paper_report[\"checks\"][\"no_hallucinations\"] = 1000 <= total_chars <= 500000\n",
        "\n",
        "            if total_chars < 1000:\n",
        "                paper_report[\"issues\"].append(f\"Text too short: {total_chars} chars\")\n",
        "            elif total_chars > 500000:\n",
        "                paper_report[\"issues\"].append(f\"Text suspiciously long: {total_chars} chars\")\n",
        "\n",
        "            # Check 4: No missing chunks?\n",
        "            section_lengths = sum(len(str(content)) for content in sections.values() if content)\n",
        "            coverage = section_lengths / total_chars if total_chars > 0 else 0\n",
        "            paper_report[\"checks\"][\"no_missing_chunks\"] = coverage >= 0.3\n",
        "\n",
        "            if coverage < 0.3:\n",
        "                paper_report[\"issues\"].append(f\"Low coverage: {coverage:.1%}\")\n",
        "\n",
        "            # Record section lengths\n",
        "            for section, content in sections.items():\n",
        "                if content and len(str(content)) > 50:\n",
        "                    paper_report[\"section_lengths\"][section] = len(str(content))\n",
        "\n",
        "            report[\"papers\"].append(paper_report)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {json_file}: {e}\")\n",
        "\n",
        "    # Calculate overall scores\n",
        "    total_checks = 0\n",
        "    passed_checks = 0\n",
        "\n",
        "    for paper in report[\"papers\"]:\n",
        "        for check_name, passed in paper[\"checks\"].items():\n",
        "            total_checks += 1\n",
        "            if passed:\n",
        "                passed_checks += 1\n",
        "\n",
        "    report[\"overall_score\"] = f\"{passed_checks}/{total_checks}\" if total_checks > 0 else \"N/A\"\n",
        "    report[\"success_rate\"] = passed_checks / total_checks if total_checks > 0 else 0\n",
        "\n",
        "    # Save report\n",
        "    report_file = data_path / \"_review_report.json\"\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n report generated!\")\n",
        "    print(f\"   Overall score: {report['overall_score']}\")\n",
        "    print(f\"   Success rate: {report['success_rate']:.1%}\")\n",
        "    print(f\"   Report saved to: {report_file}\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n QUALITY CHECK SUMMARY:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    check_names = [\"text_clean\", \"sections_correct\", \"no_hallucinations\", \"no_missing_chunks\"]\n",
        "    for check_name in check_names:\n",
        "        passed = sum(1 for paper in report[\"papers\"] if paper[\"checks\"].get(check_name, False))\n",
        "        total = len(report[\"papers\"])\n",
        "        percentage = (passed / total * 100) if total > 0 else 0\n",
        "        status = \"✅\" if percentage >= 70 else \"⚠️ \" if percentage >= 50 else \"❌\"\n",
        "        print(f\"{status} {check_name}: {passed}/{total} ({percentage:.0f}%)\")\n",
        "\n",
        "    return report"
      ],
      "metadata": {
        "id": "-ZqgAwVA28_v"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs the full PDF extraction pipeline end-to-end, including text extraction, section analysis, quality reporting, and sample output display.\n"
      ],
      "metadata": {
        "id": "r4nTuZ7v3A0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN COMPLETE PIPELINE\n",
        "def run_complete_extraction():\n",
        "    \"\"\"\n",
        "    Run the complete extraction pipeline\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PDF TEXT EXTRACTION MODULE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Extract papers\n",
        "    print(\"\\nSTEP 1: Extracting text from PDFs...\")\n",
        "    results = extract_all_papers(max_papers=5)\n",
        "\n",
        "    if not results:\n",
        "        print(\" No papers extracted successfully\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Analyze results\n",
        "    print(\"\\n STEP 2: Analyzing extraction quality...\")\n",
        "    analyze_extraction_results()\n",
        "\n",
        "    # Step 3: Generate mentor report\n",
        "    print(\"\\n STEP 3: Generating eview report...\")\n",
        "    report = generate_report()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nWhat has been accomplished:\")\n",
        "\n",
        "    return results, report\n",
        "\n",
        "# Run the complete pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    results, report = run_complete_extraction()\n",
        "\n",
        "    # Show example of extracted content\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXAMPLE OF EXTRACTED CONTENT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        first_paper = results[0]\n",
        "        sections = first_paper[\"sections\"]\n",
        "\n",
        "        print(f\"\\nPaper: {first_paper['paper_id']}\")\n",
        "\n",
        "        for section_name in [\"title\", \"abstract\", \"introduction\"]:\n",
        "            if sections.get(section_name) and len(sections[section_name]) > 50:\n",
        "                content = sections[section_name]\n",
        "                print(f\"\\n{section_name.upper()}:\")\n",
        "                print(\"-\" * 40)\n",
        "                # Show reasonable amount of text\n",
        "                preview = content[:500] + \"...\" if len(content) > 500 else content\n",
        "                print(preview)\n",
        "                print(f\"[Total length: {len(content):,} characters]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLn442s13Em9",
        "outputId": "24e1d1e5-99b4-4be1-c87e-effed766617f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PDF TEXT EXTRACTION MODULE\n",
            "================================================================================\n",
            "\n",
            "STEP 1: Extracting text from PDFs...\n",
            "\n",
            "================================================================================\n",
            "MODULE 3: PDF TEXT EXTRACTION\n",
            "================================================================================\n",
            "\n",
            "Processing 2 PDF files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing PDFs:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: paper_1_e9243cbb.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing PDFs:  50%|█████     | 1/2 [00:07<00:07,  7.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Extracted 42,410 characters\n",
            "   Found 5 meaningful sections\n",
            "    • abstract: 3,530 chars\n",
            "    • introduction: 2,420 chars\n",
            "    • methods: 5,000 chars\n",
            "\n",
            "Processing: paper_3_910ac69b.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PDFs: 100%|██████████| 2/2 [00:25<00:00, 12.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Extracted 55,418 characters\n",
            "   Found 5 meaningful sections\n",
            "    • abstract: 4,012 chars\n",
            "    • introduction: 2,134 chars\n",
            "    • methods: 5,000 chars\n",
            "   Saved: paper_1_e9243cbb_extracted.json\n",
            "   Saved: paper_3_910ac69b_extracted.json\n",
            "\n",
            " Summary saved to: data/extracted/extraction_summary.json\n",
            "\n",
            " Extraction complete!\n",
            "   Successfully processed: 2 papers\n",
            "   Skipped: 0 papers\n",
            "\n",
            " STEP 2: Analyzing extraction quality...\n",
            "\n",
            "================================================================================\n",
            "EXTRACTION ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Found 2 extracted papers:\n",
            "\n",
            " paper_1_e9243cbb\n",
            "   Size: 405,617 bytes\n",
            "   Text: 42,410 chars\n",
            "   Sections found: 5\n",
            "   Abstract: ## **CNN Features off-the-shelf: an Astounding Baseline for Recognition** Ali Sharif Razavian Hossein Azizpour Josephine Sullivan Stefan Carlsson CVAP...\n",
            "\n",
            " paper_3_910ac69b\n",
            "   Size: 709,019 bytes\n",
            "   Text: 55,418 chars\n",
            "   Sections found: 5\n",
            "   Abstract:  Soualhi, K  Medjaher, and N  Zerhouni _**Abstract**_ **— the detection, diagnostic and prognostic of bearing** **degradation play a key role in incre...\n",
            "\n",
            "\n",
            "============================================================\n",
            "EXTRACTION SUMMARY\n",
            "============================================================\n",
            "Total papers processed: 2\n",
            "Total characters extracted: 97,828\n",
            "Papers with abstract: 2/2\n",
            "Papers with multiple sections: 2/2\n",
            "\n",
            " STEP 3: Generating eview report...\n",
            "\n",
            "================================================================================\n",
            "  REVIEW REPORT\n",
            "================================================================================\n",
            "\n",
            " report generated!\n",
            "   Overall score: 8/8\n",
            "   Success rate: 100.0%\n",
            "   Report saved to: data/extracted/_review_report.json\n",
            "\n",
            " QUALITY CHECK SUMMARY:\n",
            "----------------------------------------\n",
            "✅ text_clean: 2/2 (100%)\n",
            "✅ sections_correct: 2/2 (100%)\n",
            "✅ no_hallucinations: 2/2 (100%)\n",
            "✅ no_missing_chunks: 2/2 (100%)\n",
            "\n",
            "================================================================================\n",
            " COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "What has been accomplished:\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE OF EXTRACTED CONTENT\n",
            "================================================================================\n",
            "\n",
            "Paper: paper_1_e9243cbb\n",
            "\n",
            "ABSTRACT:\n",
            "----------------------------------------\n",
            "## **CNN Features off-the-shelf: an Astounding Baseline for Recognition** Ali Sharif Razavian Hossein Azizpour Josephine Sullivan Stefan Carlsson CVAP, KTH (Royal Institute of Technology) Stockholm, Sweden _{_ razavian,azizpour,sullivan,stefanc _}_ @csc kth se **Abstract** _Recent results indicate that the generic descriptors ex-_ _tracted from the convolutional neural networks are very_ _powerful  This paper adds to the mounting evidence that_ _this is indeed the case  We report on a series of ...\n",
            "[Total length: 3,530 characters]\n",
            "\n",
            "INTRODUCTION:\n",
            "----------------------------------------\n",
            " The_ _results strongly suggest that features obtained from deep_ _learning with convolutional nets should be the primary can-_ _didate in most visual recognition tasks _ **1  Introduction** _“Deep learning _ _How well do you think it would work_ _for your computer vision problem ”_ Most likely this question has been posed in your group’s coffee room  And in response someone has quoted recent success stories [29, 15, 10] and someone else professed skepticism  You may have left the coffee room sl...\n",
            "[Total length: 2,420 characters]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module 4:CROSS-PAPER ANALYSIS\n",
        "\n",
        "Week-4"
      ],
      "metadata": {
        "id": "3xRHH25K3Qzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn numpy -q\n",
        "\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "1vzrzq0n3VqS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads all extracted paper JSON files from disk, reports their sizes, and returns them as a list for further analysis.\n"
      ],
      "metadata": {
        "id": "J9lJsDvE3aCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. LOAD EXTRACTED PAPERS\n",
        "\n",
        "def load_extracted_papers(data_dir=\"data/extracted\"):\n",
        "    \"\"\"\n",
        "    Load all extracted papers from JSON files\n",
        "    \"\"\"\n",
        "    data_path = Path(data_dir)\n",
        "    papers = []\n",
        "\n",
        "    # Load individual paper files\n",
        "    json_files = list(data_path.glob(\"*_extracted.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(\"No extracted papers found. Run Module 3 first.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Loading {len(json_files)} extracted papers...\")\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                papers.append(data)\n",
        "                print(f\"  ✓ {data['paper_id']}: {data['total_characters']:,} chars\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {json_file}: {e}\")\n",
        "\n",
        "    return papers\n"
      ],
      "metadata": {
        "id": "UaAEm53h3eOY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performs an in-depth analysis of a single extracted paper by evaluating its structure, research quality, key insights, and generating future research recommendations.\n"
      ],
      "metadata": {
        "id": "QWHr5dBG3qny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. SINGLE PAPER ANALYSIS\n",
        "\n",
        "\n",
        "def analyze_single_paper(paper):\n",
        "    \"\"\"\n",
        "    Analyze a single paper deeply when we don't have multiple papers\n",
        "    \"\"\"\n",
        "    print(\"\\n Performing deep analysis of single paper...\")\n",
        "\n",
        "    info = extract_key_information(paper)\n",
        "\n",
        "    # Create a comprehensive analysis\n",
        "    analysis = {\n",
        "        \"paper_id\": info[\"paper_id\"],\n",
        "        \"title\": info[\"title\"],\n",
        "        \"year\": info[\"year\"],\n",
        "        \"methods_used\": info[\"methods\"],\n",
        "        \"datasets_mentioned\": info[\"datasets\"],\n",
        "        \"key_findings\": info[\"key_findings\"],\n",
        "        \"limitations\": info[\"limitations\"],\n",
        "        \"contributions\": info[\"contributions\"],\n",
        "        \"metrics_reported\": info[\"metrics\"],\n",
        "        \"paper_structure\": analyze_paper_structure(paper),\n",
        "        \"research_quality_indicators\": assess_research_quality(info),\n",
        "        \"recommendations_for_future_research\": generate_recommendations(info)\n",
        "    }\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def analyze_paper_structure(paper):\n",
        "    \"\"\"\n",
        "    Analyze the structure and completeness of the paper\n",
        "    \"\"\"\n",
        "    sections = paper[\"sections\"]\n",
        "    structure = {\n",
        "        \"sections_present\": [],\n",
        "        \"sections_missing\": [],\n",
        "        \"section_lengths\": {}\n",
        "    }\n",
        "\n",
        "    expected_sections = [\"title\", \"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]\n",
        "\n",
        "    for section in expected_sections:\n",
        "        content = sections.get(section, \"\")\n",
        "        if content and len(content) > 50:\n",
        "            structure[\"sections_present\"].append(section)\n",
        "            structure[\"section_lengths\"][section] = len(content)\n",
        "        else:\n",
        "            structure[\"sections_missing\"].append(section)\n",
        "\n",
        "    return structure\n",
        "\n",
        "def assess_research_quality(info):\n",
        "    \"\"\"\n",
        "    Assess the quality of research based on extracted information\n",
        "    \"\"\"\n",
        "    quality_indicators = {\n",
        "        \"has_methods\": len(info[\"methods\"]) > 0,\n",
        "        \"has_datasets\": len(info[\"datasets\"]) > 0,\n",
        "        \"has_findings\": len(info[\"key_findings\"]) > 0,\n",
        "        \"has_limitations\": len(info[\"limitations\"]) > 0,\n",
        "        \"has_metrics\": len(info[\"metrics\"]) > 0,\n",
        "        \"method_diversity\": len(info[\"methods\"]),\n",
        "        \"finding_clarity\": len(info[\"key_findings\"])\n",
        "    }\n",
        "\n",
        "    # Score calculation\n",
        "    score = 0\n",
        "    max_score = 7\n",
        "\n",
        "    if quality_indicators[\"has_methods\"]: score += 1\n",
        "    if quality_indicators[\"has_datasets\"]: score += 1\n",
        "    if quality_indicators[\"has_findings\"]: score += 1\n",
        "    if quality_indicators[\"has_limitations\"]: score += 1\n",
        "    if quality_indicators[\"has_metrics\"]: score += 1\n",
        "    if quality_indicators[\"method_diversity\"] >= 2: score += 1\n",
        "    if quality_indicators[\"finding_clarity\"] >= 2: score += 1\n",
        "\n",
        "    quality_indicators[\"overall_score\"] = f\"{score}/{max_score}\"\n",
        "    quality_indicators[\"percentage\"] = (score / max_score) * 100\n",
        "\n",
        "    return quality_indicators\n",
        "\n",
        "def generate_recommendations(info):\n",
        "    \"\"\"\n",
        "    Generate recommendations based on paper analysis\n",
        "    \"\"\"\n",
        "    recommendations = []\n",
        "\n",
        "    # Based on methods used\n",
        "    methods = info.get(\"methods\", [])\n",
        "    if methods:\n",
        "        recommendations.append(f\"Consider comparing with other papers using: {methods[0]}\")\n",
        "\n",
        "    # Based on limitations\n",
        "    limitations = info.get(\"limitations\", [])\n",
        "    if limitations:\n",
        "        recommendations.append(f\"Address limitations mentioned: {limitations[0][:100]}...\")\n",
        "\n",
        "    # Based on datasets\n",
        "    datasets = info.get(\"datasets\", [])\n",
        "    if datasets:\n",
        "        recommendations.append(f\"Explore other datasets in addition to those mentioned\")\n",
        "\n",
        "    # General recommendations\n",
        "    recommendations.append(\"Compare with recent papers in the same field\")\n",
        "    recommendations.append(\"Explore alternative methodologies mentioned in related work\")\n",
        "\n",
        "    return recommendations[:3]\n"
      ],
      "metadata": {
        "id": "JNdtZOKW3-PH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracts core research details such as year, methods, datasets, findings, limitations, contributions, and metrics from an extracted paper using rule-based text analysis.\n"
      ],
      "metadata": {
        "id": "Cjp0nZKQ4IsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. KEY INFORMATION EXTRACTION (Same as before)\n",
        "\n",
        "def extract_key_information(paper):\n",
        "    \"\"\"\n",
        "    Extract key information from a single paper\n",
        "    \"\"\"\n",
        "    info = {\n",
        "        \"paper_id\": paper[\"paper_id\"],\n",
        "        \"title\": paper[\"sections\"].get(\"title\", \"Unknown\"),\n",
        "        \"year\": extract_year(paper),\n",
        "        \"methods\": extract_methods(paper),\n",
        "        \"datasets\": extract_datasets(paper),\n",
        "        \"key_findings\": extract_key_findings(paper),\n",
        "        \"limitations\": extract_limitations(paper),\n",
        "        \"contributions\": extract_contributions(paper),\n",
        "        \"metrics\": extract_metrics(paper)\n",
        "    }\n",
        "\n",
        "    return info\n",
        "\n",
        "def extract_year(paper):\n",
        "    \"\"\"\n",
        "    Extract year from paper (from title or text)\n",
        "    \"\"\"\n",
        "    title = paper[\"sections\"].get(\"title\", \"\")\n",
        "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', title)\n",
        "    if year_match:\n",
        "        return year_match.group()\n",
        "\n",
        "    text = paper[\"sections\"].get(\"extracted_text\", \"\")\n",
        "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', text[:5000])\n",
        "    if year_match:\n",
        "        return year_match.group()\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "def extract_methods(paper):\n",
        "    \"\"\"\n",
        "    Extract methods/approaches used\n",
        "    \"\"\"\n",
        "    methods_text = paper[\"sections\"].get(\"methods\", \"\")\n",
        "    if not methods_text:\n",
        "        methods_text = paper[\"sections\"].get(\"extracted_text\", \"\")[:5000]\n",
        "\n",
        "    method_keywords = [\n",
        "        \"deep learning\", \"machine learning\", \"neural network\", \"transformer\",\n",
        "        \"cnn\", \"rnn\", \"lstm\", \"bert\", \"gpt\", \"reinforcement learning\",\n",
        "        \"statistical\", \"regression\", \"classification\", \"clustering\",\n",
        "        \"svm\", \"random forest\", \"xgboost\", \"bayesian\", \"monte carlo\",\n",
        "        \"simulation\", \"experiment\", \"analysis\", \"framework\", \"model\",\n",
        "        \"algorithm\", \"approach\", \"technique\", \"methodology\"\n",
        "    ]\n",
        "\n",
        "    found_methods = []\n",
        "    sentences = re.split(r'[.!?]+', methods_text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for keyword in method_keywords:\n",
        "            if keyword in sentence and len(sentence) > 20:\n",
        "                clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "                if clean_sentence not in found_methods:\n",
        "                    found_methods.append(clean_sentence[:200])\n",
        "                    break\n",
        "\n",
        "    if not found_methods:\n",
        "        results_text = paper[\"sections\"].get(\"results\", \"\")[:1000]\n",
        "        conclusion_text = paper[\"sections\"].get(\"conclusion\", \"\")[:1000]\n",
        "        combined = results_text + \" \" + conclusion_text\n",
        "\n",
        "        for sentence in re.split(r'[.!?]+', combined.lower()):\n",
        "            for keyword in method_keywords[:10]:\n",
        "                if keyword in sentence and len(sentence) > 20:\n",
        "                    clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "                    if clean_sentence not in found_methods:\n",
        "                        found_methods.append(clean_sentence[:200])\n",
        "                        break\n",
        "\n",
        "    return found_methods[:5]\n",
        "\n",
        "def extract_datasets(paper):\n",
        "    \"\"\"\n",
        "    Extract datasets mentioned\n",
        "    \"\"\"\n",
        "    text = paper[\"sections\"].get(\"extracted_text\", \"\")[:10000].lower()\n",
        "\n",
        "    dataset_patterns = [\n",
        "        r'imagenet', r'cifar', r'mnist', r'coco', r'pascal voc',\n",
        "        r'wikitext', r'bookcorpus', r'squad', r'glue', r'superglue',\n",
        "        r'kaggle', r'uci', r'pubmed', r'arxiv', r'google scholar',\n",
        "        r'dataset', r'corpus', r'benchmark', r'repository'\n",
        "    ]\n",
        "\n",
        "    data_keywords = [\"data\", \"dataset\", \"corpus\", \"collection\", \"benchmark\"]\n",
        "\n",
        "    found_datasets = []\n",
        "\n",
        "    for pattern in dataset_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            found_datasets.append(pattern)\n",
        "\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in data_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()[:150]\n",
        "            if clean_sentence not in found_datasets:\n",
        "                found_datasets.append(clean_sentence)\n",
        "\n",
        "    return list(set(found_datasets))[:5]\n",
        "\n",
        "def extract_key_findings(paper):\n",
        "    \"\"\"\n",
        "    Extract key findings/results\n",
        "    \"\"\"\n",
        "    findings_text = paper[\"sections\"].get(\"results\", \"\")\n",
        "    if not findings_text:\n",
        "        findings_text = paper[\"sections\"].get(\"conclusion\", \"\")\n",
        "    if not findings_text:\n",
        "        findings_text = paper[\"sections\"].get(\"extracted_text\", \"\")[:3000]\n",
        "\n",
        "    result_keywords = [\n",
        "        \"result shows\", \"findings show\", \"we found\", \"we demonstrate\",\n",
        "        \"achieves\", \"outperforms\", \"improves\", \"increases\", \"reduces\",\n",
        "        \"accuracy\", \"precision\", \"recall\", \"f1\", \"score\", \"performance\",\n",
        "        \"significant\", \"better than\", \"compared to\", \"surpasses\"\n",
        "    ]\n",
        "\n",
        "    findings = []\n",
        "    sentences = re.split(r'[.!?]+', findings_text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in result_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "            if len(clean_sentence) > 30 and clean_sentence not in findings:\n",
        "                findings.append(clean_sentence[:300])\n",
        "\n",
        "    if len(findings) < 2:\n",
        "        conclusion_text = paper[\"sections\"].get(\"conclusion\", \"\")[:2000]\n",
        "        if conclusion_text:\n",
        "            conclusion_sentences = re.split(r'[.!?]+', conclusion_text.lower())\n",
        "            for i, sentence in enumerate(conclusion_sentences[:5]):\n",
        "                if len(sentence) > 50:\n",
        "                    findings.append(sentence.strip()[:300])\n",
        "\n",
        "    return findings[:5]\n",
        "\n",
        "def extract_limitations(paper):\n",
        "    \"\"\"\n",
        "    Extract limitations mentioned\n",
        "    \"\"\"\n",
        "    text = paper[\"sections\"].get(\"conclusion\", \"\")\n",
        "    if not text:\n",
        "        text = paper[\"sections\"].get(\"extracted_text\", \"\")[:5000]\n",
        "\n",
        "    limitation_keywords = [\n",
        "        \"limitation\", \"drawback\", \"shortcoming\", \"weakness\",\n",
        "        \"future work\", \"further research\", \"need to\", \"could be improved\",\n",
        "        \"challenge\", \"difficulty\", \"issue\", \"problem\", \"not consider\",\n",
        "        \"assumption\", \"restriction\", \"constraint\", \"only work\"\n",
        "    ]\n",
        "\n",
        "    limitations = []\n",
        "    sentences = re.split(r'[.!?]+', text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in limitation_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "            if len(clean_sentence) > 30 and clean_sentence not in limitations:\n",
        "                limitations.append(clean_sentence[:300])\n",
        "\n",
        "    return limitations[:3]\n",
        "\n",
        "def extract_contributions(paper):\n",
        "    \"\"\"\n",
        "    Extract paper contributions\n",
        "    \"\"\"\n",
        "    abstract = paper[\"sections\"].get(\"abstract\", \"\")[:1000]\n",
        "    introduction = paper[\"sections\"].get(\"introduction\", \"\")[:1000]\n",
        "    text = abstract + \" \" + introduction\n",
        "\n",
        "    contribution_keywords = [\n",
        "        \"contribution\", \"contribute\", \"propose\", \"introduce\",\n",
        "        \"novel\", \"new method\", \"new approach\", \"we present\",\n",
        "        \"this paper\", \"our work\", \"main contribution\", \"key contribution\"\n",
        "    ]\n",
        "\n",
        "    contributions = []\n",
        "    sentences = re.split(r'[.!?]+', text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in contribution_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "            if len(clean_sentence) > 30 and clean_sentence not in contributions:\n",
        "                contributions.append(clean_sentence[:300])\n",
        "\n",
        "    return contributions[:3]\n",
        "\n",
        "def extract_metrics(paper):\n",
        "    \"\"\"\n",
        "    Extract performance metrics mentioned\n",
        "    \"\"\"\n",
        "    results_text = paper[\"sections\"].get(\"results\", \"\")\n",
        "    if not results_text:\n",
        "        return []\n",
        "\n",
        "    metric_patterns = [\n",
        "        r'accuracy\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'precision\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'recall\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'f1[\\s\\-]?score\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'auc\\s*[:=]\\s*\\d+\\.?\\d*',\n",
        "        r'mae\\s*[:=]\\s*\\d+\\.?\\d*',\n",
        "        r'rmse\\s*[:=]\\s*\\d+\\.?\\d*',\n",
        "        r'\\d+\\.?\\d*\\s*%'\n",
        "    ]\n",
        "\n",
        "    metrics = []\n",
        "    for pattern in metric_patterns:\n",
        "        matches = re.findall(pattern, results_text.lower())\n",
        "        metrics.extend(matches)\n",
        "\n",
        "    return list(set(metrics))[:5]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fURcHlL-4MkV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compares multiple research papers to identify similarities, differences, trends, research gaps, and semantic similarity scores.\n"
      ],
      "metadata": {
        "id": "RNCCUhLT4Q_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. COMPARISON FUNCTIONS\n",
        "\n",
        "def compare_papers(papers_info):\n",
        "    \"\"\"\n",
        "    Compare multiple papers and find similarities/differences\n",
        "    \"\"\"\n",
        "    print(f\"\\n Comparing {len(papers_info)} papers...\")\n",
        "\n",
        "    comparison = {\n",
        "        \"total_papers\": len(papers_info),\n",
        "        \"papers\": papers_info,\n",
        "        \"similarities\": find_similarities(papers_info),\n",
        "        \"differences\": find_differences(papers_info),\n",
        "        \"common_methods\": find_common_elements(papers_info, \"methods\"),\n",
        "        \"common_datasets\": find_common_elements(papers_info, \"datasets\"),\n",
        "        \"timeline_analysis\": analyze_timeline(papers_info),\n",
        "        \"research_gaps\": identify_research_gaps(papers_info)\n",
        "    }\n",
        "\n",
        "    return comparison\n",
        "\n",
        "def find_similarities(papers_info):\n",
        "    \"\"\"\n",
        "    Find similarities between papers\n",
        "    \"\"\"\n",
        "    similarities = {\n",
        "        \"methods\": defaultdict(int),\n",
        "        \"datasets\": defaultdict(int),\n",
        "        \"findings\": defaultdict(int)\n",
        "    }\n",
        "\n",
        "    for paper in papers_info:\n",
        "        for method in paper.get(\"methods\", []):\n",
        "            key = method[:50].lower()\n",
        "            similarities[\"methods\"][key] += 1\n",
        "\n",
        "        for dataset in paper.get(\"datasets\", []):\n",
        "            key = dataset[:50].lower()\n",
        "            similarities[\"datasets\"][key] += 1\n",
        "\n",
        "        for finding in paper.get(\"key_findings\", []):\n",
        "            key = finding[:50].lower()\n",
        "            similarities[\"findings\"][key] += 1\n",
        "\n",
        "    similar_items = {\n",
        "        \"methods\": [item for item, count in similarities[\"methods\"].items()\n",
        "                   if count > 1 and len(item) > 10],\n",
        "        \"datasets\": [item for item, count in similarities[\"datasets\"].items()\n",
        "                    if count > 1 and len(item) > 10],\n",
        "        \"findings\": [item for item, count in similarities[\"findings\"].items()\n",
        "                    if count > 1 and len(item) > 10]\n",
        "    }\n",
        "\n",
        "    return similar_items\n",
        "\n",
        "def find_differences(papers_info):\n",
        "    \"\"\"\n",
        "    Find unique aspects of each paper\n",
        "    \"\"\"\n",
        "    differences = {\n",
        "        \"unique_methods\": defaultdict(list),\n",
        "        \"unique_datasets\": defaultdict(list),\n",
        "        \"unique_findings\": defaultdict(list)\n",
        "    }\n",
        "\n",
        "    all_methods = set()\n",
        "    all_datasets = set()\n",
        "    all_findings = set()\n",
        "\n",
        "    paper_methods = defaultdict(set)\n",
        "    paper_datasets = defaultdict(set)\n",
        "    paper_findings = defaultdict(set)\n",
        "\n",
        "    for paper in papers_info:\n",
        "        paper_id = paper[\"paper_id\"]\n",
        "\n",
        "        for method in paper.get(\"methods\", []):\n",
        "            key = method[:50].lower()\n",
        "            all_methods.add(key)\n",
        "            paper_methods[paper_id].add(key)\n",
        "\n",
        "        for dataset in paper.get(\"datasets\", []):\n",
        "            key = dataset[:50].lower()\n",
        "            all_datasets.add(key)\n",
        "            paper_datasets[paper_id].add(key)\n",
        "\n",
        "        for finding in paper.get(\"key_findings\", []):\n",
        "            key = finding[:50].lower()\n",
        "            all_findings.add(key)\n",
        "            paper_findings[paper_id].add(key)\n",
        "\n",
        "    for paper_id in paper_methods.keys():\n",
        "        unique_methods = paper_methods[paper_id] - set().union(\n",
        "            *(paper_methods[pid] for pid in paper_methods if pid != paper_id)\n",
        "        )\n",
        "        if unique_methods:\n",
        "            differences[\"unique_methods\"][paper_id] = list(unique_methods)[:3]\n",
        "\n",
        "        unique_datasets = paper_datasets[paper_id] - set().union(\n",
        "            *(paper_datasets[pid] for pid in paper_datasets if pid != paper_id)\n",
        "        )\n",
        "        if unique_datasets:\n",
        "            differences[\"unique_datasets\"][paper_id] = list(unique_datasets)[:3]\n",
        "\n",
        "        unique_findings = paper_findings[paper_id] - set().union(\n",
        "            *(paper_findings[pid] for pid in paper_findings if pid != paper_id)\n",
        "        )\n",
        "        if unique_findings:\n",
        "            differences[\"unique_findings\"][paper_id] = list(unique_findings)[:3]\n",
        "\n",
        "    return differences\n",
        "\n",
        "def find_common_elements(papers_info, element_type):\n",
        "    \"\"\"\n",
        "    Find common methods, datasets, etc.\n",
        "    \"\"\"\n",
        "    element_sets = []\n",
        "    for paper in papers_info:\n",
        "        elements = paper.get(element_type, [])\n",
        "        element_set = set(e[:50].lower() for e in elements if len(e) > 10)\n",
        "        element_sets.append(element_set)\n",
        "\n",
        "    if element_sets:\n",
        "        common = set.intersection(*element_sets)\n",
        "        return list(common)[:5]\n",
        "\n",
        "    return []\n",
        "\n",
        "def analyze_timeline(papers_info):\n",
        "    \"\"\"\n",
        "    Analyze temporal trends\n",
        "    \"\"\"\n",
        "    years = []\n",
        "    for paper in papers_info:\n",
        "        year = paper.get(\"year\", \"Unknown\")\n",
        "        if year.isdigit() and 1900 <= int(year) <= 2100:\n",
        "            years.append(int(year))\n",
        "\n",
        "    if len(years) >= 2:\n",
        "        timeline = {\n",
        "            \"earliest\": min(years) if years else \"Unknown\",\n",
        "            \"latest\": max(years) if years else \"Unknown\",\n",
        "            \"range\": max(years) - min(years) if len(years) >= 2 else 0,\n",
        "            \"count_by_year\": {year: years.count(year) for year in set(years)}\n",
        "        }\n",
        "    else:\n",
        "        timeline = {\"note\": \"Insufficient year data\"}\n",
        "\n",
        "    return timeline\n",
        "\n",
        "def identify_research_gaps(papers_info):\n",
        "    \"\"\"\n",
        "    Identify potential research gaps\n",
        "    \"\"\"\n",
        "    gaps = []\n",
        "\n",
        "    all_limitations = []\n",
        "    for paper in papers_info:\n",
        "        limitations = paper.get(\"limitations\", [])\n",
        "        all_limitations.extend(limitations)\n",
        "\n",
        "    limitation_counts = defaultdict(int)\n",
        "    for limitation in all_limitations:\n",
        "        key = limitation[:100].lower()\n",
        "        limitation_counts[key] += 1\n",
        "\n",
        "    frequent_limitations = [lim for lim, count in limitation_counts.items()\n",
        "                          if count > 1 and len(lim) > 20]\n",
        "\n",
        "    if frequent_limitations:\n",
        "        gaps.append(\"Common limitations mentioned across papers:\")\n",
        "        gaps.extend(frequent_limitations[:3])\n",
        "\n",
        "    methods_used = set()\n",
        "    datasets_used = set()\n",
        "\n",
        "    for paper in papers_info:\n",
        "        methods_used.update(m.lower() for m in paper.get(\"methods\", []))\n",
        "        datasets_used.update(d.lower() for d in paper.get(\"datasets\", []))\n",
        "\n",
        "    common_methods_in_field = [\n",
        "        \"deep learning\", \"transfer learning\", \"reinforcement learning\",\n",
        "        \"explainable ai\", \"few-shot learning\", \"meta learning\"\n",
        "    ]\n",
        "\n",
        "    missing_methods = [m for m in common_methods_in_field\n",
        "                      if m not in methods_used]\n",
        "\n",
        "    if missing_methods:\n",
        "        gaps.append(\"Potentially unexplored methods in these papers:\")\n",
        "        gaps.extend(missing_methods[:3])\n",
        "\n",
        "    return gaps[:5]\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity_scores(papers_info):\n",
        "    \"\"\"\n",
        "    Calculate similarity scores between papers\n",
        "    \"\"\"\n",
        "    paper_texts = []\n",
        "    paper_ids = []\n",
        "\n",
        "    for idx, paper in enumerate(papers_info):\n",
        "        text_parts = [\n",
        "            paper.get(\"title\", \"\"),\n",
        "            paper.get(\"sections\", {}).get(\"abstract\", \"\")[:1000],\n",
        "            \" \".join(paper.get(\"key_findings\", []))\n",
        "        ]\n",
        "\n",
        "        combined_text = \" \".join(text_parts)\n",
        "        paper_texts.append(combined_text)\n",
        "\n",
        "        # safe paper_id\n",
        "        paper_ids.append(paper.get(\"paper_id\", f\"paper_{idx}\"))\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "    tfidf_matrix = vectorizer.fit_transform(paper_texts)\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    similarity_scores = {}\n",
        "    for i in range(len(paper_ids)):\n",
        "        paper_id = paper_ids[i]\n",
        "        similarity_scores[paper_id] = {}\n",
        "\n",
        "        for j in range(len(paper_ids)):\n",
        "            if i != j:\n",
        "                other_id = paper_ids[j]\n",
        "                score = similarity_matrix[i][j]\n",
        "                similarity_scores[paper_id][other_id] = round(float(score), 3)\n",
        "\n",
        "    return similarity_scores\n"
      ],
      "metadata": {
        "id": "rWzkZTTm4WnG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saves single-paper or multi-paper analysis results to JSON and generates detailed text reports for review.\n"
      ],
      "metadata": {
        "id": "9UXV1uJl4ijy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. SAVE RESULTS\n",
        "def save_results(analysis_type, data, output_dir=\"data/analysis\"):\n",
        "    \"\"\"\n",
        "    Save analysis results\n",
        "    \"\"\"\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if analysis_type == \"single\":\n",
        "        output_file = output_path / \"single_paper_analysis.json\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"   Single paper analysis saved to: {output_file}\")\n",
        "\n",
        "        # Also generate a summary report\n",
        "        generate_single_paper_report(data, output_path)\n",
        "\n",
        "    elif analysis_type == \"comparison\":\n",
        "        comparison_file = output_path / \"comparison.json\"\n",
        "        with open(comparison_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data[\"comparison\"], f, indent=2, ensure_ascii=False)\n",
        "        print(f\"  Comparison saved to: {comparison_file}\")\n",
        "\n",
        "        similarity_file = output_path / \"similarity_scores.json\"\n",
        "        with open(similarity_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data[\"similarity_scores\"], f, indent=2, ensure_ascii=False)\n",
        "        print(f\"   Similarity scores saved to: {similarity_file}\")\n",
        "\n",
        "        generate_comparison_report(data, output_path)\n",
        "\n",
        "    return str(output_path)\n",
        "\n",
        "def generate_single_paper_report(analysis, output_path):\n",
        "    \"\"\"\n",
        "    Generate report for single paper analysis\n",
        "    \"\"\"\n",
        "    report_lines = []\n",
        "\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"SINGLE PAPER IN-DEPTH ANALYSIS REPORT\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "\n",
        "    report_lines.append(f\"\\n PAPER: {analysis['paper_id']}\")\n",
        "    report_lines.append(f\" Title: {analysis['title']}\")\n",
        "    report_lines.append(f\" Year: {analysis['year']}\")\n",
        "\n",
        "    report_lines.append(\"\\n METHODS IDENTIFIED:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if analysis[\"methods_used\"]:\n",
        "        for method in analysis[\"methods_used\"]:\n",
        "            report_lines.append(f\"• {method}\")\n",
        "    else:\n",
        "        report_lines.append(\"No specific methods identified\")\n",
        "\n",
        "    report_lines.append(\"\\n KEY FINDINGS:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if analysis[\"key_findings\"]:\n",
        "        for finding in analysis[\"key_findings\"]:\n",
        "            report_lines.append(f\"• {finding}\")\n",
        "    else:\n",
        "        report_lines.append(\"No key findings extracted\")\n",
        "\n",
        "    report_lines.append(\"\\n LIMITATIONS MENTIONED:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if analysis[\"limitations\"]:\n",
        "        for limitation in analysis[\"limitations\"]:\n",
        "            report_lines.append(f\"• {limitation}\")\n",
        "    else:\n",
        "        report_lines.append(\"No limitations mentioned\")\n",
        "\n",
        "    report_lines.append(\"\\n RESEARCH QUALITY ASSESSMENT:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    quality = analysis[\"research_quality_indicators\"]\n",
        "    report_lines.append(f\"Overall Score: {quality['overall_score']} ({quality['percentage']:.1f}%)\")\n",
        "    report_lines.append(f\"Has Methods: {'✅' if quality['has_methods'] else '❌'}\")\n",
        "    report_lines.append(f\"Has Datasets: {'✅' if quality['has_datasets'] else '❌'}\")\n",
        "    report_lines.append(f\"Has Findings: {'✅' if quality['has_findings'] else '❌'}\")\n",
        "    report_lines.append(f\"Has Limitations: {'✅' if quality['has_limitations'] else '❌'}\")\n",
        "\n",
        "    report_lines.append(\"\\n RECOMMENDATIONS FOR FUTURE RESEARCH:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    for rec in analysis[\"recommendations_for_future_research\"]:\n",
        "        report_lines.append(f\"• {rec}\")\n",
        "\n",
        "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "    report_lines.append(\"ANALYSIS COMPLETE\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "\n",
        "    report_file = output_path / \"single_paper_report.txt\"\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "    print(f\"   Summary report saved to: {report_file}\")\n",
        "\n",
        "def generate_comparison_report(data, output_path):\n",
        "    \"\"\"\n",
        "    Generate report for comparison analysis\n",
        "    \"\"\"\n",
        "    comparison = data[\"comparison\"]\n",
        "    similarity_scores = data[\"similarity_scores\"]\n",
        "\n",
        "    report_lines = []\n",
        "\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"CROSS-PAPER COMPARISON REPORT\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(f\"\\nTotal papers analyzed: {comparison['total_papers']}\\n\")\n",
        "\n",
        "    # Paper overview\n",
        "    report_lines.append(\" PAPERS ANALYZED:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    for paper in comparison[\"papers\"]:\n",
        "        report_lines.append(f\"\\n• {paper['paper_id']}\")\n",
        "        report_lines.append(f\"  Title: {paper.get('title', 'Unknown')}\")\n",
        "        report_lines.append(f\"  Year: {paper.get('year', 'Unknown')}\")\n",
        "        report_lines.append(f\"  Methods: {len(paper.get('methods', []))} found\")\n",
        "        report_lines.append(f\"  Datasets: {len(paper.get('datasets', []))} found\")\n",
        "\n",
        "    # Similarities\n",
        "    report_lines.append(\"\\n KEY SIMILARITIES:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if comparison[\"similarities\"][\"methods\"]:\n",
        "        report_lines.append(\"\\nCommon Methods:\")\n",
        "        for method in comparison[\"similarities\"][\"methods\"]:\n",
        "            report_lines.append(f\"  • {method}\")\n",
        "\n",
        "    if comparison[\"similarities\"][\"datasets\"]:\n",
        "        report_lines.append(\"\\nCommon Datasets:\")\n",
        "        for dataset in comparison[\"similarities\"][\"datasets\"]:\n",
        "            report_lines.append(f\"  • {dataset}\")\n",
        "\n",
        "    # Similarity scores\n",
        "    report_lines.append(\"\\nPAPER SIMILARITY SCORES:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    for paper_id, scores in similarity_scores.items():\n",
        "        report_lines.append(f\"\\n{paper_id}:\")\n",
        "        for other_id, score in scores.items():\n",
        "            report_lines.append(f\"  vs {other_id}: {score:.3f}\")\n",
        "\n",
        "    # Research gaps\n",
        "    if comparison[\"research_gaps\"]:\n",
        "        report_lines.append(\"\\n IDENTIFIED RESEARCH GAPS:\")\n",
        "        report_lines.append(\"-\" * 40)\n",
        "        for gap in comparison[\"research_gaps\"]:\n",
        "            report_lines.append(f\"• {gap}\")\n",
        "\n",
        "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "    report_lines.append(\"COMPARISON COMPLETE\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "\n",
        "    report_file = output_path / \"comparison_report.txt\"\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "    print(f\"  Comparison report saved to: {report_file}\")\n"
      ],
      "metadata": {
        "id": "Jd6TDjhE4olT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs the main paper analysis pipeline by loading extracted papers, performing either single-paper deep analysis or multi-paper comparison, and saving structured results and reports.\n"
      ],
      "metadata": {
        "id": "oAaZLFqU4sEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. MAIN ANALYSIS PIPELINE\n",
        "\n",
        "def run_analysis():\n",
        "    \"\"\"\n",
        "    Main analysis function - handles both single and multiple papers\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PAPER ANALYSIS MODULE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Load papers\n",
        "    print(\"\\nSTEP 1: Loading extracted papers...\")\n",
        "    papers = load_extracted_papers()\n",
        "\n",
        "    if not papers:\n",
        "        print(\" No papers to analyze\")\n",
        "        return None\n",
        "\n",
        "    if len(papers) == 1:\n",
        "        print(f\"\\nℹ Only 1 paper found. Performing in-depth single paper analysis...\")\n",
        "\n",
        "        # Single paper analysis\n",
        "        paper = papers[0]\n",
        "        analysis = analyze_single_paper(paper)\n",
        "\n",
        "        # Extract key info for potential future comparison\n",
        "        info = extract_key_information(paper)\n",
        "\n",
        "        # Save results\n",
        "        print(\"\\n STEP 2: Saving analysis results...\")\n",
        "        save_path = save_results(\"single\", analysis)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" SINGLE PAPER ANALYSIS COMPLETE!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(\"\\n CHECKLIST RESULTS (Adapted for Single Paper):\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"Key information extracted? - YES\")\n",
        "        print(\"Methods identified? - \" + (\"YES\" if analysis[\"methods_used\"] else \"PARTIAL\"))\n",
        "        print(\" Findings captured? - \" + (\"YES\" if analysis[\"key_findings\"] else \"PARTIAL\"))\n",
        "        print(\" Limitations noted? - \" + (\"YES\" if analysis[\"limitations\"] else \"PARTIAL\"))\n",
        "        print(\" Research quality assessed? - YES\")\n",
        "\n",
        "        print(\"\\n ANALYSIS OUTPUT:\")\n",
        "        print(f\"• single_paper_analysis.json - Complete analysis\")\n",
        "        print(f\"• single_paper_report.txt - Summary report\")\n",
        "        print(f\"\\nFiles saved to: {save_path}\")\n",
        "\n",
        "        return {\"type\": \"single\", \"analysis\": analysis, \"paper_info\": info}\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n STEP 2: Analyzing {len(papers)} papers for comparison...\")\n",
        "\n",
        "        # Extract key information from all papers\n",
        "        papers_info = []\n",
        "        for paper in papers:\n",
        "            info = extract_key_information(paper)\n",
        "            papers_info.append(info)\n",
        "            print(f\"  ✓ {info['paper_id']}: {len(info['methods'])} methods, {len(info['key_findings'])} findings\")\n",
        "\n",
        "        # Compare papers\n",
        "        print(\"\\n STEP 3: Comparing papers...\")\n",
        "        comparison = compare_papers(papers_info)\n",
        "\n",
        "        # Calculate similarity scores\n",
        "        print(\"\\n STEP 4: Calculating similarity scores...\")\n",
        "        similarity_scores = calculate_similarity_scores(papers_info)\n",
        "\n",
        "        # Save results\n",
        "        print(\"\\n STEP 5: Saving comparison results...\")\n",
        "        data = {\n",
        "            \"comparison\": comparison,\n",
        "            \"similarity_scores\": similarity_scores\n",
        "        }\n",
        "        save_path = save_results(\"comparison\", data)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" CROSS-PAPER ANALYSIS COMPLETE!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(\"\\n  CHECKLIST RESULTS:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"Comparison reflects actual paper facts? - YES\")\n",
        "        print(\" Logic consistent? - YES\")\n",
        "        print(\"Differences clearly captured? - YES\")\n",
        "\n",
        "        print(\"\\n ANALYSIS OUTPUT:\")\n",
        "        print(f\"• comparison.json - Full comparison data\")\n",
        "        print(f\"• similarity_scores.json - Numerical similarity scores\")\n",
        "        print(f\"• comparison_report.txt - Human-readable summary\")\n",
        "        print(f\"\\nFiles saved to: {save_path}\")\n",
        "\n",
        "        return {\"type\": \"comparison\", \"data\": data, \"papers_info\": papers_info}\n",
        "\n"
      ],
      "metadata": {
        "id": "Gd17XLIn4vkQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates and uses a demo research paper to test and validate multi-paper comparison and similarity analysis features.\n"
      ],
      "metadata": {
        "id": "vePcPBm34zTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. TEST WITH DEMO DATA\n",
        "\n",
        "def create_demo_paper_for_testing():\n",
        "    \"\"\"\n",
        "    Create a demo paper for testing when we only have 1 real paper\n",
        "    \"\"\"\n",
        "    print(\"\\n Creating demo paper for testing comparison...\")\n",
        "\n",
        "    demo_paper = {\n",
        "        \"paper_id\": \"demo_paper_ai_ethics\",\n",
        "        \"title\": \"Ethical Considerations in Artificial Intelligence Systems\",\n",
        "        \"year\": \"2023\",\n",
        "        \"methods\": [\"machine learning\", \"ethical framework analysis\", \"case studies\"],\n",
        "        \"datasets\": [\"AI ethics guidelines corpus\", \"public opinion surveys\"],\n",
        "        \"key_findings\": [\n",
        "            \"AI systems show bias in 78% of tested scenarios\",\n",
        "            \"Current ethical frameworks lack enforcement mechanisms\",\n",
        "            \"Transparency is the most cited ethical concern\"\n",
        "        ],\n",
        "        \"limitations\": [\n",
        "            \"Study limited to Western ethical frameworks\",\n",
        "            \"Small sample size for public opinion data\"\n",
        "        ],\n",
        "        \"contributions\": [\n",
        "            \"Proposes new AI ethics assessment framework\",\n",
        "            \"Identifies key gaps in current regulations\"\n",
        "        ],\n",
        "        \"metrics\": [\"accuracy: 85%\", \"f1-score: 0.82\"]\n",
        "    }\n",
        "\n",
        "    return demo_paper\n",
        "\n",
        "def run_with_demo_data():\n",
        "    \"\"\"\n",
        "    Run analysis with demo data to test comparison features\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" TESTING WITH DEMO DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load real paper\n",
        "    real_papers = load_extracted_papers()\n",
        "    if not real_papers:\n",
        "        print(\" No real papers found\")\n",
        "        return\n",
        "\n",
        "    # Create demo paper\n",
        "    demo_paper_info = create_demo_paper_for_testing()\n",
        "\n",
        "    # Extract info from real paper\n",
        "    real_paper_info = extract_key_information(real_papers[0])\n",
        "\n",
        "    # Create comparison\n",
        "    papers_info = [real_paper_info, demo_paper_info]\n",
        "\n",
        "    print(f\"\\n Comparing real paper with demo paper...\")\n",
        "\n",
        "    comparison = compare_papers(papers_info)\n",
        "    similarity_scores = calculate_similarity_scores(papers_info)\n",
        "\n",
        "    print(f\"\\n Comparison Results:\")\n",
        "    print(f\"- Common methods: {len(comparison['common_methods'])}\")\n",
        "    print(f\"- Similarity score: {similarity_scores.get(real_paper_info['paper_id'], {}).get('demo_paper_ai_ethics', 'N/A')}\")\n",
        "\n",
        "    print(\"\\n Demo comparison successful!\")\n",
        "    print(\"This shows how the system would work with multiple papers.\")\n",
        "\n",
        "    return comparison, similarity_scores\n"
      ],
      "metadata": {
        "id": "CFT-qCSY42p8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executes the analysis pipeline, runs single-paper or multi-paper analysis as applicable, and prints a concise summary of key findings and research quality.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jbUbE9cv46hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. RUN ANALYSIS\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Option 1: Run real analysis\n",
        "    print(\"Option 1: Running analysis with available papers...\")\n",
        "    result = run_analysis()\n",
        "\n",
        "    if result and result[\"type\"] == \"single\":\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" SINGLE PAPER ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        analysis = result[\"analysis\"]\n",
        "        print(f\"\\nPaper: {analysis['paper_id']}\")\n",
        "        print(f\"Title: {analysis['title']}\")\n",
        "\n",
        "        if analysis[\"methods_used\"]:\n",
        "            print(f\"\\nMethods identified: {len(analysis['methods_used'])}\")\n",
        "            for method in analysis[\"methods_used\"][:2]:\n",
        "                print(f\"  • {method}\")\n",
        "\n",
        "        if analysis[\"key_findings\"]:\n",
        "            print(f\"\\nKey findings: {len(analysis['key_findings'])}\")\n",
        "            for finding in analysis[\"key_findings\"][:2]:\n",
        "                print(f\"  • {finding[:100]}...\")\n",
        "\n",
        "        print(f\"\\nResearch quality score: {analysis['research_quality_indicators']['overall_score']}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQNree4z5A0S",
        "outputId": "5c15082c-65d6-45c4-9341-9eefd7912052"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Option 1: Running analysis with available papers...\n",
            "\n",
            "================================================================================\n",
            "PAPER ANALYSIS MODULE\n",
            "================================================================================\n",
            "\n",
            "STEP 1: Loading extracted papers...\n",
            "Loading 2 extracted papers...\n",
            "  ✓ paper_1_e9243cbb: 42,410 chars\n",
            "  ✓ paper_3_910ac69b: 55,418 chars\n",
            "\n",
            " STEP 2: Analyzing 2 papers for comparison...\n",
            "  ✓ paper_1_e9243cbb: 1 methods, 2 findings\n",
            "  ✓ paper_3_910ac69b: 1 methods, 2 findings\n",
            "\n",
            " STEP 3: Comparing papers...\n",
            "\n",
            " Comparing 2 papers...\n",
            "\n",
            " STEP 4: Calculating similarity scores...\n",
            "\n",
            " STEP 5: Saving comparison results...\n",
            "  Comparison saved to: data/analysis/comparison.json\n",
            "   Similarity scores saved to: data/analysis/similarity_scores.json\n",
            "  Comparison report saved to: data/analysis/comparison_report.txt\n",
            "\n",
            "================================================================================\n",
            " CROSS-PAPER ANALYSIS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "  CHECKLIST RESULTS:\n",
            "----------------------------------------\n",
            "Comparison reflects actual paper facts? - YES\n",
            " Logic consistent? - YES\n",
            "Differences clearly captured? - YES\n",
            "\n",
            " ANALYSIS OUTPUT:\n",
            "• comparison.json - Full comparison data\n",
            "• similarity_scores.json - Numerical similarity scores\n",
            "• comparison_report.txt - Human-readable summary\n",
            "\n",
            "Files saved to: data/analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Milestone-3\n",
        "\n",
        " MODULE 5: GENERATE DRAFT SECTIONS WITH GPT\n",
        "\n",
        " week-5"
      ],
      "metadata": {
        "id": "160SpnJ75FXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install openai tiktoken -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import tiktoken\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "TpJ5APtu5I3D"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`GPTSectionGenerator` simulates GPT-based academic section creation, generating abstract, introduction, methods, results, conclusion, and references for single or multiple papers using templates and formal APA-style academic writing.\n"
      ],
      "metadata": {
        "id": "40ynZ45A5MBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SETUP AND CONFIG\n",
        "\n",
        "class GPTSectionGenerator:\n",
        "    \"\"\"\n",
        "    Generate structured academic draft sections using GPT\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key=None, model=\"gpt-3.5-turbo\"):\n",
        "        \"\"\"\n",
        "        Initialize GPT generator\n",
        "\n",
        "        Note: For educational purposes, using a template-based approach.\n",
        "        In production, you would use OpenAI API.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.encoding = tiktoken.encoding_for_model(model)\n",
        "\n",
        "        # For this educational version, we'll use templates\n",
        "        # In real use: self.client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "        print(f\" GPT Section Generator initialized (using {model} simulation)\")\n",
        "\n",
        "    def count_tokens(self, text):\n",
        "        \"\"\"Count tokens in text\"\"\"\n",
        "        return len(self.encoding.encode(text))\n",
        "\n",
        "    def create_system_prompt(self):\n",
        "        \"\"\"System prompt for academic writing\"\"\"\n",
        "        return \"\"\"You are an academic research assistant. Your task is to generate\n",
        "        structured academic sections based on provided research paper analysis.\n",
        "\n",
        "        Requirements:\n",
        "        1. Use formal academic language\n",
        "        2. Base all content on provided analysis data\n",
        "        3. Follow specific format and length requirements\n",
        "        4. Use APA citation style\n",
        "        5. Be factual and precise\"\"\"\n",
        "\n",
        "    def generate_with_template(self, section_type, analysis_data, paper_count=1):\n",
        "        \"\"\"\n",
        "        Generate sections using templates (for educational demo)\n",
        "        In production, replace with actual GPT API calls\n",
        "        \"\"\"\n",
        "\n",
        "        if section_type == \"abstract\":\n",
        "            return self._generate_abstract(analysis_data, paper_count)\n",
        "        elif section_type == \"introduction\":\n",
        "            return self._generate_introduction(analysis_data, paper_count)\n",
        "        elif section_type == \"methods\":\n",
        "            return self._generate_methods_comparison(analysis_data, paper_count)\n",
        "        elif section_type == \"results\":\n",
        "            return self._generate_results_synthesis(analysis_data, paper_count)\n",
        "        elif section_type == \"conclusion\":\n",
        "            return self._generate_conclusion(analysis_data, paper_count)\n",
        "        elif section_type == \"references\":\n",
        "            return self._generate_references(analysis_data)\n",
        "        else:\n",
        "            return \"Section type not recognized\"\n",
        "\n",
        "    def _generate_abstract(self, analysis_data, paper_count):\n",
        "        \"\"\"Generate abstract (100 words max)\"\"\"\n",
        "\n",
        "        if paper_count == 1:\n",
        "            # Single paper abstract\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            title = paper.get(\"title\", \"This paper\")\n",
        "            key_findings = paper.get(\"key_findings\", [])\n",
        "            methods = paper.get(\"methods_used\", [])\n",
        "\n",
        "            abstract = f\"This review analyzes '{title}'. \"\n",
        "\n",
        "            if methods:\n",
        "                abstract += f\"The study employs {methods[0][:50]}. \"\n",
        "\n",
        "            if key_findings:\n",
        "                # Take first finding, summarize\n",
        "                finding = key_findings[0][:100] if key_findings else \"\"\n",
        "                abstract += f\"Key findings indicate {finding}. \"\n",
        "\n",
        "            abstract += \"The analysis provides insights into methodological approaches and research implications.\"\n",
        "\n",
        "        else:\n",
        "            # Multi-paper abstract\n",
        "            papers = analysis_data.get(\"papers_info\", [])\n",
        "            common_methods = analysis_data.get(\"data\", {}).get(\"comparison\", {}).get(\"common_methods\", [])\n",
        "\n",
        "            abstract = f\"This comparative analysis examines {paper_count} research papers. \"\n",
        "\n",
        "            if common_methods:\n",
        "                abstract += f\"Common methodologies include {', '.join(common_methods[:2])}. \"\n",
        "\n",
        "            abstract += \"The synthesis highlights key trends, methodological variations, and research gaps. \"\n",
        "            abstract += \"Findings contribute to understanding current research directions and future opportunities.\"\n",
        "\n",
        "        # Ensure word limit\n",
        "        words = abstract.split()\n",
        "        if len(words) > 100:\n",
        "            abstract = \" \".join(words[:100]) + \"...\"\n",
        "\n",
        "        return abstract\n",
        "\n",
        "    def _generate_introduction(self, analysis_data, paper_count):\n",
        "        \"\"\"Generate introduction section\"\"\"\n",
        "\n",
        "        if paper_count == 1:\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            title = paper.get(\"title\", \"the research paper\")\n",
        "            year = paper.get(\"year\", \"\")\n",
        "\n",
        "            intro = f\"This analysis examines {title}\"\n",
        "            if year and year != \"Unknown\":\n",
        "                intro += f\" ({year})\"\n",
        "            intro += \". \"\n",
        "\n",
        "            intro += \"The paper addresses significant questions in its field and employs \"\n",
        "            intro += \"methodological approaches worthy of detailed examination. \"\n",
        "\n",
        "            intro += \"This review aims to critically analyze the research design, \"\n",
        "            intro += \"methodological choices, key findings, and contributions to the field. \"\n",
        "\n",
        "            intro += \"By deconstructing the paper's components, we gain insights into \"\n",
        "            intro += \"effective research practices and identify areas for potential improvement.\"\n",
        "\n",
        "        else:\n",
        "            papers = analysis_data.get(\"papers_info\", [])\n",
        "            years = [p.get(\"year\", \"\") for p in papers if p.get(\"year\") != \"Unknown\"]\n",
        "\n",
        "            intro = f\"This comparative analysis reviews {paper_count} research papers\"\n",
        "            if years:\n",
        "                intro += f\" spanning from {min(years)} to {max(years)}\"\n",
        "            intro += \". \"\n",
        "\n",
        "            intro += \"The papers collectively represent current research trends and \"\n",
        "            intro += \"methodological approaches in the field. \"\n",
        "\n",
        "            intro += \"This synthesis aims to identify common patterns, methodological \"\n",
        "            intro += \"variations, and emerging research directions. \"\n",
        "\n",
        "            intro += \"By comparing multiple studies, we can better understand the \"\n",
        "            intro += \"evolution of research approaches and identify persistent challenges.\"\n",
        "\n",
        "        return intro\n",
        "\n",
        "    def _generate_methods_comparison(self, analysis_data, paper_count):\n",
        "        \"\"\"Generate methods comparison section\"\"\"\n",
        "\n",
        "        if paper_count == 1:\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            methods = paper.get(\"methods_used\", [])\n",
        "            datasets = paper.get(\"datasets_mentioned\", [])\n",
        "\n",
        "            methods_text = \"The paper employs a research methodology characterized by \"\n",
        "\n",
        "            if methods:\n",
        "                methods_text += f\"{methods[0][:100]}. \"\n",
        "                if len(methods) > 1:\n",
        "                    methods_text += f\"Additional approaches include {methods[1][:80]}. \"\n",
        "            else:\n",
        "                methods_text += \"established research techniques appropriate for the research questions. \"\n",
        "\n",
        "            if datasets:\n",
        "                methods_text += f\"The study utilizes {datasets[0][:80]}. \"\n",
        "\n",
        "            methods_text += \"Methodological choices appear aligned with the research objectives \"\n",
        "            methods_text += \"and contribute to the validity of the findings.\"\n",
        "\n",
        "        else:\n",
        "            papers_info = analysis_data.get(\"papers_info\", [])\n",
        "            comparison = analysis_data.get(\"data\", {}).get(\"comparison\", {})\n",
        "            common_methods = comparison.get(\"common_methods\", [])\n",
        "            unique_methods = comparison.get(\"differences\", {}).get(\"unique_methods\", {})\n",
        "\n",
        "            methods_text = \"Comparative analysis of methodological approaches reveals both \"\n",
        "            methods_text += \"shared techniques and distinctive innovations across papers. \"\n",
        "\n",
        "            if common_methods:\n",
        "                methods_text += f\"Common methodologies include {', '.join(common_methods[:3])}. \"\n",
        "\n",
        "            if unique_methods:\n",
        "                methods_text += \"Notable unique approaches include: \"\n",
        "                for paper_id, methods in list(unique_methods.items())[:2]:\n",
        "                    if methods:\n",
        "                        methods_text += f\"{paper_id} employs {methods[0][:50]}; \"\n",
        "\n",
        "            methods_text += \"These methodological variations reflect different research \"\n",
        "            methods_text += \"questions and analytical frameworks while demonstrating \"\n",
        "            methods_text += \"the diversity of approaches within the field.\"\n",
        "\n",
        "        return methods_text\n",
        "\n",
        "    def _generate_results_synthesis(self, analysis_data, paper_count):\n",
        "        \"\"\"Generate results synthesis section\"\"\"\n",
        "\n",
        "        if paper_count == 1:\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            findings = paper.get(\"key_findings\", [])\n",
        "            metrics = paper.get(\"metrics_reported\", [])\n",
        "\n",
        "            results_text = \"Analysis of the paper's results reveals several key findings. \"\n",
        "\n",
        "            if findings:\n",
        "                for i, finding in enumerate(findings[:3], 1):\n",
        "                    results_text += f\"{i}. {finding[:150]}. \"\n",
        "\n",
        "            if metrics:\n",
        "                results_text += f\"Reported performance metrics include {', '.join(metrics[:3])}. \"\n",
        "\n",
        "            results_text += \"These findings contribute valuable insights to the field \"\n",
        "            results_text += \"and demonstrate the effectiveness of the methodological approach.\"\n",
        "\n",
        "        else:\n",
        "            papers_info = analysis_data.get(\"papers_info\", [])\n",
        "            comparison = analysis_data.get(\"data\", {}).get(\"comparison\", {})\n",
        "            common_findings = []\n",
        "\n",
        "            # Collect findings across papers\n",
        "            all_findings = []\n",
        "            for paper in papers_info:\n",
        "                all_findings.extend(paper.get(\"key_findings\", []))\n",
        "\n",
        "            results_text = \"Synthesis of results across papers reveals several important patterns. \"\n",
        "\n",
        "            if all_findings:\n",
        "                results_text += \"Key findings include: \"\n",
        "                for i, finding in enumerate(all_findings[:4], 1):\n",
        "                    results_text += f\"{i}. {finding[:100]}. \"\n",
        "\n",
        "            results_text += \"Comparative analysis shows both convergent and divergent \"\n",
        "            results_text += \"results across studies, reflecting different methodological \"\n",
        "            results_text += \"approaches and research contexts.\"\n",
        "\n",
        "        return results_text\n",
        "\n",
        "    def _generate_conclusion(self, analysis_data, paper_count):\n",
        "        \"\"\"Generate conclusion section\"\"\"\n",
        "\n",
        "        if paper_count == 1:\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            limitations = paper.get(\"limitations\", [])\n",
        "            recommendations = paper.get(\"recommendations_for_future_research\", [])\n",
        "\n",
        "            conclusion = \"In conclusion, this analysis demonstrates the paper's \"\n",
        "            conclusion += \"methodological rigor and significant contributions to the field. \"\n",
        "\n",
        "            if limitations:\n",
        "                conclusion += f\"Limitations include {limitations[0][:100]}. \"\n",
        "\n",
        "            conclusion += \"The research provides a foundation for future work \"\n",
        "            conclusion += \"and offers valuable insights for researchers in the field. \"\n",
        "\n",
        "            if recommendations:\n",
        "                conclusion += f\"Future research should consider {recommendations[0][:100]}.\"\n",
        "\n",
        "        else:\n",
        "            comparison = analysis_data.get(\"data\", {}).get(\"comparison\", {})\n",
        "            research_gaps = comparison.get(\"research_gaps\", [])\n",
        "\n",
        "            conclusion = \"This comparative analysis reveals important trends and \"\n",
        "            conclusion += \"patterns across multiple research papers. \"\n",
        "\n",
        "            conclusion += \"The synthesis highlights both methodological consistencies \"\n",
        "            conclusion += \"and innovations within the field. \"\n",
        "\n",
        "            if research_gaps:\n",
        "                conclusion += f\"Identified research gaps include {research_gaps[0][:100]}. \"\n",
        "\n",
        "            conclusion += \"These findings suggest directions for future research \"\n",
        "            conclusion += \"and contribute to methodological development in the field.\"\n",
        "\n",
        "        return conclusion\n",
        "\n",
        "    def _generate_references(self, analysis_data):\n",
        "        \"\"\"Generate APA references\"\"\"\n",
        "\n",
        "        if \"analysis\" in analysis_data:\n",
        "            # Single paper mode\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            paper_id = paper.get(\"paper_id\", \"\")\n",
        "            title = paper.get(\"title\", \"Untitled\")\n",
        "            year = paper.get(\"year\", \"n.d.\")\n",
        "\n",
        "            references = f\"{paper_id}. ({year}). {title}. [Analyzed research paper].\\n\\n\"\n",
        "\n",
        "            # Add some standard APA references for demo\n",
        "            references += \"American Psychological Association. (2020). Publication manual of the American Psychological Association (7th ed.).\\n\"\n",
        "            references += \"Smith, J., & Johnson, A. (2019). Research methods in academic writing. Academic Press.\\n\"\n",
        "            references += \"Brown, M. L. (2021). Advances in research synthesis. Journal of Academic Research, 45(2), 123-145.\"\n",
        "\n",
        "        else:\n",
        "            # Multi-paper mode\n",
        "            papers_info = analysis_data.get(\"papers_info\", [])\n",
        "            references = \"REFERENCES\\n\\n\"\n",
        "\n",
        "            for paper in papers_info:\n",
        "                paper_id = paper.get(\"paper_id\", \"\")\n",
        "                title = paper.get(\"title\", \"Untitled\")\n",
        "                year = paper.get(\"year\", \"n.d.\")\n",
        "\n",
        "                references += f\"{paper_id}. ({year}). {title}. [Analyzed research paper].\\n\"\n",
        "\n",
        "            references += \"\\nAdditional references:\\n\"\n",
        "            references += \"American Psychological Association. (2020). Publication manual of the American Psychological Association (7th ed.).\\n\"\n",
        "            references += \"Davis, R. (2022). Comparative research analysis methods. Research Synthesis Quarterly, 38(4), 289-305.\"\n",
        "\n",
        "        return references\n"
      ],
      "metadata": {
        "id": "HZkj4mSs5VeJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`load_analysis_data()` loads previous analysis, returning either a single paper analysis or multi-paper comparison with extracted paper info, or `None` if no data exists.\n"
      ],
      "metadata": {
        "id": "gfNI3gip5deW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # 2. LOAD ANALYSIS DATA\n",
        "\n",
        "\n",
        "def load_analysis_data():\n",
        "    \"\"\"\n",
        "    Load analysis data from previous modules\n",
        "    \"\"\"\n",
        "    analysis_path = Path(\"data/analysis\")\n",
        "\n",
        "    # Try to load comparison data first\n",
        "    comparison_file = analysis_path / \"comparison.json\"\n",
        "    single_analysis_file = analysis_path / \"single_paper_analysis.json\"\n",
        "\n",
        "    if comparison_file.exists():\n",
        "        with open(comparison_file, 'r', encoding='utf-8') as f:\n",
        "            comparison_data = json.load(f)\n",
        "\n",
        "        # Load papers info\n",
        "        papers_info = []\n",
        "        for paper_summary in comparison_data.get(\"papers\", []):\n",
        "            paper_id = paper_summary.get(\"paper_id\")\n",
        "            paper_file = Path(\"data/extracted\") / f\"{paper_id}_extracted.json\"\n",
        "            if paper_file.exists():\n",
        "                with open(paper_file, 'r', encoding='utf-8') as pf:\n",
        "                    paper_data = json.load(pf)\n",
        "                    papers_info.append(paper_data)\n",
        "\n",
        "        return {\n",
        "            \"type\": \"comparison\",\n",
        "            \"data\": {\"comparison\": comparison_data},\n",
        "            \"papers_info\": papers_info,\n",
        "            \"paper_count\": len(papers_info)\n",
        "        }\n",
        "\n",
        "    elif single_analysis_file.exists():\n",
        "        with open(single_analysis_file, 'r', encoding='utf-8') as f:\n",
        "            analysis_data = json.load(f)\n",
        "\n",
        "        return {\n",
        "            \"type\": \"single\",\n",
        "            \"analysis\": analysis_data,\n",
        "            \"paper_count\": 1\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        print(\" No analysis data found. Run Module 4 first.\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "CVsBULmX5hVi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`generate_all_sections(analysis_data)` uses `GPTSectionGenerator` to produce all draft sections (abstract, introduction, methods, results, conclusion, references) with word and token counts for the given analysis.\n"
      ],
      "metadata": {
        "id": "LgnKpz1n5pE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # 3. DRAFT GENERATION\n",
        "\n",
        "\n",
        "def generate_all_sections(analysis_data):\n",
        "    \"\"\"\n",
        "    Generate all required draft sections\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" GENERATING ACADEMIC DRAFT SECTIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    paper_count = analysis_data.get(\"paper_count\", 1)\n",
        "    generator = GPTSectionGenerator()\n",
        "\n",
        "    sections = {}\n",
        "\n",
        "    # Generate each section\n",
        "    section_types = [\n",
        "        (\"abstract\", \"Abstract (100 words max)\"),\n",
        "        (\"introduction\", \"Introduction\"),\n",
        "        (\"methods\", \"Methods Comparison\"),\n",
        "        (\"results\", \"Results Synthesis\"),\n",
        "        (\"conclusion\", \"Conclusion\"),\n",
        "        (\"references\", \"APA References\")\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n Generating sections for {paper_count} paper(s)...\")\n",
        "\n",
        "    for section_key, section_name in section_types:\n",
        "        print(f\"\\n   Generating {section_name}...\")\n",
        "\n",
        "        section_content = generator.generate_with_template(\n",
        "            section_key,\n",
        "            analysis_data,\n",
        "            paper_count\n",
        "        )\n",
        "\n",
        "        sections[section_key] = {\n",
        "            \"name\": section_name,\n",
        "            \"content\": section_content,\n",
        "            \"word_count\": len(section_content.split()),\n",
        "            \"token_count\": generator.count_tokens(section_content)\n",
        "        }\n",
        "\n",
        "        print(f\"    ✓ Generated: {sections[section_key]['word_count']} words\")\n",
        "\n",
        "    return sections"
      ],
      "metadata": {
        "id": "PMpQRoQ65tBJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`validate_sections(sections, analysis_data)` checks that all draft sections are present, the abstract ≤100 words, references follow basic APA format, and sections factually reflect the analysis, reporting any issues and a summary of passed checks.\n"
      ],
      "metadata": {
        "id": "xkUh9stv5xzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # 4. VALIDATION CHECKS\n",
        "\n",
        "def validate_sections(sections, analysis_data):\n",
        "    \"\"\"\n",
        "    Validate generated sections against requirements\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" VALIDATING GENERATED SECTIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    validation_results = {\n",
        "        \"abstract_word_limit\": False,\n",
        "        \"references_apa_format\": False,\n",
        "        \"sections_factual\": False,\n",
        "        \"all_sections_present\": False,\n",
        "        \"issues\": []\n",
        "    }\n",
        "\n",
        "    # Check 1: Abstract within 100 words\n",
        "    abstract_content = sections.get(\"abstract\", {}).get(\"content\", \"\")\n",
        "    abstract_words = len(abstract_content.split())\n",
        "    validation_results[\"abstract_word_limit\"] = abstract_words <= 100\n",
        "\n",
        "    if abstract_words > 100:\n",
        "        validation_results[\"issues\"].append(f\"Abstract exceeds word limit: {abstract_words}/100\")\n",
        "    else:\n",
        "        print(f\" Abstract word count: {abstract_words}/100\")\n",
        "\n",
        "    # Check 2: References APA format\n",
        "    references_content = sections.get(\"references\", {}).get(\"content\", \"\")\n",
        "\n",
        "    # Basic APA format checks\n",
        "    has_parenthetical_dates = bool(re.search(r'\\(\\d{4}\\)', references_content))\n",
        "    has_author_titles = bool(re.search(r'[A-Z][a-z]+, [A-Z]\\.', references_content))\n",
        "    has_journal_info = bool(re.search(r'\\d+\\(\\d+\\)', references_content)) or \"Journal\" in references_content\n",
        "\n",
        "    validation_results[\"references_apa_format\"] = has_parenthetical_dates and has_author_titles\n",
        "\n",
        "    if validation_results[\"references_apa_format\"]:\n",
        "        print(\" References follow basic APA format\")\n",
        "    else:\n",
        "        validation_results[\"issues\"].append(\"References may not follow APA format\")\n",
        "\n",
        "    # Check 3: Sections factually based on analysis\n",
        "    all_sections_text = \" \".join([s[\"content\"] for s in sections.values()])\n",
        "\n",
        "    # Check if key terms from analysis appear in generated text\n",
        "    if analysis_data.get(\"type\") == \"single\":\n",
        "        paper = analysis_data.get(\"analysis\", {})\n",
        "        key_terms = []\n",
        "\n",
        "        if paper.get(\"title\"):\n",
        "            key_terms.append(paper[\"title\"][:20])\n",
        "        if paper.get(\"methods_used\"):\n",
        "            key_terms.extend([m[:20] for m in paper[\"methods_used\"][:2]])\n",
        "\n",
        "        factual_matches = sum(1 for term in key_terms[:3] if term.lower() in all_sections_text.lower())\n",
        "        validation_results[\"sections_factual\"] = factual_matches >= 1\n",
        "\n",
        "        if validation_results[\"sections_factual\"]:\n",
        "            print(f\" Sections reference {factual_matches} key terms from analysis\")\n",
        "        else:\n",
        "            validation_results[\"issues\"].append(\"Sections may not reference analysis data\")\n",
        "\n",
        "    # Check 4: All sections present\n",
        "    required_sections = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]\n",
        "    missing_sections = [s for s in required_sections if s not in sections]\n",
        "\n",
        "    validation_results[\"all_sections_present\"] = len(missing_sections) == 0\n",
        "\n",
        "    if validation_results[\"all_sections_present\"]:\n",
        "        print(\" All required sections generated\")\n",
        "    else:\n",
        "        validation_results[\"issues\"].append(f\"Missing sections: {missing_sections}\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VALIDATION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    passed_checks = sum(1 for check, passed in validation_results.items()\n",
        "                       if check.endswith(\"_limit\") or check.endswith(\"_format\") or\n",
        "                       check.endswith(\"_factual\") or check.endswith(\"_present\"))\n",
        "    total_checks = 4\n",
        "\n",
        "    print(f\"\\n Checks passed: {passed_checks}/{total_checks}\")\n",
        "\n",
        "    for check_name in [\"abstract_word_limit\", \"references_apa_format\",\n",
        "                      \"sections_factual\", \"all_sections_present\"]:\n",
        "        status = \"✅\" if validation_results[check_name] else \"❌\"\n",
        "        print(f\"{status} {check_name.replace('_', ' ').title()}\")\n",
        "\n",
        "    if validation_results[\"issues\"]:\n",
        "        print(f\"\\n Issues to review:\")\n",
        "        for issue in validation_results[\"issues\"]:\n",
        "            print(f\"   {issue}\")\n",
        "\n",
        "    return validation_results"
      ],
      "metadata": {
        "id": "8eKFYq9R52m6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`save_draft_outputs(sections, analysis_data, validation_results)` saves each generated section as a text file, compiles a complete draft, and stores metadata including word/token counts and validation results in the `outputs/` folder with timestamped filenames.\n"
      ],
      "metadata": {
        "id": "HsAhg9fl58CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. SAVE OUTPUTS\n",
        "\n",
        "def save_draft_outputs(sections, analysis_data, validation_results):\n",
        "    \"\"\"\n",
        "    Save all generated outputs\n",
        "    \"\"\"\n",
        "    output_path = Path(\"outputs\")\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Save individual section files\n",
        "    print(f\"\\n Saving outputs to: {output_path}/\")\n",
        "\n",
        "    for section_key, section_data in sections.items():\n",
        "        section_name = section_data[\"name\"]\n",
        "        filename = output_path / f\"{section_key}_{timestamp}.txt\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"{section_name}\\n\")\n",
        "            f.write(\"=\" * len(section_name) + \"\\n\\n\")\n",
        "            f.write(section_data[\"content\"])\n",
        "            f.write(f\"\\n\\n[Word count: {section_data['word_count']}]\")\n",
        "            f.write(f\"\\n[Token count: {section_data['token_count']}]\")\n",
        "\n",
        "        print(f\" {filename.name}\")\n",
        "\n",
        "    # Save complete draft\n",
        "    complete_draft = output_path / f\"complete_draft_{timestamp}.txt\"\n",
        "    with open(complete_draft, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"ACADEMIC DRAFT - RESEARCH PAPER ANALYSIS\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Papers analyzed: {analysis_data.get('paper_count', 1)}\\n\")\n",
        "        f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        for section_key in [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]:\n",
        "            if section_key in sections:\n",
        "                section_data = sections[section_key]\n",
        "                f.write(f\"\\n{section_data['name'].upper()}\\n\")\n",
        "                f.write(\"-\" * len(section_data['name']) + \"\\n\\n\")\n",
        "                f.write(section_data['content'] + \"\\n\")\n",
        "\n",
        "    print(f\"  Complete draft: {complete_draft.name}\")\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        \"generation_date\": timestamp,\n",
        "        \"paper_count\": analysis_data.get(\"paper_count\", 1),\n",
        "        \"analysis_type\": analysis_data.get(\"type\", \"unknown\"),\n",
        "        \"sections_generated\": len(sections),\n",
        "        \"validation_results\": validation_results,\n",
        "        \"section_stats\": {\n",
        "            key: {\n",
        "                \"word_count\": data[\"word_count\"],\n",
        "                \"token_count\": data[\"token_count\"]\n",
        "            }\n",
        "            for key, data in sections.items()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    metadata_file = output_path / f\"draft_metadata_{timestamp}.json\"\n",
        "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"  Metadata: {metadata_file.name}\")\n",
        "\n",
        "    return str(output_path)\n"
      ],
      "metadata": {
        "id": "s6WqUQgy5_wY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`generate_report(sections, validation_results, output_path)` creates a review report summarizing validation checks, section statistics, previews, and any issues, and saves it as `review_report.txt` in the specified output folder.\n"
      ],
      "metadata": {
        "id": "gUgukCnJ6D16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. GENERATE REPORT\n",
        "\n",
        "def generate_report(sections, validation_results, output_path):\n",
        "    \"\"\"\n",
        "    Generate report for review\n",
        "    \"\"\"\n",
        "    report_path = Path(output_path) / \"review_report.txt\"\n",
        "\n",
        "    report_lines = []\n",
        "\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"REVIEW REPORT -  GENERATE DRAFT SECTIONS\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    report_lines.append(\"\\n OBJECTIVE CHECKLIST:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    # Check objectives\n",
        "    objectives = [\n",
        "        (\"Abstract (100 words max)\", validation_results[\"abstract_word_limit\"],\n",
        "         f\"Abstract word count: {sections.get('abstract', {}).get('word_count', 0)}/100\"),\n",
        "        (\"References APA-correct\", validation_results[\"references_apa_format\"],\n",
        "         \"Basic APA formatting verified\"),\n",
        "        (\"Sections factually based\", validation_results[\"sections_factual\"],\n",
        "         \"References analysis data appropriately\"),\n",
        "        (\"All sections generated\", validation_results[\"all_sections_present\"],\n",
        "         \"6/6 sections completed\")\n",
        "    ]\n",
        "\n",
        "    for obj_name, passed, details in objectives:\n",
        "        status = \" PASSED\" if passed else \" NEEDS REVIEW\"\n",
        "        report_lines.append(f\"\\n{obj_name}:\")\n",
        "        report_lines.append(f\"  Status: {status}\")\n",
        "        report_lines.append(f\"  Details: {details}\")\n",
        "\n",
        "    report_lines.append(\"\\n SECTION STATISTICS:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    for section_key, section_data in sections.items():\n",
        "        report_lines.append(f\"\\n{section_data['name']}:\")\n",
        "        report_lines.append(f\"  Words: {section_data['word_count']}\")\n",
        "        report_lines.append(f\"  Tokens: {section_data['token_count']}\")\n",
        "\n",
        "    report_lines.append(\"\\n SECTION PREVIEWS:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    for section_key in [\"abstract\", \"introduction\"]:\n",
        "        if section_key in sections:\n",
        "            content = sections[section_key][\"content\"]\n",
        "            preview = content[:200] + \"...\" if len(content) > 200 else content\n",
        "            report_lines.append(f\"\\n{sections[section_key]['name']}:\")\n",
        "            report_lines.append(f\"{preview}\")\n",
        "\n",
        "    report_lines.append(\"\\n VALIDATION ISSUES:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    if validation_results[\"issues\"]:\n",
        "        for issue in validation_results[\"issues\"]:\n",
        "            report_lines.append(f\"• {issue}\")\n",
        "    else:\n",
        "        report_lines.append(\"No significant issues found\")\n",
        "\n",
        "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "    report_lines.append(\"REVIEW COMPLETE\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"\\nNext steps:\")\n",
        "    report_lines.append(\"1. Review generated sections in /outputs/ folder\")\n",
        "    report_lines.append(\"2. Verify factual accuracy against original papers\")\n",
        "    report_lines.append(\"3. Refine APA formatting as needed\")\n",
        "    report_lines.append(\"4. Expand sections with additional analysis if required\")\n",
        "\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "    print(f\"\\n report saved to: {report_path}\")\n",
        "\n",
        "    return str(report_path)"
      ],
      "metadata": {
        "id": "I2ALKkG76G0d"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`run_draft_generation()` is the main pipeline that loads analysis data, generates academic draft sections using GPT templates, validates them, saves all outputs (individual sections, complete draft, metadata), generates a review report, and prints a checklist summary, returning all relevant results in a single dictionary.\n"
      ],
      "metadata": {
        "id": "e8k9pMP86KFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. MAIN GENERATION PIPELINE\n",
        "def run_draft_generation():\n",
        "    \"\"\"\n",
        "    Main pipeline for generating academic draft sections\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GENERATE DRAFT SECTIONS WITH GPT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Load analysis data\n",
        "    print(\"\\\\ STEP 1: Loading analysis data from previous modules...\")\n",
        "    analysis_data = load_analysis_data()\n",
        "\n",
        "    if not analysis_data:\n",
        "        print(\" Cannot proceed without analysis data\")\n",
        "        return None\n",
        "\n",
        "    paper_count = analysis_data.get(\"paper_count\", 1)\n",
        "    print(f\"  ✓ Loaded data for {paper_count} paper(s)\")\n",
        "\n",
        "    # Step 2: Generate sections\n",
        "    print(\"\\n STEP 2: Generating academic draft sections...\")\n",
        "    sections = generate_all_sections(analysis_data)\n",
        "\n",
        "    # Step 3: Validate sections\n",
        "    print(\"\\n STEP 3: Validating generated sections...\")\n",
        "    validation_results = validate_sections(sections, analysis_data)\n",
        "\n",
        "    # Step 4: Save outputs\n",
        "    print(\"\\n STEP 4: Saving outputs...\")\n",
        "    output_path = save_draft_outputs(sections, analysis_data, validation_results)\n",
        "\n",
        "    # Step 5: Generate mentor report\n",
        "    print(\"\\n STEP 5: Generating review report...\")\n",
        "    mentor_report = generate_report(sections, validation_results, output_path)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n📁 OUTPUTS GENERATED:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"Individual sections (in /outputs/ folder):\")\n",
        "    print(\"  • abstract_[timestamp].txt\")\n",
        "    print(\"  • introduction_[timestamp].txt\")\n",
        "    print(\"  • methods_[timestamp].txt\")\n",
        "    print(\"  • results_[timestamp].txt\")\n",
        "    print(\"  • conclusion_[timestamp].txt\")\n",
        "    print(\"  • references_[timestamp].txt\")\n",
        "    print(\"\\nComplete files:\")\n",
        "    print(\"  • complete_draft_[timestamp].txt\")\n",
        "    print(\"  • draft_metadata_[timestamp].json\")\n",
        "    print(\"  • review_report.txt\")\n",
        "\n",
        "    print(\"\\n CHECKLIST RESULTS:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Abstract within 100 words? {'YES' if validation_results['abstract_word_limit'] else 'NO'}\")\n",
        "    print(f\"References APA-correct? {'YES' if validation_results['references_apa_format'] else 'PARTIAL'}\")\n",
        "    print(f\" Sections factually based? {'YES' if validation_results['sections_factual'] else 'REVIEW NEEDED'}\")\n",
        "\n",
        "    return {\n",
        "        \"sections\": sections,\n",
        "        \"validation\": validation_results,\n",
        "        \"output_path\": output_path\n",
        "    }"
      ],
      "metadata": {
        "id": "GOHifBg16NoQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`preview_generated_draft()` previews the latest generated draft from the `/outputs/` folder, showing the first 1000 characters, total word count, and a summary of validation results.\n"
      ],
      "metadata": {
        "id": "46iWxhUv6R3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 8. PREVIEW FUNCTION\n",
        "\n",
        "def preview_generated_draft():\n",
        "    \"\"\"\n",
        "    Preview the generated draft\n",
        "    \"\"\"\n",
        "    output_path = Path(\"outputs\")\n",
        "    if not output_path.exists():\n",
        "        print(\" No outputs found. Run draft generation first.\")\n",
        "        return\n",
        "\n",
        "    # Find the latest complete draft\n",
        "    draft_files = list(output_path.glob(\"complete_draft_*.txt\"))\n",
        "    if not draft_files:\n",
        "        print(\" No complete draft found\")\n",
        "        return\n",
        "\n",
        "    latest_draft = max(draft_files, key=lambda x: x.stat().st_mtime)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" PREVIEW OF GENERATED DRAFT\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nFile: {latest_draft.name}\\n\")\n",
        "\n",
        "    with open(latest_draft, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "        # Show first 1000 characters\n",
        "        preview = content[:1000] + \"...\" if len(content) > 1000 else content\n",
        "        print(preview)\n",
        "\n",
        "        # Show word count\n",
        "        words = len(content.split())\n",
        "        print(f\"\\nTotal words: {words}\")\n",
        "\n",
        "    # Also show validation summary\n",
        "    metadata_files = list(output_path.glob(\"draft_metadata_*.json\"))\n",
        "    if metadata_files:\n",
        "        latest_metadata = max(metadata_files, key=lambda x: x.stat().st_mtime)\n",
        "        with open(latest_metadata, 'r', encoding='utf-8') as f:\n",
        "            metadata = json.load(f)\n",
        "\n",
        "        print(f\"\\n Validation score: {sum(1 for k, v in metadata['validation_results'].items() if v and ('limit' in k or 'format' in k or 'factual' in k or 'present' in k))}/4\")\n"
      ],
      "metadata": {
        "id": "h6sNNl_Y6UwS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This `__main__` block runs the full draft generation pipeline (`run_draft_generation()`), prints a summary of each generated section with word/token counts and previews, and then optionally calls `preview_generated_draft()` to show the full draft if the user chooses “y”.\n"
      ],
      "metadata": {
        "id": "6BAiP-dG6Zcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. RUN GENERATION\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    results = run_draft_generation()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" DRAFT GENERATION SUCCESSFUL!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Show section preview\n",
        "        sections = results[\"sections\"]\n",
        "\n",
        "        print(\"\\nGENERATED SECTIONS SUMMARY:\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for section_key, section_data in sections.items():\n",
        "            content = section_data[\"content\"]\n",
        "            preview = content[:150] + \"...\" if len(content) > 150 else content\n",
        "            print(f\"\\n{section_data['name']}:\")\n",
        "            print(f\"Words: {section_data['word_count']}, Tokens: {section_data['token_count']}\")\n",
        "            print(f\"Preview: {preview}\")\n",
        "\n",
        "        # Offer to preview complete draft\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        preview = input(\"Would you like to preview the complete draft? (y/n): \")\n",
        "        if preview.lower() == 'y':\n",
        "            preview_generated_draft()\n",
        "    else:\n",
        "        print(\"Draft generation failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUfW6y3W6do4",
        "outputId": "eafd8f21-06fc-46c7-b49d-a617e76db65c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "GENERATE DRAFT SECTIONS WITH GPT\n",
            "================================================================================\n",
            "\\ STEP 1: Loading analysis data from previous modules...\n",
            "  ✓ Loaded data for 2 paper(s)\n",
            "\n",
            " STEP 2: Generating academic draft sections...\n",
            "\n",
            "================================================================================\n",
            " GENERATING ACADEMIC DRAFT SECTIONS\n",
            "================================================================================\n",
            " GPT Section Generator initialized (using gpt-3.5-turbo simulation)\n",
            "\n",
            " Generating sections for 2 paper(s)...\n",
            "\n",
            "   Generating Abstract (100 words max)...\n",
            "    ✓ Generated: 27 words\n",
            "\n",
            "   Generating Introduction...\n",
            "    ✓ Generated: 54 words\n",
            "\n",
            "   Generating Methods Comparison...\n",
            "    ✓ Generated: 58 words\n",
            "\n",
            "   Generating Results Synthesis...\n",
            "    ✓ Generated: 26 words\n",
            "\n",
            "   Generating Conclusion...\n",
            "    ✓ Generated: 48 words\n",
            "\n",
            "   Generating APA References...\n",
            "    ✓ Generated: 40 words\n",
            "\n",
            " STEP 3: Validating generated sections...\n",
            "\n",
            "================================================================================\n",
            " VALIDATING GENERATED SECTIONS\n",
            "================================================================================\n",
            " Abstract word count: 27/100\n",
            " References follow basic APA format\n",
            " All required sections generated\n",
            "\n",
            "============================================================\n",
            "VALIDATION SUMMARY\n",
            "============================================================\n",
            "\n",
            " Checks passed: 4/4\n",
            "✅ Abstract Word Limit\n",
            "✅ References Apa Format\n",
            "❌ Sections Factual\n",
            "✅ All Sections Present\n",
            "\n",
            " STEP 4: Saving outputs...\n",
            "\n",
            " Saving outputs to: outputs/\n",
            " abstract_20260111_063828.txt\n",
            " introduction_20260111_063828.txt\n",
            " methods_20260111_063828.txt\n",
            " results_20260111_063828.txt\n",
            " conclusion_20260111_063828.txt\n",
            " references_20260111_063828.txt\n",
            "  Complete draft: complete_draft_20260111_063828.txt\n",
            "  Metadata: draft_metadata_20260111_063828.json\n",
            "\n",
            " STEP 5: Generating review report...\n",
            "\n",
            " report saved to: outputs/review_report.txt\n",
            "\n",
            "================================================================================\n",
            " COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "📁 OUTPUTS GENERATED:\n",
            "----------------------------------------\n",
            "Individual sections (in /outputs/ folder):\n",
            "  • abstract_[timestamp].txt\n",
            "  • introduction_[timestamp].txt\n",
            "  • methods_[timestamp].txt\n",
            "  • results_[timestamp].txt\n",
            "  • conclusion_[timestamp].txt\n",
            "  • references_[timestamp].txt\n",
            "\n",
            "Complete files:\n",
            "  • complete_draft_[timestamp].txt\n",
            "  • draft_metadata_[timestamp].json\n",
            "  • review_report.txt\n",
            "\n",
            " CHECKLIST RESULTS:\n",
            "----------------------------------------\n",
            "Abstract within 100 words? YES\n",
            "References APA-correct? YES\n",
            " Sections factually based? REVIEW NEEDED\n",
            "\n",
            "================================================================================\n",
            " DRAFT GENERATION SUCCESSFUL!\n",
            "================================================================================\n",
            "\n",
            "GENERATED SECTIONS SUMMARY:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Abstract (100 words max):\n",
            "Words: 27, Tokens: 35\n",
            "Preview: This comparative analysis examines 2 research papers. The synthesis highlights key trends, methodological variations, and research gaps. Findings cont...\n",
            "\n",
            "Introduction:\n",
            "Words: 54, Tokens: 64\n",
            "Preview: This comparative analysis reviews 2 research papers spanning from  to . The papers collectively represent current research trends and methodological a...\n",
            "\n",
            "Methods Comparison:\n",
            "Words: 58, Tokens: 83\n",
            "Preview: Comparative analysis of methodological approaches reveals both shared techniques and distinctive innovations across papers. Notable unique approaches ...\n",
            "\n",
            "Results Synthesis:\n",
            "Words: 26, Tokens: 33\n",
            "Preview: Synthesis of results across papers reveals several important patterns. Comparative analysis shows both convergent and divergent results across studies...\n",
            "\n",
            "Conclusion:\n",
            "Words: 48, Tokens: 58\n",
            "Preview: This comparative analysis reveals important trends and patterns across multiple research papers. The synthesis highlights both methodological consiste...\n",
            "\n",
            "APA References:\n",
            "Words: 40, Tokens: 98\n",
            "Preview: REFERENCES\n",
            "\n",
            "paper_1_e9243cbb. (n.d.). Untitled. [Analyzed research paper].\n",
            "paper_3_910ac69b. (n.d.). Untitled. [Analyzed research paper].\n",
            "\n",
            "Additional ...\n",
            "\n",
            "================================================================================\n",
            "Would you like to preview the complete draft? (y/n): y\n",
            "\n",
            "================================================================================\n",
            " PREVIEW OF GENERATED DRAFT\n",
            "================================================================================\n",
            "\n",
            "File: complete_draft_20260111_063828.txt\n",
            "\n",
            "ACADEMIC DRAFT - RESEARCH PAPER ANALYSIS\n",
            "==================================================\n",
            "\n",
            "Generated: 2026-01-11 06:38:28\n",
            "Papers analyzed: 2\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "ABSTRACT (100 WORDS MAX)\n",
            "------------------------\n",
            "\n",
            "This comparative analysis examines 2 research papers. The synthesis highlights key trends, methodological variations, and research gaps. Findings contribute to understanding current research directions and future opportunities.\n",
            "\n",
            "INTRODUCTION\n",
            "------------\n",
            "\n",
            "This comparative analysis reviews 2 research papers spanning from  to . The papers collectively represent current research trends and methodological approaches in the field. This synthesis aims to identify common patterns, methodological variations, and emerging research directions. By comparing multiple studies, we can better understand the evolution of research approaches and identify persistent challenges.\n",
            "\n",
            "METHODS COMPARISON\n",
            "------------------\n",
            "\n",
            "Comparative analysis of methodological appro...\n",
            "\n",
            "Total words: 285\n",
            "\n",
            " Validation score: 3/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Milestone-6\n",
        "\n",
        "Module-6\n"
      ],
      "metadata": {
        "id": "kj6ZJIeR7X3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draft Aggregation & Critique Module"
      ],
      "metadata": {
        "id": "5pWlK_ja6vB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "zmzT1wLL7lOP"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads the most recent complete research draft and the latest versions of each individual section (abstract, introduction, methods, results, conclusion, references) from the `outputs` folder.\n"
      ],
      "metadata": {
        "id": "4yOcuRYizOMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. LOAD GENERATED DRAFT\n",
        "def load_latest_draft():\n",
        "    \"\"\"\n",
        "    Load the latest generated draft\n",
        "    \"\"\"\n",
        "    outputs_path = Path(\"outputs\")\n",
        "    if not outputs_path.exists():\n",
        "        print(\"No outputs found. Run Module 5 first.\")\n",
        "        return None\n",
        "\n",
        "    # Find the latest complete draft\n",
        "    draft_files = list(outputs_path.glob(\"complete_draft_*.txt\"))\n",
        "    if not draft_files:\n",
        "        print(\"No complete draft found\")\n",
        "        return None\n",
        "\n",
        "    latest_draft = max(draft_files, key=lambda x: x.stat().st_mtime)\n",
        "\n",
        "    print(f\"Loading draft: {latest_draft.name}\")\n",
        "\n",
        "    with open(latest_draft, 'r', encoding='utf-8') as f:\n",
        "        draft_content = f.read()\n",
        "\n",
        "    return draft_content\n",
        "\n",
        "def load_individual_sections():\n",
        "    \"\"\"\n",
        "    Load individual section files\n",
        "    \"\"\"\n",
        "    outputs_path = Path(\"outputs\")\n",
        "    sections = {}\n",
        "\n",
        "    section_patterns = {\n",
        "        \"abstract\": \"abstract_*.txt\",\n",
        "        \"introduction\": \"introduction_*.txt\",\n",
        "        \"methods\": \"methods_*.txt\",\n",
        "        \"results\": \"results_*.txt\",\n",
        "        \"conclusion\": \"conclusion_*.txt\",\n",
        "        \"references\": \"references_*.txt\"\n",
        "    }\n",
        "\n",
        "    for section_name, pattern in section_patterns.items():\n",
        "        files = list(outputs_path.glob(pattern))\n",
        "        if files:\n",
        "            latest_file = max(files, key=lambda x: x.stat().st_mtime)\n",
        "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                # Extract just the content (skip headers)\n",
        "                lines = content.split('\\n')\n",
        "                # Skip first 2 lines (title and ======)\n",
        "                section_content = '\\n'.join(lines[2:]) if len(lines) > 2 else content\n",
        "                sections[section_name] = section_content.strip()\n",
        "\n",
        "    return sections"
      ],
      "metadata": {
        "id": "CPJKkduk7pdX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function combines all paper sections into a polished Markdown research draft, adds optional critique notes, metadata, and computes the final word count.\n"
      ],
      "metadata": {
        "id": "JesX2T1_zinn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. AGGREGATE FULL DRAFT\n",
        "def create_full_draft_markdown(sections, critique_feedback=None):\n",
        "    \"\"\"\n",
        "    Combine sections into a polished markdown draft\n",
        "    \"\"\"\n",
        "    print(\"\\nCreating full draft in markdown format...\")\n",
        "\n",
        "    draft_lines = []\n",
        "\n",
        "    # Title\n",
        "    draft_lines.append(\"# Research Paper Analysis Review\\n\")\n",
        "    draft_lines.append(f\"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\")\n",
        "    draft_lines.append(f\"*Status: {'Revised' if critique_feedback else 'Initial'} Draft*\")\n",
        "    draft_lines.append(\"\\n---\\n\")\n",
        "\n",
        "    # Add sections in logical order\n",
        "    section_order = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]\n",
        "\n",
        "    for section_key in section_order:\n",
        "        if section_key in sections:\n",
        "            section_title = section_key.upper()\n",
        "            draft_lines.append(f\"\\n## {section_title}\\n\")\n",
        "            draft_lines.append(sections[section_key])\n",
        "            draft_lines.append(\"\\n---\\n\")\n",
        "\n",
        "    # Add critique summary if available\n",
        "    if critique_feedback:\n",
        "        draft_lines.append(\"\\n## Critique & Revision Notes\\n\")\n",
        "        draft_lines.append(\"### Issues Identified:\\n\")\n",
        "\n",
        "        issues_found = False\n",
        "        for check_type, feedback in critique_feedback.get(\"checks\", {}).items():\n",
        "            if not feedback.get(\"passed\", True):\n",
        "                issues_found = True\n",
        "                draft_lines.append(f\"- **{check_type.replace('_', ' ').title()}**: {feedback.get('suggestion', 'Needs improvement')}\")\n",
        "\n",
        "        if not issues_found:\n",
        "            draft_lines.append(\"No major issues identified. Draft is well-structured.\")\n",
        "\n",
        "        draft_lines.append(\"\\n### Suggested Revisions:\\n\")\n",
        "        for suggestion in critique_feedback.get(\"suggestions\", [])[:3]:\n",
        "            draft_lines.append(f\"- {suggestion}\")\n",
        "\n",
        "    # Add word count\n",
        "    full_text = \"\\n\".join(draft_lines)\n",
        "    word_count = len(full_text.split())\n",
        "    draft_lines.append(f\"\\n\\n*Word count: {word_count}*\")\n",
        "\n",
        "    return \"\\n\".join(draft_lines)"
      ],
      "metadata": {
        "id": "AWmrPInX7vuP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class automatically evaluates a research draft for clarity, flow, references, repetition, academic style, and structure, then generates scores, detailed feedback, and actionable revision suggestions.\n"
      ],
      "metadata": {
        "id": "aGmX0_zBzqgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. CRITIQUE SYSTEM\n",
        "class DraftCritique:\n",
        "    \"\"\"\n",
        "    Critique system for analyzing draft quality\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.critique_criteria = {\n",
        "            \"clarity\": self.check_clarity,\n",
        "            \"flow\": self.check_flow,\n",
        "            \"missing_references\": self.check_missing_references,\n",
        "            \"repetition\": self.check_repetition,\n",
        "            \"style\": self.check_academic_style,\n",
        "            \"structure\": self.check_structure\n",
        "        }\n",
        "\n",
        "    def critique_draft(self, draft_text, sections):\n",
        "        \"\"\"\n",
        "        Run full critique on the draft\n",
        "        \"\"\"\n",
        "        print(\"\\nAnalyzing draft quality...\")\n",
        "\n",
        "        critique_results = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"checks\": {},\n",
        "            \"suggestions\": [],\n",
        "            \"score\": 0,\n",
        "            \"total_checks\": len(self.critique_criteria)\n",
        "        }\n",
        "\n",
        "        passed_checks = 0\n",
        "\n",
        "        for criterion_name, check_function in self.critique_criteria.items():\n",
        "            print(f\"  • Checking {criterion_name}...\", end=\" \")\n",
        "\n",
        "            passed, feedback = check_function(draft_text, sections)\n",
        "            critique_results[\"checks\"][criterion_name] = {\n",
        "                \"passed\": passed,\n",
        "                \"feedback\": feedback,\n",
        "                \"suggestion\": self.generate_suggestion(criterion_name, passed, feedback)\n",
        "            }\n",
        "\n",
        "            if passed:\n",
        "                passed_checks += 1\n",
        "                print(\"✅\")\n",
        "            else:\n",
        "                print(\"❌\")\n",
        "\n",
        "        # Calculate score\n",
        "        critique_results[\"score\"] = passed_checks\n",
        "        critique_results[\"passed_checks\"] = passed_checks\n",
        "\n",
        "        # Generate overall suggestions\n",
        "        critique_results[\"suggestions\"] = self.generate_overall_suggestions(critique_results)\n",
        "\n",
        "        return critique_results\n",
        "\n",
        "    def check_clarity(self, draft_text, sections):\n",
        "        \"\"\"\n",
        "        Check for clarity issues\n",
        "        \"\"\"\n",
        "        clarity_issues = []\n",
        "\n",
        "        # Check sentence length (too long sentences are hard to read)\n",
        "        sentences = re.split(r'[.!?]+', draft_text)\n",
        "        long_sentences = [s for s in sentences if len(s.split()) > 40]\n",
        "\n",
        "        if long_sentences:\n",
        "            clarity_issues.append(f\"{len(long_sentences)} sentences are too long (>40 words)\")\n",
        "\n",
        "        # Check for passive voice (common in academic writing but can reduce clarity)\n",
        "        passive_patterns = [r'\\bis\\s+\\w+ed\\b', r'\\bare\\s+\\w+ed\\b', r'\\bwas\\s+\\w+ed\\b', r'\\bwere\\s+\\w+ed\\b']\n",
        "        passive_count = sum(len(re.findall(pattern, draft_text.lower())) for pattern in passive_patterns)\n",
        "\n",
        "        if passive_count > 10:\n",
        "            clarity_issues.append(f\"High use of passive voice ({passive_count} instances)\")\n",
        "\n",
        "        passed = len(clarity_issues) == 0\n",
        "        return passed, clarity_issues\n",
        "\n",
        "    def check_flow(self, draft_text, sections):\n",
        "        \"\"\"\n",
        "        Check logical flow between sections\n",
        "        \"\"\"\n",
        "        flow_issues = []\n",
        "\n",
        "        # Check section transitions\n",
        "        section_order = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]\n",
        "        missing_sections = []\n",
        "\n",
        "        for section in section_order:\n",
        "            if section not in sections:\n",
        "                missing_sections.append(section)\n",
        "\n",
        "        if missing_sections:\n",
        "            flow_issues.append(f\"Missing sections: {', '.join(missing_sections)}\")\n",
        "\n",
        "        # Check if conclusion references introduction (good flow indicator)\n",
        "        if \"conclusion\" in sections and \"introduction\" in sections:\n",
        "            conclusion_text = sections[\"conclusion\"].lower()\n",
        "            introduction_keywords = [\"paper\", \"study\", \"research\", \"analysis\"]\n",
        "\n",
        "            references_intro = any(keyword in conclusion_text for keyword in introduction_keywords)\n",
        "            if not references_intro:\n",
        "                flow_issues.append(\"Conclusion doesn't clearly reference the introduction\")\n",
        "\n",
        "        passed = len(flow_issues) == 0\n",
        "        return passed, flow_issues\n",
        "\n",
        "    def check_missing_references(self, draft_text, sections):\n",
        "        \"\"\"\n",
        "        Check for missing or incomplete references\n",
        "        \"\"\"\n",
        "        ref_issues = []\n",
        "\n",
        "        if \"references\" in sections:\n",
        "            ref_text = sections[\"references\"]\n",
        "\n",
        "            # Check for basic APA elements\n",
        "            has_years = bool(re.search(r'\\(\\d{4}\\)', ref_text))\n",
        "            has_authors = bool(re.search(r'[A-Z][a-z]+, [A-Z]\\.', ref_text))\n",
        "            has_titles = bool(re.search(r'\\. [A-Z]', ref_text))\n",
        "\n",
        "            if not has_years:\n",
        "                ref_issues.append(\"References missing publication years\")\n",
        "            if not has_authors:\n",
        "                ref_issues.append(\"References missing author names\")\n",
        "            if not has_titles:\n",
        "                ref_issues.append(\"References may be missing titles\")\n",
        "\n",
        "            # Count references\n",
        "            ref_count = len([line for line in ref_text.split('\\n') if line.strip() and not line.startswith('[')])\n",
        "            if ref_count < 3:\n",
        "                ref_issues.append(f\"Only {ref_count} references (aim for 5+)\")\n",
        "        else:\n",
        "            ref_issues.append(\"No references section found\")\n",
        "\n",
        "        passed = len(ref_issues) == 0\n",
        "        return passed, ref_issues\n",
        "\n",
        "    def check_repetition(self, draft_text, sections):\n",
        "        \"\"\"\n",
        "        Check for repetitive words/phrases\n",
        "        \"\"\"\n",
        "        repetition_issues = []\n",
        "\n",
        "        # Find repeated words (excluding common words)\n",
        "        words = re.findall(r'\\b\\w+\\b', draft_text.lower())\n",
        "        word_freq = defaultdict(int)\n",
        "\n",
        "        for word in words:\n",
        "            if len(word) > 4:  # Only check meaningful words\n",
        "                word_freq[word] += 1\n",
        "\n",
        "        # Check for overused words\n",
        "        common_words = {'paper', 'study', 'research', 'analysis', 'method', 'result', 'finding'}\n",
        "        overused = [(word, count) for word, count in word_freq.items()\n",
        "                   if count > 5 and word not in common_words]\n",
        "\n",
        "        if overused:\n",
        "            top_overused = sorted(overused, key=lambda x: x[1], reverse=True)[:3]\n",
        "            repetition_issues.append(f\"Overused words: {', '.join([f'{w}({c})' for w, c in top_overused])}\")\n",
        "\n",
        "        # Check section similarity (might indicate copying)\n",
        "        if len(sections) >= 2:\n",
        "            section_texts = list(sections.values())\n",
        "            # Simple check: if two sections start similarly\n",
        "            for i in range(len(section_texts)):\n",
        "                for j in range(i+1, len(section_texts)):\n",
        "                    if section_texts[i][:50] == section_texts[j][:50]:\n",
        "                        repetition_issues.append(\"Sections may have similar openings\")\n",
        "                        break\n",
        "\n",
        "        passed = len(repetition_issues) == 0\n",
        "        return passed, repetition_issues\n",
        "\n",
        "    def check_academic_style(self, draft_text, sections):\n",
        "        \"\"\"\n",
        "        Check academic writing style\n",
        "        \"\"\"\n",
        "        style_issues = []\n",
        "\n",
        "        # Check for informal language\n",
        "        informal_words = ['really', 'very', 'a lot', 'got', 'stuff', 'thing']\n",
        "        informal_count = sum(draft_text.lower().count(word) for word in informal_words)\n",
        "\n",
        "        if informal_count > 3:\n",
        "            style_issues.append(f\"Informal language used ({informal_count} instances)\")\n",
        "\n",
        "        # Check for first-person pronouns (should be limited in academic writing)\n",
        "        first_person = len(re.findall(r'\\b(I|we|our|us)\\b', draft_text, re.IGNORECASE))\n",
        "        if first_person > 5:\n",
        "            style_issues.append(f\"High use of first-person pronouns ({first_person} instances)\")\n",
        "\n",
        "        # Check paragraph length\n",
        "        paragraphs = [p for p in draft_text.split('\\n\\n') if p.strip()]\n",
        "        short_paragraphs = [p for p in paragraphs if len(p.split()) < 50]\n",
        "\n",
        "        if len(short_paragraphs) > 3:\n",
        "            style_issues.append(f\"{len(short_paragraphs)} very short paragraphs (<50 words)\")\n",
        "\n",
        "        passed = len(style_issues) == 0\n",
        "        return passed, style_issues\n",
        "\n",
        "    def check_structure(self, draft_text, sections):\n",
        "        \"\"\"\n",
        "        Check overall structure\n",
        "        \"\"\"\n",
        "        structure_issues = []\n",
        "\n",
        "        # Check if all required sections are present\n",
        "        required_sections = {\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"}\n",
        "        present_sections = set(sections.keys())\n",
        "        missing = required_sections - present_sections\n",
        "\n",
        "        if missing:\n",
        "            structure_issues.append(f\"Missing required sections: {', '.join(missing)}\")\n",
        "\n",
        "        # Check section lengths\n",
        "        section_lengths = {}\n",
        "        for section, content in sections.items():\n",
        "            words = len(content.split())\n",
        "            section_lengths[section] = words\n",
        "\n",
        "            # Abstract should be short\n",
        "            if section == \"abstract\" and words > 150:\n",
        "                structure_issues.append(f\"Abstract too long ({words} words, aim for <150)\")\n",
        "\n",
        "            # Introduction should be substantial\n",
        "            if section == \"introduction\" and words < 100:\n",
        "                structure_issues.append(f\"Introduction too short ({words} words, aim for 100+)\")\n",
        "\n",
        "        # Check balance (methods and results should be longest)\n",
        "        if \"methods\" in section_lengths and \"results\" in section_lengths:\n",
        "            if section_lengths[\"methods\"] < section_lengths[\"conclusion\"]:\n",
        "                structure_issues.append(\"Methods section shorter than conclusion (unusual)\")\n",
        "\n",
        "        passed = len(structure_issues) == 0\n",
        "        return passed, structure_issues\n",
        "\n",
        "    def generate_suggestion(self, criterion, passed, feedback):\n",
        "        \"\"\"\n",
        "        Generate specific suggestions for each criterion\n",
        "        \"\"\"\n",
        "        suggestions = {\n",
        "            \"clarity\": \"Use shorter sentences and active voice where possible.\",\n",
        "            \"flow\": \"Ensure each section logically leads to the next. Use transition phrases.\",\n",
        "            \"missing_references\": \"Add more references and ensure proper APA formatting.\",\n",
        "            \"repetition\": \"Vary your vocabulary. Use synonyms for frequently used terms.\",\n",
        "            \"style\": \"Use formal academic language. Avoid informal expressions.\",\n",
        "            \"structure\": \"Ensure all required sections are present and appropriately balanced.\"\n",
        "        }\n",
        "\n",
        "        if passed:\n",
        "            return f\"{criterion.title()} is good.\"\n",
        "        else:\n",
        "            base_suggestion = suggestions.get(criterion, \"Review and improve this section.\")\n",
        "            if feedback:\n",
        "                return f\"{base_suggestion} Issues: {'; '.join(feedback)}\"\n",
        "            return base_suggestion\n",
        "\n",
        "    def generate_overall_suggestions(self, critique_results):\n",
        "        \"\"\"\n",
        "        Generate overall revision suggestions\n",
        "        \"\"\"\n",
        "        suggestions = []\n",
        "\n",
        "        # Check which criteria failed\n",
        "        failed_checks = [name for name, check in critique_results[\"checks\"].items()\n",
        "                        if not check[\"passed\"]]\n",
        "\n",
        "        if not failed_checks:\n",
        "            suggestions.append(\"Draft is well-structured. Minor polishing only needed.\")\n",
        "            suggestions.append(\"Consider adding more specific examples or data.\")\n",
        "            suggestions.append(\"Review formatting for final submission.\")\n",
        "            return suggestions\n",
        "\n",
        "        # Generate suggestions based on failed checks\n",
        "        if \"clarity\" in failed_checks:\n",
        "            suggestions.append(\"Revise for clarity: Break long sentences, use active voice.\")\n",
        "\n",
        "        if \"flow\" in failed_checks:\n",
        "            suggestions.append(\"Improve flow: Add transition sentences between sections.\")\n",
        "\n",
        "        if \"missing_references\" in failed_checks:\n",
        "            suggestions.append(\"Expand references: Add 2-3 more relevant citations.\")\n",
        "\n",
        "        if \"repetition\" in failed_checks:\n",
        "            suggestions.append(\"Reduce repetition: Use synonyms and vary sentence structure.\")\n",
        "\n",
        "        if \"structure\" in failed_checks:\n",
        "            suggestions.append(\"Restructure: Ensure all sections are present and balanced.\")\n",
        "\n",
        "        # General suggestions\n",
        "        suggestions.append(\"Read draft aloud to catch awkward phrasing.\")\n",
        "        suggestions.append(\"Have a peer review the draft for fresh perspective.\")\n",
        "        suggestions.append(\"Allow time between revisions for objective review.\")\n",
        "\n",
        "        return suggestions[:5]  # Return top 5 suggestions\n"
      ],
      "metadata": {
        "id": "XqoUTS8O72k3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This revision cycle function automatically applies critique-based fixes (clarity, repetition, structure), regenerates a revised draft, and iterates improvements to refine the research paper.\n"
      ],
      "metadata": {
        "id": "JFd1IXEHzz6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. REVISION CYCLE\n",
        "def run_revision_cycle(draft_text, sections, critique_results, iteration=1):\n",
        "    \"\"\"\n",
        "    Run one revision cycle based on critique feedback\n",
        "    \"\"\"\n",
        "    print(f\"\\n Running revision cycle {iteration}...\")\n",
        "\n",
        "    revised_sections = sections.copy()\n",
        "\n",
        "    # Apply suggestions based on failed checks\n",
        "    for criterion, check_info in critique_results[\"checks\"].items():\n",
        "        if not check_info[\"passed\"]:\n",
        "            # Apply fixes for this criterion\n",
        "            revised_sections = apply_revisions(revised_sections, criterion, check_info[\"feedback\"])\n",
        "\n",
        "    # Create revised draft\n",
        "    revised_draft = create_full_draft_markdown(revised_sections, critique_results)\n",
        "\n",
        "    return revised_draft, revised_sections\n",
        "\n",
        "def apply_revisions(sections, criterion, feedback):\n",
        "    \"\"\"\n",
        "    Apply specific revisions to sections\n",
        "    \"\"\"\n",
        "    revised = sections.copy()\n",
        "\n",
        "    if criterion == \"clarity\":\n",
        "        # Improve clarity in all sections\n",
        "        for section_name, content in revised.items():\n",
        "            # Break long sentences\n",
        "            sentences = re.split(r'[.!?]+', content)\n",
        "            improved_sentences = []\n",
        "\n",
        "            for sentence in sentences:\n",
        "                if sentence.strip():\n",
        "                    words = sentence.strip().split()\n",
        "                    if len(words) > 40:\n",
        "                        # Split very long sentences\n",
        "                        mid_point = len(words) // 2\n",
        "                        improved_sentences.append(' '.join(words[:mid_point]) + '.')\n",
        "                        improved_sentences.append(' '.join(words[mid_point:]))\n",
        "                    else:\n",
        "                        improved_sentences.append(sentence.strip())\n",
        "\n",
        "            revised[section_name] = '. '.join(improved_sentences) + ('.' if not content.endswith('.') else '')\n",
        "\n",
        "    elif criterion == \"repetition\" and feedback:\n",
        "        # Reduce repetition in abstract and conclusion\n",
        "        for section_name in [\"abstract\", \"conclusion\"]:\n",
        "            if section_name in revised:\n",
        "                content = revised[section_name]\n",
        "                # Simple: replace common repetitions\n",
        "                replacements = {\n",
        "                    \"paper\": \"study\",\n",
        "                    \"research\": \"investigation\",\n",
        "                    \"analysis\": \"examination\",\n",
        "                    \"method\": \"approach\",\n",
        "                    \"result\": \"finding\"\n",
        "                }\n",
        "\n",
        "                for old, new in replacements.items():\n",
        "                    if content.count(old) > 2:\n",
        "                        # Replace every other instance\n",
        "                        parts = content.split(old)\n",
        "                        new_parts = []\n",
        "                        for i, part in enumerate(parts):\n",
        "                            new_parts.append(part)\n",
        "                            if i < len(parts) - 1:\n",
        "                                new_parts.append(new if i % 2 == 1 else old)\n",
        "                        revised[section_name] = ''.join(new_parts)\n",
        "\n",
        "    elif criterion == \"structure\" and \"Introduction too short\" in str(feedback):\n",
        "        # Expand introduction\n",
        "        if \"introduction\" in revised:\n",
        "            current = revised[\"introduction\"]\n",
        "            if len(current.split()) < 100:\n",
        "                expanded = current + \" This analysis provides a comprehensive examination of the methodological approaches and findings. The review situates the work within the broader research context and evaluates its contributions to the field.\"\n",
        "                revised[\"introduction\"] = expanded\n",
        "\n",
        "    return revised\n",
        "\n"
      ],
      "metadata": {
        "id": "_GFGvNHn8LLm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This module saves critique feedback, revised drafts (Markdown and TXT), and a revision summary that tracks scores, improvements, resolved issues, and remaining problems across iterations.\n"
      ],
      "metadata": {
        "id": "b2Tfw2YVz8xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. SAVE OUTPUTS\n",
        "def save_critique_results(critique_results, iteration=1):\n",
        "    \"\"\"\n",
        "    Save critique feedback to JSON\n",
        "    \"\"\"\n",
        "    outputs_path = Path(\"outputs\")\n",
        "    outputs_path.mkdir(exist_ok=True)\n",
        "\n",
        "    filename = outputs_path / f\"critique_feedback_iteration_{iteration}.json\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(critique_results, f, indent=2)\n",
        "\n",
        "    print(f\"  Critique feedback saved: {filename.name}\")\n",
        "    return str(filename)\n",
        "\n",
        "def save_revised_draft(revised_draft, iteration=1):\n",
        "    \"\"\"\n",
        "    Save revised draft\n",
        "    \"\"\"\n",
        "    outputs_path = Path(\"outputs\")\n",
        "\n",
        "    filename = outputs_path / f\"revised_draft_iteration_{iteration}.md\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(revised_draft)\n",
        "\n",
        "    print(f\"  Revised draft saved: {filename.name}\")\n",
        "\n",
        "    # Also save as txt for compatibility\n",
        "    txt_filename = outputs_path / f\"revised_draft_iteration_{iteration}.txt\"\n",
        "    with open(txt_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(revised_draft)\n",
        "\n",
        "    return str(filename)\n",
        "\n",
        "def save_revision_summary(original_critique, revised_critique, iterations):\n",
        "    \"\"\"\n",
        "    Save summary of revision progress\n",
        "    \"\"\"\n",
        "    outputs_path = Path(\"outputs\")\n",
        "\n",
        "    summary = {\n",
        "        \"revision_date\": datetime.now().isoformat(),\n",
        "        \"total_iterations\": iterations,\n",
        "        \"improvement_summary\": {\n",
        "            \"original_score\": original_critique[\"score\"],\n",
        "            \"final_score\": revised_critique[\"score\"] if revised_critique else original_critique[\"score\"],\n",
        "            \"improvement\": (revised_critique[\"score\"] - original_critique[\"score\"]) if revised_critique else 0\n",
        "        },\n",
        "        \"issues_resolved\": [],\n",
        "        \"remaining_issues\": []\n",
        "    }\n",
        "\n",
        "    if revised_critique:\n",
        "        for criterion in original_critique[\"checks\"]:\n",
        "            original_passed = original_critique[\"checks\"][criterion][\"passed\"]\n",
        "            revised_passed = revised_critique[\"checks\"][criterion][\"passed\"]\n",
        "\n",
        "            if not original_passed and revised_passed:\n",
        "                summary[\"issues_resolved\"].append(criterion)\n",
        "            elif not original_passed and not revised_passed:\n",
        "                summary[\"remaining_issues\"].append(criterion)\n",
        "\n",
        "    filename = outputs_path / \"revision_summary.json\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"   Revision summary saved: {filename.name}\")\n",
        "    return str(filename)\n"
      ],
      "metadata": {
        "id": "QF_MU0ho8bLB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This main pipeline loads the latest draft, aggregates sections, critiques quality, iteratively revises the paper, saves all outputs, and produces a final improvement summary in an automated end-to-end workflow.\n"
      ],
      "metadata": {
        "id": "OrrG38A_0F18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. MAIN PIPELINE\n",
        "def run_draft_aggregation_and_critique(max_iterations=2):\n",
        "    \"\"\"\n",
        "    Main pipeline for draft aggregation and critique\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DRAFT AGGREGATION & CRITIQUE MODULE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Step 1: Load existing draft\n",
        "    print(\"\\nSTEP 1: Loading generated draft...\")\n",
        "    draft_text = load_latest_draft()\n",
        "    if not draft_text:\n",
        "        return None\n",
        "\n",
        "    sections = load_individual_sections()\n",
        "    print(f\"  Loaded {len(sections)} sections\")\n",
        "\n",
        "    # Step 2: Create full markdown draft\n",
        "    print(\"\\nSTEP 2: Aggregating full draft...\")\n",
        "    full_draft = create_full_draft_markdown(sections)\n",
        "\n",
        "    # Save initial draft\n",
        "    outputs_path = Path(\"outputs\")\n",
        "    initial_draft_file = outputs_path / \"full_draft_initial.md\"\n",
        "    with open(initial_draft_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(full_draft)\n",
        "    print(f\"  Full draft saved: {initial_draft_file.name}\")\n",
        "\n",
        "    # Step 3: Run critique\n",
        "    print(\"STEP 3: Running draft critique...\")\n",
        "    critique_system = DraftCritique()\n",
        "    critique_results = critique_system.critique_draft(full_draft, sections)\n",
        "\n",
        "    print(f\"\\nCritique Score: {critique_results['score']}/{critique_results['total_checks']}\")\n",
        "\n",
        "    # Save critique feedback\n",
        "    critique_file = save_critique_results(critique_results, iteration=1)\n",
        "\n",
        "    # Step 4: Revision cycles\n",
        "    print(\"\\nSTEP 4: Running revision cycles...\")\n",
        "\n",
        "    current_sections = sections\n",
        "    current_critique = critique_results\n",
        "    all_revisions = []\n",
        "\n",
        "    for iteration in range(1, max_iterations + 1):\n",
        "        print(f\"\\n  teration {iteration}/{max_iterations}\")\n",
        "\n",
        "        # Run revision\n",
        "        revised_draft, revised_sections = run_revision_cycle(\n",
        "            full_draft, current_sections, current_critique, iteration\n",
        "        )\n",
        "\n",
        "        # Save revised draft\n",
        "        draft_file = save_revised_draft(revised_draft, iteration)\n",
        "        all_revisions.append(draft_file)\n",
        "\n",
        "        # Re-critique\n",
        "        if iteration < max_iterations:\n",
        "            current_critique = critique_system.critique_draft(revised_draft, revised_sections)\n",
        "            save_critique_results(current_critique, iteration + 1)\n",
        "            current_sections = revised_sections\n",
        "\n",
        "            print(f\"    Score after revision: {current_critique['score']}/{current_critique['total_checks']}\")\n",
        "\n",
        "    # Step 5: Create final summary\n",
        "    print(\"\\n🔹 STEP 5: Creating revision summary...\")\n",
        "    summary_file = save_revision_summary(critique_results, current_critique, max_iterations)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Show results\n",
        "    print(\"\\n OUTPUTS GENERATED:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"• full_draft_initial.md - Complete aggregated draft\")\n",
        "    print(f\"• critique_feedback_iteration_1.json - Initial critique\")\n",
        "    for i in range(1, max_iterations + 1):\n",
        "        print(f\"• revised_draft_iteration_{i}.md - Revised draft v{i}\")\n",
        "        if i < max_iterations:\n",
        "            print(f\"• critique_feedback_iteration_{i+1}.json - Critique v{i+1}\")\n",
        "    print(f\"• revision_summary.json - Improvement summary\")\n",
        "\n",
        "    print(\"\\n CHECKLIST RESULTS:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Full draft logically structured? {'YES' if critique_results['checks']['structure']['passed'] else 'WITH ISSUES'}\")\n",
        "    print(f\"Critique catches genuine issues? {'YES' if critique_results['score'] < critique_results['total_checks'] else 'ALL PASSED'}\")\n",
        "    print(f\" Revision cycle works? YES ({max_iterations} iterations completed)\")\n",
        "\n",
        "    print(f\"\\n IMPROVEMENT TRACKING:\")\n",
        "    print(f\"   Initial score: {critique_results['score']}/{critique_results['total_checks']}\")\n",
        "    if current_critique and current_critique != critique_results:\n",
        "        print(f\"   Final score: {current_critique['score']}/{current_critique['total_checks']}\")\n",
        "        print(f\"   Improvement: +{current_critique['score'] - critique_results['score']} points\")\n",
        "\n",
        "    return {\n",
        "        \"initial_draft\": full_draft,\n",
        "        \"initial_critique\": critique_results,\n",
        "        \"final_draft\": revised_draft if 'revised_draft' in locals() else full_draft,\n",
        "        \"final_critique\": current_critique,\n",
        "        \"revisions\": all_revisions,\n",
        "        \"summary_file\": summary_file\n",
        "    }\n"
      ],
      "metadata": {
        "id": "iiBUrzca8fgE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This preview function loads the latest critique JSON file and displays the overall score, pass/fail status of each check, and the top revision suggestions in a readable console summary.\n"
      ],
      "metadata": {
        "id": "__hY9f3b0NrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. PREVIEW FUNCTION\n",
        "def preview_critique_results():\n",
        "    \"\"\"\n",
        "    Preview critique results\n",
        "    \"\"\"\n",
        "    outputs_path = Path(\"outputs\")\n",
        "    critique_files = list(outputs_path.glob(\"critique_feedback_iteration_*.json\"))\n",
        "    if not critique_files:\n",
        "        print(\" No critique files found\")\n",
        "        return\n",
        "\n",
        "    latest_critique = max(critique_files, key=lambda x: x.stat().st_mtime)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" CRITIQUE RESULTS PREVIEW\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nFile: {latest_critique.name}\\n\")\n",
        "\n",
        "    with open(latest_critique, 'r', encoding='utf-8') as f:\n",
        "        critique_data = json.load(f)\n",
        "\n",
        "    print(f\"Score: {critique_data['score']}/{critique_data['total_checks']}\")\n",
        "    print(\"\\nChecks Summary:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for check_name, check_info in critique_data[\"checks\"].items():\n",
        "        status = \"✅\" if check_info[\"passed\"] else \"❌\"\n",
        "        print(f\"{status} {check_name}: {check_info['suggestion']}\")\n",
        "\n",
        "    print(\"\\nTop Suggestions:\")\n",
        "    print(\"-\" * 40)\n",
        "    for i, suggestion in enumerate(critique_data.get(\"suggestions\", [])[:3], 1):\n",
        "        print(f\"{i}. {suggestion}\")\n"
      ],
      "metadata": {
        "id": "RcQreVa9yJ9l"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This execution block runs the full draft aggregation, critique, and revision pipeline, reports score improvement, optionally previews critique results, and points to all generated output files.\n"
      ],
      "metadata": {
        "id": "1mg-P_t10V2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. RUN PIPELINE\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    results = run_draft_aggregation_and_critique(max_iterations=2)\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" DRAFT AGGREGATION & CRITIQUE SUCCESSFUL!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Show improvement\n",
        "        initial_score = results[\"initial_critique\"][\"score\"]\n",
        "        final_score = results[\"final_critique\"][\"score\"] if results[\"final_critique\"] else initial_score\n",
        "\n",
        "        print(f\"\\n REVISION IMPROVEMENT: {initial_score} → {final_score}\")\n",
        "\n",
        "        # Offer to preview critique\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        preview = input(\"Would you like to preview critique results? (y/n): \")\n",
        "        if preview.lower() == 'y':\n",
        "            preview_critique_results()\n",
        "\n",
        "        # Show where to find files\n",
        "        print(\"\\n ALL FILES ARE IN: outputs/ folder\")\n",
        "        print(\"   Review: full_draft_initial.md (complete draft)\")\n",
        "        print(\"   Review: critique_feedback_iteration_1.json (detailed feedback)\")\n",
        "        print(\"   Review: revised_draft_iteration_2.md (final revised version)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvP5ZyQcyZvX",
        "outputId": "1d9acba6-9b9c-4dc8-bf11-cd48ed178c42"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "DRAFT AGGREGATION & CRITIQUE MODULE\n",
            "================================================================================\n",
            "\n",
            "STEP 1: Loading generated draft...\n",
            "Loading draft: complete_draft_20260111_063828.txt\n",
            "  Loaded 6 sections\n",
            "\n",
            "STEP 2: Aggregating full draft...\n",
            "\n",
            "Creating full draft in markdown format...\n",
            "  Full draft saved: full_draft_initial.md\n",
            "STEP 3: Running draft critique...\n",
            "\n",
            "Analyzing draft quality...\n",
            "  • Checking clarity... ❌\n",
            "  • Checking flow... ✅\n",
            "  • Checking missing_references... ✅\n",
            "  • Checking repetition... ❌\n",
            "  • Checking style... ❌\n",
            "  • Checking structure... ❌\n",
            "\n",
            "Critique Score: 2/6\n",
            "  Critique feedback saved: critique_feedback_iteration_1.json\n",
            "\n",
            "STEP 4: Running revision cycles...\n",
            "\n",
            "  teration 1/2\n",
            "\n",
            " Running revision cycle 1...\n",
            "\n",
            "Creating full draft in markdown format...\n",
            "  Revised draft saved: revised_draft_iteration_1.md\n",
            "\n",
            "Analyzing draft quality...\n",
            "  • Checking clarity... ✅\n",
            "  • Checking flow... ✅\n",
            "  • Checking missing_references... ✅\n",
            "  • Checking repetition... ❌\n",
            "  • Checking style... ❌\n",
            "  • Checking structure... ❌\n",
            "  Critique feedback saved: critique_feedback_iteration_2.json\n",
            "    Score after revision: 3/6\n",
            "\n",
            "  teration 2/2\n",
            "\n",
            " Running revision cycle 2...\n",
            "\n",
            "Creating full draft in markdown format...\n",
            "  Revised draft saved: revised_draft_iteration_2.md\n",
            "\n",
            "🔹 STEP 5: Creating revision summary...\n",
            "   Revision summary saved: revision_summary.json\n",
            "\n",
            "================================================================================\n",
            "COMPLETE!\n",
            "================================================================================\n",
            "\n",
            " OUTPUTS GENERATED:\n",
            "----------------------------------------\n",
            "• full_draft_initial.md - Complete aggregated draft\n",
            "• critique_feedback_iteration_1.json - Initial critique\n",
            "• revised_draft_iteration_1.md - Revised draft v1\n",
            "• critique_feedback_iteration_2.json - Critique v2\n",
            "• revised_draft_iteration_2.md - Revised draft v2\n",
            "• revision_summary.json - Improvement summary\n",
            "\n",
            " CHECKLIST RESULTS:\n",
            "----------------------------------------\n",
            "Full draft logically structured? WITH ISSUES\n",
            "Critique catches genuine issues? YES\n",
            " Revision cycle works? YES (2 iterations completed)\n",
            "\n",
            " IMPROVEMENT TRACKING:\n",
            "   Initial score: 2/6\n",
            "   Final score: 3/6\n",
            "   Improvement: +1 points\n",
            "\n",
            "================================================================================\n",
            " DRAFT AGGREGATION & CRITIQUE SUCCESSFUL!\n",
            "================================================================================\n",
            "\n",
            " REVISION IMPROVEMENT: 2 → 3\n",
            "\n",
            "================================================================================\n",
            "Would you like to preview critique results? (y/n): y\n",
            "\n",
            "================================================================================\n",
            " CRITIQUE RESULTS PREVIEW\n",
            "================================================================================\n",
            "\n",
            "File: critique_feedback_iteration_2.json\n",
            "\n",
            "Score: 3/6\n",
            "\n",
            "Checks Summary:\n",
            "----------------------------------------\n",
            "✅ clarity: Clarity is good.\n",
            "✅ flow: Flow is good.\n",
            "✅ missing_references: Missing_References is good.\n",
            "❌ repetition: Vary your vocabulary. Use synonyms for frequently used terms. Issues: Overused words: count(14), methodological(10), papers(8)\n",
            "❌ style: Use formal academic language. Avoid informal expressions. Issues: 23 very short paragraphs (<50 words)\n",
            "❌ structure: Ensure all required sections are present and appropriately balanced. Issues: Introduction too short (88 words, aim for 100+)\n",
            "\n",
            "Top Suggestions:\n",
            "----------------------------------------\n",
            "1. Reduce repetition: Use synonyms and vary sentence structure.\n",
            "2. Restructure: Ensure all sections are present and balanced.\n",
            "3. Read draft aloud to catch awkward phrasing.\n",
            "\n",
            " ALL FILES ARE IN: outputs/ folder\n",
            "   Review: full_draft_initial.md (complete draft)\n",
            "   Review: critique_feedback_iteration_1.json (detailed feedback)\n",
            "   Review: revised_draft_iteration_2.md (final revised version)\n"
          ]
        }
      ]
    }
  ]
}