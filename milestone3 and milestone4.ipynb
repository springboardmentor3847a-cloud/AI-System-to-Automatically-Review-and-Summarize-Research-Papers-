{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "my api key: wFKolR3bfa5XUZaFntmdo5AXd7kL506y1klYRd3y\n"
      ],
      "metadata": {
        "id": "zViEIR-ucCBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install semanticscholar -q\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from semanticscholar import SemanticScholar\n",
        "\n",
        "\n",
        "def get_semantic_scholar_client():\n",
        "    \"\"\"\n",
        "    Initializes Semantic Scholar API client.\n",
        "\n",
        "    Priority:\n",
        "    1. Environment variable (recommended)\n",
        "    2. Manual API key (fallback)\n",
        "    3. Limited-access mode\n",
        "\n",
        "    Returns:\n",
        "        SemanticScholar object\n",
        "    \"\"\"\n",
        "\n",
        "    # Load .env variables (if available)\n",
        "    load_dotenv()\n",
        "\n",
        "    # OPTION 1: Environment variable\n",
        "    api_key = os.getenv(\"wFKolR3bfa5XUZaFntmdo5AXu7kL506y1klYRd3y\")\n",
        "\n",
        "    # OPTION 2: MANUAL KEY (PUT YOUR KEY HERE IF NEEDED)\n",
        "    # ‚ö† REMOVE BEFORE SUBMISSION\n",
        "    MANUAL_API_KEY = \"\"  # <-- Paste your API key here if needed\n",
        "\n",
        "    if api_key:\n",
        "        print(\"üîë API key loaded from environment.\")\n",
        "        return SemanticScholar(api_key=api_key)\n",
        "\n",
        "    elif MANUAL_API_KEY:\n",
        "        print(\"üîë API key loaded manually.\")\n",
        "        return SemanticScholar(api_key=MANUAL_API_KEY)\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö† No API key found.\")\n",
        "        print(\"‚ö† Running in LIMITED API mode.\")\n",
        "        return SemanticScholar()\n",
        "\n",
        "# ============================================================\n",
        "# MODULE 0: SEMANTIC SCHOLAR API CONFIGURATION\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "This module handles:\n",
        "‚úî Secure API key loading\n",
        "‚úî Semantic Scholar client initialization\n",
        "‚úî Safe fallback to limited-access mode\n",
        "\n",
        "All other modules import and reuse this client.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "c7GI6fyv3_cQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8fbb3349-5829-4922-84c2-1dbd9a30033b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis module handles:\\n‚úî Secure API key loading\\n‚úî Semantic Scholar client initialization\\n‚úî Safe fallback to limited-access mode\\n\\nAll other modules import and reuse this client.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODULE 1: Topic Input & Paper Search (LIMITED TO 50 PAPERS)\n",
        "\n",
        "!pip install semanticscholar python-dotenv requests tabulate pandas -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "from semanticscholar import SemanticScholar\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0. CREATE PROJECT FOLDER STRUCTURE\n",
        "# ------------------------------------------------------------\n",
        "def create_project_folders():\n",
        "    folders = [\n",
        "        \"data\",\n",
        "        \"data/search_results\",\n",
        "        \"data/raw_pdfs\",\n",
        "        \"data/metadata\"\n",
        "    ]\n",
        "    for folder in folders:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "    print(\"üìÅ Project folders verified/created.\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. SETUP API KEY\n",
        "# ------------------------------------------------------------\n",
        "def setup_api_key():\n",
        "    load_dotenv()\n",
        "    API_KEY = os.getenv(\"wFKolR3bfa5XUZaFntmdo5AXd7kL506y1klYRd3y\")\n",
        "\n",
        "    if not API_KEY:\n",
        "        print(\"‚ö† No API key found. Running with LIMITED API rate.\")\n",
        "        return SemanticScholar()\n",
        "    else:\n",
        "        print(\"üîë API Key loaded successfully.\")\n",
        "        return SemanticScholar(api_key=API_KEY)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. SEARCH PAPERS (STRICT LIMIT = 50)\n",
        "# ------------------------------------------------------------\n",
        "def search_papers(topic, year_filter=None, open_access_only=False):\n",
        "    sch = setup_api_key()\n",
        "\n",
        "    MAX_PAPERS = 50\n",
        "    print(f\"\\nüîé Searching for papers on: '{topic}'\")\n",
        "    print(f\"üì° Fetching up to {MAX_PAPERS} papers from Semantic Scholar...\")\n",
        "\n",
        "    try:\n",
        "        results = sch.search_paper(\n",
        "            query=topic,\n",
        "            limit=MAX_PAPERS,\n",
        "            fields=[\n",
        "                \"paperId\", \"title\", \"abstract\", \"year\", \"authors\",\n",
        "                \"citationCount\", \"openAccessPdf\", \"url\", \"venue\"\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        papers = []\n",
        "\n",
        "        for paper in results:\n",
        "            if len(papers) >= MAX_PAPERS:\n",
        "                break  # HARD STOP\n",
        "\n",
        "            # Apply optional filters\n",
        "            if year_filter and paper.year != year_filter:\n",
        "                continue\n",
        "            if open_access_only and not paper.openAccessPdf:\n",
        "                continue\n",
        "\n",
        "            papers.append({\n",
        "                \"paperId\": paper.paperId,\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": [a[\"name\"] for a in paper.authors] if paper.authors else [],\n",
        "                \"year\": paper.year,\n",
        "                \"abstract\": (paper.abstract[:300] + \"...\") if paper.abstract else \"No abstract available\",\n",
        "                \"citationCount\": paper.citationCount,\n",
        "                \"venue\": paper.venue,\n",
        "                \"url\": paper.url,\n",
        "                \"pdf_url\": paper.openAccessPdf[\"url\"] if paper.openAccessPdf else None,\n",
        "                \"has_pdf\": bool(paper.openAccessPdf)\n",
        "            })\n",
        "\n",
        "        print(\"\\nüìä Search Completed!\")\n",
        "        print(f\"   ‚û§ Total papers collected: {len(papers)}\")\n",
        "        print(f\"   ‚û§ Papers with PDF: {sum(p['has_pdf'] for p in papers)}\")\n",
        "\n",
        "        return {\n",
        "            \"topic\": topic,\n",
        "            \"total_results\": len(papers),\n",
        "            \"papers\": papers\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during search: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. SAVE RESULTS TO JSON + CSV\n",
        "# ------------------------------------------------------------\n",
        "def save_search_results(data):\n",
        "    topic_clean = data[\"topic\"].replace(\" \", \"_\")\n",
        "    json_path = f\"data/search_results/{topic_clean}.json\"\n",
        "    csv_path = f\"data/search_results/{topic_clean}.csv\"\n",
        "\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "    print(f\"üíæ JSON saved to: {json_path}\")\n",
        "\n",
        "    df = pd.DataFrame(data[\"papers\"])\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"üìä CSV saved to: {csv_path}\")\n",
        "\n",
        "    return json_path, csv_path\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. DISPLAY RESULTS\n",
        "# ------------------------------------------------------------\n",
        "def display_search_results(data):\n",
        "    papers = data[\"papers\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(f\"SEARCH SUMMARY: {data['topic']}\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    print(f\"\\nüìå Total Papers: {len(papers)}\")\n",
        "    print(f\"üìå Papers with PDF: {sum(p['has_pdf'] for p in papers)}\")\n",
        "    print(f\"üìå Papers without PDF: {sum(not p['has_pdf'] for p in papers)}\")\n",
        "\n",
        "    table_data = []\n",
        "    for p in papers[:10]:\n",
        "        table_data.append([\n",
        "            p[\"title\"][:40] + (\"...\" if len(p[\"title\"]) > 40 else \"\"),\n",
        "            p[\"year\"],\n",
        "            p[\"citationCount\"],\n",
        "            \"Yes\" if p[\"has_pdf\"] else \"No\"\n",
        "        ])\n",
        "\n",
        "    print(\"\\nüìù TOP PAPERS TABLE:\")\n",
        "    print(tabulate(table_data, headers=[\"Title\", \"Year\", \"Citations\", \"PDF\"], tablefmt=\"pretty\"))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. MAIN EXECUTION\n",
        "# ------------------------------------------------------------\n",
        "def main_search():\n",
        "    create_project_folders()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" MODULE 1: TOPIC INPUT & PAPER SEARCH\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    topic = input(\"\\nEnter research topic: \").strip() or \"machine learning\"\n",
        "\n",
        "    year_input = input(\"Filter by year? (Press Enter to skip): \").strip()\n",
        "    year_filter = int(year_input) if year_input.isdigit() else None\n",
        "\n",
        "    oa_input = input(\"Open access only? (y/n): \").lower().strip()\n",
        "    open_access_only = oa_input == \"y\"\n",
        "\n",
        "    results = search_papers(\n",
        "        topic,\n",
        "        year_filter=year_filter,\n",
        "        open_access_only=open_access_only\n",
        "    )\n",
        "\n",
        "    if not results or results[\"total_results\"] == 0:\n",
        "        print(\"\\n‚ùå No papers found.\")\n",
        "        return None\n",
        "\n",
        "    json_path, csv_path = save_search_results(results)\n",
        "    display_search_results(results)\n",
        "\n",
        "    print(\"\\n‚úÖ Module 1 Completed Successfully!\")\n",
        "    print(f\"‚û° JSON saved at: {json_path}\")\n",
        "    print(f\"‚û° CSV saved at: {csv_path}\")\n",
        "    print(\"‚û° Proceed to Module 2 for downloading PDFs.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# RUN MODULE 1\n",
        "main_search()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLDnpu2s2x1j",
        "outputId": "dd352559-b57b-41bd-ff4e-50abebbb2702"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Project folders verified/created.\n",
            "\n",
            "================================================================================\n",
            " MODULE 1: TOPIC INPUT & PAPER SEARCH\n",
            "================================================================================\n",
            "\n",
            "Enter research topic: environment\n",
            "Filter by year? (Press Enter to skip): \n",
            "Open access only? (y/n): y\n",
            "‚ö† No API key found. Running with LIMITED API rate.\n",
            "\n",
            "üîé Searching for papers on: 'environment'\n",
            "üì° Fetching up to 50 papers from Semantic Scholar...\n",
            "\n",
            "üìä Search Completed!\n",
            "   ‚û§ Total papers collected: 50\n",
            "   ‚û§ Papers with PDF: 50\n",
            "üíæ JSON saved to: data/search_results/environment.json\n",
            "üìä CSV saved to: data/search_results/environment.csv\n",
            "\n",
            "==========================================================================================\n",
            "SEARCH SUMMARY: environment\n",
            "==========================================================================================\n",
            "\n",
            "üìå Total Papers: 50\n",
            "üìå Papers with PDF: 50\n",
            "üìå Papers without PDF: 0\n",
            "\n",
            "üìù TOP PAPERS TABLE:\n",
            "+---------------------------------------------+------+-----------+-----+\n",
            "|                    Title                    | Year | Citations | PDF |\n",
            "+---------------------------------------------+------+-----------+-----+\n",
            "| ape 5.0: an environment for modern phylo... | 2018 |   6905    | Yes |\n",
            "| R: A language and environment for statis... | 2014 |  341592   | Yes |\n",
            "| Cytoscape: a software environment for in... | 2003 |   43198   | Yes |\n",
            "|    Matplotlib: A 2D Graphics Environment    | 2007 |   22859   | Yes |\n",
            "| WebArena: A Realistic Web Environment fo... | 2023 |    807    | Yes |\n",
            "| Pesticides: An alarming detrimental to h... | 2024 |    250    | Yes |\n",
            "| Towards Smart and Reconfigurable Environ... | 2019 |   3461    | Yes |\n",
            "| Degradation Rates of Plastics in the Env... | 2020 |   2269    | Yes |\n",
            "|    Decision-making in fuzzy environment     | 2012 |   6253    | Yes |\n",
            "| Authoritative sources in a hyperlinked e... | 1999 |   11192   | Yes |\n",
            "+---------------------------------------------+------+-----------+-----+\n",
            "\n",
            "‚úÖ Module 1 Completed Successfully!\n",
            "‚û° JSON saved at: data/search_results/environment.json\n",
            "‚û° CSV saved at: data/search_results/environment.csv\n",
            "‚û° Proceed to Module 2 for downloading PDFs.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'environment',\n",
              " 'total_results': 50,\n",
              " 'papers': [{'paperId': '191067949003f2351044690f235648c79ef7621c',\n",
              "   'title': 'ape 5.0: an environment for modern phylogenetics and evolutionary analyses in R',\n",
              "   'authors': ['E. Paradis', 'K. Schliep'],\n",
              "   'year': 2018,\n",
              "   'abstract': 'Summary\\nAfter more than fifteen years of existence, the R package ape has continuously grown its contents, and has been used by a growing community of users. The release of version 5.0 has marked a leap towards a modern software for evolutionary analyses. Efforts have been put to improve efficiency,...',\n",
              "   'citationCount': 6905,\n",
              "   'venue': 'Bioinform.',\n",
              "   'url': 'https://www.semanticscholar.org/paper/191067949003f2351044690f235648c79ef7621c',\n",
              "   'pdf_url': 'https://academic.oup.com/bioinformatics/article-pdf/35/3/526/27699799/bty633.pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '659408b243cec55de8d0a3bc51b81173007aa89b',\n",
              "   'title': 'R: A language and environment for statistical computing.',\n",
              "   'authors': ['R. Team'],\n",
              "   'year': 2014,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 341592,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/659408b243cec55de8d0a3bc51b81173007aa89b',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '002a36359ab0680fac3dd492a113475b5e782ae0',\n",
              "   'title': 'Cytoscape: a software environment for integrated models of biomolecular interaction networks.',\n",
              "   'authors': ['Paul Shannon',\n",
              "    'Andrew Markiel',\n",
              "    'Owen Ozier',\n",
              "    'N. Baliga',\n",
              "    'Jonathan T Wang',\n",
              "    'Daniel Ramage',\n",
              "    'Nada Amin',\n",
              "    'Benno Schwikowski',\n",
              "    'T. Ideker'],\n",
              "   'year': 2003,\n",
              "   'abstract': 'Cytoscape is an open source software project for integrating biomolecular interaction networks with high-throughput expression data and other molecular states into a unified conceptual framework. Although applicable to any system of molecular components and interactions, Cytoscape is most powerful w...',\n",
              "   'citationCount': 43198,\n",
              "   'venue': 'Genome Research',\n",
              "   'url': 'https://www.semanticscholar.org/paper/002a36359ab0680fac3dd492a113475b5e782ae0',\n",
              "   'pdf_url': 'https://genome.cshlp.org/content/13/11/2498.full.pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '412a0bb5a3baa91b62053d82c562bc172df0439f',\n",
              "   'title': 'Matplotlib: A 2D Graphics Environment',\n",
              "   'authors': ['John D. Hunter'],\n",
              "   'year': 2007,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 22859,\n",
              "   'venue': 'Computing in science & engineering (Print)',\n",
              "   'url': 'https://www.semanticscholar.org/paper/412a0bb5a3baa91b62053d82c562bc172df0439f',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'e41482f4ee984f17382f6cdd900df094d928be06',\n",
              "   'title': 'WebArena: A Realistic Web Environment for Building Autonomous Agents',\n",
              "   'authors': ['Shuyan Zhou',\n",
              "    'Frank F. Xu',\n",
              "    'Hao Zhu',\n",
              "    'Xuhui Zhou',\n",
              "    'Robert Lo',\n",
              "    'Abishek Sridhar',\n",
              "    'Xianyi Cheng',\n",
              "    'Yonatan Bisk',\n",
              "    'Daniel Fried',\n",
              "    'Uri Alon',\n",
              "    'Graham Neubig'],\n",
              "   'year': 2023,\n",
              "   'abstract': 'With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build ...',\n",
              "   'citationCount': 807,\n",
              "   'venue': 'International Conference on Learning Representations',\n",
              "   'url': 'https://www.semanticscholar.org/paper/e41482f4ee984f17382f6cdd900df094d928be06',\n",
              "   'pdf_url': 'https://arxiv.org/pdf/2307.13854',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'eff90d322ce41d0855e0ef0c02d5797f9e193b5b',\n",
              "   'title': 'Pesticides: An alarming detrimental to health and environment.',\n",
              "   'authors': ['Rajwinder Kaur',\n",
              "    'Diksha Choudhary',\n",
              "    'Samriddhi Bali',\n",
              "    'Shubhdeep Singh Bandral',\n",
              "    'Varinder Singh',\n",
              "    'Md. Altamash Ahmad',\n",
              "    'Nidhi Rani',\n",
              "    'Thakur Gurjeet Singh',\n",
              "    'Balakumar Chandrasekaran'],\n",
              "   'year': 2024,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 250,\n",
              "   'venue': 'Science of the Total Environment',\n",
              "   'url': 'https://www.semanticscholar.org/paper/eff90d322ce41d0855e0ef0c02d5797f9e193b5b',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '36e3111b1b2e86e17ed38245d65d61756f5e35af',\n",
              "   'title': 'Towards Smart and Reconfigurable Environment: Intelligent Reflecting Surface Aided Wireless Network',\n",
              "   'authors': ['Qingqing Wu', 'Rui Zhang'],\n",
              "   'year': 2019,\n",
              "   'abstract': 'IRS is a new and revolutionizing technology that is able to significantly improve the performance of wireless communication networks, by smartly reconfiguring the wireless propagation environment with the use of massive low-cost passive reflecting elements integrated on a planar surface. Specificall...',\n",
              "   'citationCount': 3461,\n",
              "   'venue': 'IEEE Communications Magazine',\n",
              "   'url': 'https://www.semanticscholar.org/paper/36e3111b1b2e86e17ed38245d65d61756f5e35af',\n",
              "   'pdf_url': 'https://arxiv.org/pdf/1905.00152',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '163abd9b5158e40a51202b46a269bde24ed4f87a',\n",
              "   'title': 'Degradation Rates of Plastics in the Environment',\n",
              "   'authors': ['Ali Chamas',\n",
              "    'H. Moon',\n",
              "    'Jiajia Zheng',\n",
              "    'Yang Qiu',\n",
              "    'Tarnuma Tabassum',\n",
              "    'J. Jang',\n",
              "    'M. Abu‚ÄêOmar',\n",
              "    'S. Scott',\n",
              "    'S. Suh'],\n",
              "   'year': 2020,\n",
              "   'abstract': 'Plastic waste is currently generated at a rate approaching 400 Mt year‚Äì1. The amount of plastics accumulating in the environment is growing rapidly, yet our understanding of its persistence is very......',\n",
              "   'citationCount': 2269,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/163abd9b5158e40a51202b46a269bde24ed4f87a',\n",
              "   'pdf_url': 'https://pubs.acs.org/doi/pdf/10.1021/acssuschemeng.9b06635',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '64c1064a50a10f651a02d17faa714baf12dba488',\n",
              "   'title': 'Decision-making in fuzzy environment',\n",
              "   'authors': ['Chitrasen Samantra'],\n",
              "   'year': 2012,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 6253,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/64c1064a50a10f651a02d17faa714baf12dba488',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'f1962baa356e10c4fd936b9e9f910869a5864a75',\n",
              "   'title': 'Authoritative sources in a hyperlinked environment',\n",
              "   'authors': ['J. Kleinberg'],\n",
              "   'year': 1999,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 11192,\n",
              "   'venue': 'ACM-SIAM Symposium on Discrete Algorithms',\n",
              "   'url': 'https://www.semanticscholar.org/paper/f1962baa356e10c4fd936b9e9f910869a5864a75',\n",
              "   'pdf_url': 'https://dl.acm.org/doi/pdf/10.1145/324133.324140',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '2839bb1419590c965be2f7ffc323e0cd68241829',\n",
              "   'title': 'Heavy metal pollution in the environment and their toxicological effects on humans',\n",
              "   'authors': ['J. Briffa', 'E. Sinagra', 'R. Blundell'],\n",
              "   'year': 2020,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 2987,\n",
              "   'venue': 'Heliyon',\n",
              "   'url': 'https://www.semanticscholar.org/paper/2839bb1419590c965be2f7ffc323e0cd68241829',\n",
              "   'pdf_url': 'http://www.cell.com/article/S2405844020315346/pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '38664d31f642ec7b39f7859400313e4c394c048e',\n",
              "   'title': 'SWISS‚ÄêMODEL and the Swiss‚ÄêPdb Viewer: An environment for comparative protein modeling',\n",
              "   'authors': ['N. Guex', 'M. Peitsch'],\n",
              "   'year': 1997,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 11294,\n",
              "   'venue': 'Electrophoresis',\n",
              "   'url': 'https://www.semanticscholar.org/paper/38664d31f642ec7b39f7859400313e4c394c048e',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '51ec796a5c5d226c8ba77437271dc50b1adf1bff',\n",
              "   'title': 'On Limits of Wireless Communications in a Fading Environment when Using Multiple Antennas',\n",
              "   'authors': ['G. Foschini', 'M. Gans'],\n",
              "   'year': 1998,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 11288,\n",
              "   'venue': 'Wireless personal communications',\n",
              "   'url': 'https://www.semanticscholar.org/paper/51ec796a5c5d226c8ba77437271dc50b1adf1bff',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'd1da853892ab1db2db040b43f4fd296d639c5636',\n",
              "   'title': 'Agriculture Development, Pesticide Application and Its Impact on the Environment',\n",
              "   'authors': ['Muyesaier Tudi',\n",
              "    'Huada Daniel Ruan',\n",
              "    'Li Wang',\n",
              "    'J. Lyu',\n",
              "    'R. Sadler',\n",
              "    'D. Connell',\n",
              "    'C. Chu',\n",
              "    'D. Phung'],\n",
              "   'year': 2021,\n",
              "   'abstract': 'Pesticides are indispensable in agricultural production. They have been used by farmers to control weeds and insects, and their remarkable increases in agricultural products have been reported. The increase in the world‚Äôs population in the 20th century could not have been possible without a parallel...',\n",
              "   'citationCount': 1797,\n",
              "   'venue': 'International Journal of Environmental Research and Public Health',\n",
              "   'url': 'https://www.semanticscholar.org/paper/d1da853892ab1db2db040b43f4fd296d639c5636',\n",
              "   'pdf_url': 'https://www.mdpi.com/1660-4601/18/3/1112/pdf?version=1611886950',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '7a2bfd5490d3e4026a8f97d03e9141e08f671660',\n",
              "   'title': 'Antibiotic resistance in the environment',\n",
              "   'authors': ['D. Larsson', 'Carl-Fredrik Flach'],\n",
              "   'year': 2021,\n",
              "   'abstract': 'Antibiotic resistance is a global health challenge, involving the transfer of bacteria and genes between humans, animals and the environment. Although multiple barriers restrict the flow of both bacteria and genes, pathogens recurrently acquire new resistance factors from other species, thereby redu...',\n",
              "   'citationCount': 1731,\n",
              "   'venue': 'Nature Reviews Microbiology',\n",
              "   'url': 'https://www.semanticscholar.org/paper/7a2bfd5490d3e4026a8f97d03e9141e08f671660',\n",
              "   'pdf_url': 'https://www.nature.com/articles/s41579-021-00649-x.pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'd038d7acea15ed1a4454ed47f53da7135415b8a4',\n",
              "   'title': 'Andromeda: a peptide search engine integrated into the MaxQuant environment.',\n",
              "   'authors': ['J. Cox',\n",
              "    'Nadin Neuhauser',\n",
              "    'Annette Michalski',\n",
              "    'R. Scheltema',\n",
              "    'J. V. Olsen',\n",
              "    'M. Mann'],\n",
              "   'year': 2011,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 5321,\n",
              "   'venue': 'Journal of Proteome Research',\n",
              "   'url': 'https://www.semanticscholar.org/paper/d038d7acea15ed1a4454ed47f53da7135415b8a4',\n",
              "   'pdf_url': 'https://doi.org/10.1021/pr101065j',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '60b3d1bfcb7a2bbd46cde6c6c5c91334cf1d13a3',\n",
              "   'title': 'Microplastics in the marine environment.',\n",
              "   'authors': ['A. Andrady'],\n",
              "   'year': 2011,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 6307,\n",
              "   'venue': 'Marine Pollution Bulletin',\n",
              "   'url': 'https://www.semanticscholar.org/paper/60b3d1bfcb7a2bbd46cde6c6c5c91334cf1d13a3',\n",
              "   'pdf_url': 'https://doi.org/10.1016/j.marpolbul.2011.05.030',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'c21252cac5b6c33012d2a071433f79ab0beeb0fb',\n",
              "   'title': 'The SWISS-MODEL workspace: a web-based environment for protein structure homology modelling',\n",
              "   'authors': ['Konstantin Arnold',\n",
              "    'L. Bordoli',\n",
              "    'J√ºrgen Kopp',\n",
              "    'Torsten Schwede'],\n",
              "   'year': 2006,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 7380,\n",
              "   'venue': 'Bioinform.',\n",
              "   'url': 'https://www.semanticscholar.org/paper/c21252cac5b6c33012d2a071433f79ab0beeb0fb',\n",
              "   'pdf_url': 'https://academic.oup.com/bioinformatics/article-pdf/22/2/195/16851678/bti770.pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '13ceda93a3f46544d59fad1c22c6e07c03bf515a',\n",
              "   'title': 'KEGG for linking genomes to life and the environment',\n",
              "   'authors': ['M. Kanehisa',\n",
              "    'M. Araki',\n",
              "    'S. Goto',\n",
              "    'M. Hattori',\n",
              "    'M. Hirakawa',\n",
              "    'M. Itoh',\n",
              "    'Toshiaki Katayama',\n",
              "    'S. Kawashima',\n",
              "    'Shujiro Okuda',\n",
              "    'T. Tokimatsu',\n",
              "    'Yoshihiro Yamanishi'],\n",
              "   'year': 2007,\n",
              "   'abstract': 'KEGG (http://www.genome.jp/kegg/) is a database of biological systems that integrates genomic, chemical and systemic functional information. KEGG provides a reference knowledge base for linking genomes to life through the process of PATHWAY mapping, which is to map, for example, a genomic or transcr...',\n",
              "   'citationCount': 6348,\n",
              "   'venue': 'Nucleic Acids Res.',\n",
              "   'url': 'https://www.semanticscholar.org/paper/13ceda93a3f46544d59fad1c22c6e07c03bf515a',\n",
              "   'pdf_url': 'https://academic.oup.com/nar/article-pdf/36/suppl_1/D480/7632780/gkm882.pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '288e26900d776e26d2dd45b91e4f50980e55a498',\n",
              "   'title': 'The perception of the environment : essays on livelihood, dwelling and skill',\n",
              "   'authors': ['T. Ingold'],\n",
              "   'year': 2000,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 6321,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/288e26900d776e26d2dd45b91e4f50980e55a498',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'c5da1106929752414ca6d110f9ceb4500662cb9b',\n",
              "   'title': 'PowerMarker: an integrated analysis environment for genetic marker analysis',\n",
              "   'authors': ['Kejun Liu', 'S. Muse'],\n",
              "   'year': 2005,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 4224,\n",
              "   'venue': 'Bioinform.',\n",
              "   'url': 'https://www.semanticscholar.org/paper/c5da1106929752414ca6d110f9ceb4500662cb9b',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '428f880c29b7af69e305a2bf73e425dfb9d14ec8',\n",
              "   'title': 'Evaporation and environment.',\n",
              "   'authors': ['J. Monteith'],\n",
              "   'year': 1965,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 5380,\n",
              "   'venue': 'Symposia of the Society for Experimental Biology',\n",
              "   'url': 'https://www.semanticscholar.org/paper/428f880c29b7af69e305a2bf73e425dfb9d14ec8',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'abbd6f8840092bd8e4e3684d3297e1b707abdc5d',\n",
              "   'title': 'Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment',\n",
              "   'authors': ['IU C.L.L', 'J. Layland'],\n",
              "   'year': 1989,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 7037,\n",
              "   'venue': 'JACM',\n",
              "   'url': 'https://www.semanticscholar.org/paper/abbd6f8840092bd8e4e3684d3297e1b707abdc5d',\n",
              "   'pdf_url': 'https://dl.acm.org/doi/pdf/10.1145/321738.321743',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'b1e141498735c775d232fe1b81432ae748b96ab7',\n",
              "   'title': 'ARB: a software environment for sequence data.',\n",
              "   'authors': ['W. Ludwig',\n",
              "    'O. Strunk',\n",
              "    'Ralf Westram',\n",
              "    'L. Richter',\n",
              "    'H. Meier',\n",
              "    'Yadhukumar',\n",
              "    'Arno Buchner',\n",
              "    'Tina Lai',\n",
              "    'Susanne Steppi',\n",
              "    'Gangolf Jobb',\n",
              "    'Wolfram F√∂rster',\n",
              "    'Igor Brettske',\n",
              "    'Stefan Gerber',\n",
              "    'Anton W Ginhart',\n",
              "    'Oliver Gross',\n",
              "    'Silke Grumann',\n",
              "    'Stefan Hermann',\n",
              "    'R. Jost',\n",
              "    'A. K√∂nig',\n",
              "    'T. Li√ü',\n",
              "    'Ralph L√ºssmann',\n",
              "    'M. May',\n",
              "    'Bj√∂rn Nonhoff',\n",
              "    'B. Reichel',\n",
              "    'R. Strehlow',\n",
              "    'A. Stamatakis',\n",
              "    'Norbert Stuckmann',\n",
              "    'Alexander Vilbig',\n",
              "    'M. Lenke',\n",
              "    'T. Ludwig',\n",
              "    'A. Bode',\n",
              "    'K. Schleifer'],\n",
              "   'year': 2004,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 6986,\n",
              "   'venue': 'Nucleic Acids Research',\n",
              "   'url': 'https://www.semanticscholar.org/paper/b1e141498735c775d232fe1b81432ae748b96ab7',\n",
              "   'pdf_url': 'https://academic.oup.com/nar/article-pdf/32/4/1363/6258500/gkh293.pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '09991899ddc3fed0421afafc0fa63b46cbe3e4ed',\n",
              "   'title': 'Critical Inquiry in a Text-Based Environment: Computer Conferencing in Higher Education',\n",
              "   'authors': ['D. Garrison', 'T. Anderson', 'W. Archer'],\n",
              "   'year': 1999,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 5839,\n",
              "   'venue': 'Internet and Higher Education',\n",
              "   'url': 'https://www.semanticscholar.org/paper/09991899ddc3fed0421afafc0fa63b46cbe3e4ed',\n",
              "   'pdf_url': 'http://auspace.athabascau.ca:8080/bitstream/2149/739/1/critical_inquiry_in_a_text.pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '1bde55d23c7722e7bbd41d06f8a28ab29f5ef241',\n",
              "   'title': 'Assessing the Work Environment for Creativity',\n",
              "   'authors': ['Teresa M. Amabile',\n",
              "    'Regina Conti',\n",
              "    'Heather M. Coon',\n",
              "    'J. Lazenby',\n",
              "    'M. Herron'],\n",
              "   'year': 1996,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 5975,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/1bde55d23c7722e7bbd41d06f8a28ab29f5ef241',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '4e4812c79144c2bedbcc1f5f6a04c12f683c10a5',\n",
              "   'title': 'R Development Core Team (2010): R: A language and environment for statistical computing',\n",
              "   'authors': ['P. Dalgaard'],\n",
              "   'year': 2010,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 4823,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/4e4812c79144c2bedbcc1f5f6a04c12f683c10a5',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '2e78ff39f485c9daaf26dd82f36fb0ccd09e34da',\n",
              "   'title': 'The Environment and Disease: Association or Causation?',\n",
              "   'authors': ['A. B. Hill'],\n",
              "   'year': 1965,\n",
              "   'abstract': 'In 1965, Sir Austin Bradford Hill offered his thoughts on: ‚ÄúWhat aspects of [an] association should we especially consider before deciding that the most likely interpretation of it is causation?‚Äù He proposed nine means for reasoning about the association, which he named as: strength, consistency, sp...',\n",
              "   'citationCount': 7222,\n",
              "   'venue': 'Proceedings of the Royal Society of Medicine',\n",
              "   'url': 'https://www.semanticscholar.org/paper/2e78ff39f485c9daaf26dd82f36fb0ccd09e34da',\n",
              "   'pdf_url': 'https://journals.sagepub.com/doi/pdf/10.1177/003591576505800503',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '837c39e0380d556e09de72a592265b7667e72827',\n",
              "   'title': 'Per- and polyfluoroalkyl substances in the environment',\n",
              "   'authors': ['Marina G Evich',\n",
              "    'Mary J. B. Davis',\n",
              "    'James P. McCord',\n",
              "    'Brad Acrey',\n",
              "    'Jill A. Awkerman',\n",
              "    'D. Knappe',\n",
              "    'A. Lindstrom',\n",
              "    'T. Speth',\n",
              "    'C. Tebes-Stevens',\n",
              "    'M. Strynar',\n",
              "    'Zhanyun Wang',\n",
              "    'E. J. Weber',\n",
              "    'W. M. Henderson',\n",
              "    'J. Washington'],\n",
              "   'year': 2022,\n",
              "   'abstract': 'Over the past several years, the term PFAS (per- and polyfluoroalkyl substances) has grown to be emblematic of environmental contamination, garnering public, scientific, and regulatory concern. PFAS are synthesized by two processes, direct fluorination (e.g., electrochemical fluorination) and oligom...',\n",
              "   'citationCount': 1169,\n",
              "   'venue': 'Science',\n",
              "   'url': 'https://www.semanticscholar.org/paper/837c39e0380d556e09de72a592265b7667e72827',\n",
              "   'pdf_url': 'https://www.science.org/doi/pdf/10.1126/science.abg9065?download=true',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '2c2641ec43e268f8f3c340a1a969d963139d3afd',\n",
              "   'title': 'Microplastics in the marine environment: a review of the methods used for identification and quantification.',\n",
              "   'authors': ['V. Hidalgo‚ÄêRuz',\n",
              "    'L. Gutow',\n",
              "    'Richard C. Thompson',\n",
              "    'M. Thiel'],\n",
              "   'year': 2012,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 4176,\n",
              "   'venue': 'Environmental Science and Technology',\n",
              "   'url': 'https://www.semanticscholar.org/paper/2c2641ec43e268f8f3c340a1a969d963139d3afd',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'd9d664152ebbbc8a704888fb0447fffb401a6daf',\n",
              "   'title': 'Economic Growth and the Environment',\n",
              "   'authors': ['T. Panayotou'],\n",
              "   'year': 2000,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 4038,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/d9d664152ebbbc8a704888fb0447fffb401a6daf',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'ffdcfcc5a3e34970d99c7aa09ea20327e02c5afa',\n",
              "   'title': 'Travel and the Built Environment',\n",
              "   'authors': ['R. Ewing', 'R. Cervero'],\n",
              "   'year': 2010,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 4558,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/ffdcfcc5a3e34970d99c7aa09ea20327e02c5afa',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'f9de9097587fafb362ecacfd8341d986e51570cc',\n",
              "   'title': 'Environment and Planning D: Society and Space',\n",
              "   'authors': ['N. Thrift'],\n",
              "   'year': 1995,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 4120,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/f9de9097587fafb362ecacfd8341d986e51570cc',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'f37ef42eb6a3b3c8c6aa3e06f6736b0678598492',\n",
              "   'title': 'Heavy metal toxicity and the environment.',\n",
              "   'authors': ['P. Tchounwou', 'C. Yedjou', 'A. Patlolla', 'D. Sutton'],\n",
              "   'year': 2012,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 5274,\n",
              "   'venue': 'Experientia. Supplementum',\n",
              "   'url': 'https://www.semanticscholar.org/paper/f37ef42eb6a3b3c8c6aa3e06f6736b0678598492',\n",
              "   'pdf_url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4144270',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'd6ca7b32ddf13529942c678ff177a096f7257d4b',\n",
              "   'title': 'The Role of the Linguistic Environment in Second Language Acquisition',\n",
              "   'authors': ['Michael H. Long'],\n",
              "   'year': 1996,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 3715,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/d6ca7b32ddf13529942c678ff177a096f7257d4b',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '433d14e40f0d5362df4016270ba97e13371bc42a',\n",
              "   'title': 'The atomic simulation environment-a Python library for working with atoms.',\n",
              "   'authors': ['Ask Hjorth Larsen',\n",
              "    'Jens J√∏rgen Mortensen',\n",
              "    'J. Blomqvist',\n",
              "    'I. Castelli',\n",
              "    'R. Christensen',\n",
              "    'M. Dulak',\n",
              "    'J. Friis',\n",
              "    'M. Groves',\n",
              "    'B. Hammer',\n",
              "    'Cory Hargus',\n",
              "    'E. Hermes',\n",
              "    'P. C. Jennings',\n",
              "    'Peter Bjerre Jensen',\n",
              "    'J. Kermode',\n",
              "    'J. Kitchin',\n",
              "    'Esben Leonhard Kolsbjerg',\n",
              "    'J. Kubal',\n",
              "    'K. Kaasbjerg',\n",
              "    'S. Lysgaard',\n",
              "    'J√≥n Bergmann Maronsson',\n",
              "    'Tristan Maxson',\n",
              "    'T. Olsen',\n",
              "    'L. Pastewka',\n",
              "    'Andrew A. Peterson',\n",
              "    'C. Rostgaard',\n",
              "    'J. Schi√∏tz',\n",
              "    'O. Sch√ºtt',\n",
              "    'M. Strange',\n",
              "    'K. Thygesen',\n",
              "    'T. Vegge',\n",
              "    'L. Vilhelmsen',\n",
              "    'M. Walter',\n",
              "    'Z. Zeng',\n",
              "    'K. Jacobsen'],\n",
              "   'year': 2017,\n",
              "   'abstract': 'The atomic simulation environment (ASE) is a software package written in the Python programming language with the aim of setting up, steering, and analyzing atomistic simulations. In ASE, tasks are fully scripted in Python. The powerful syntax of Python combined with the NumPy array library make it ...',\n",
              "   'citationCount': 2790,\n",
              "   'venue': 'Journal of Physics: Condensed Matter',\n",
              "   'url': 'https://www.semanticscholar.org/paper/433d14e40f0d5362df4016270ba97e13371bc42a',\n",
              "   'pdf_url': 'http://wrap.warwick.ac.uk/87141/1/WRAP_Larsen%2Bet%2Bal_2017_J._Phys.%253A_Condens._Matter.pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '81eaedd1aaa095d022c79c38ccf678bf43fafc71',\n",
              "   'title': 'Extensions of the TOPSIS for group decision-making under fuzzy environment',\n",
              "   'authors': ['Chen-Tung Chen'],\n",
              "   'year': 2000,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 3511,\n",
              "   'venue': 'Fuzzy Sets Syst.',\n",
              "   'url': 'https://www.semanticscholar.org/paper/81eaedd1aaa095d022c79c38ccf678bf43fafc71',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'f82e4ff4f003581330338aaae71f60316e58dd26',\n",
              "   'title': 'The Arcade Learning Environment: An Evaluation Platform for General Agents',\n",
              "   'authors': ['Marc G. Bellemare',\n",
              "    'Yavar Naddaf',\n",
              "    'J. Veness',\n",
              "    'Michael Bowling'],\n",
              "   'year': 2012,\n",
              "   'abstract': 'In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interest...',\n",
              "   'citationCount': 3189,\n",
              "   'venue': 'Journal of Artificial Intelligence Research',\n",
              "   'url': 'https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26',\n",
              "   'pdf_url': 'https://jair.org/index.php/jair/article/download/10819/25823',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'd4e4aaba907c3b72b8b42bf81e0a73919a5307ba',\n",
              "   'title': 'The Reorienting System of the Human Brain: From Environment to Theory of Mind',\n",
              "   'authors': ['M. Corbetta', 'G. Patel', 'G. Shulman'],\n",
              "   'year': 2008,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 3749,\n",
              "   'venue': 'Neuron',\n",
              "   'url': 'https://www.semanticscholar.org/paper/d4e4aaba907c3b72b8b42bf81e0a73919a5307ba',\n",
              "   'pdf_url': 'http://www.cell.com/article/S0896627308003693/pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'f106b9efd9f7e336e7968277a630858427ea20b7',\n",
              "   'title': 'The biological control of chemical factors in the environment.',\n",
              "   'authors': ['A. C. Redfield'],\n",
              "   'year': 1960,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 4161,\n",
              "   'venue': 'Science in progress',\n",
              "   'url': 'https://www.semanticscholar.org/paper/f106b9efd9f7e336e7968277a630858427ea20b7',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'abb9c4e284e9eba6e8e9ed0d8ad65159b80beaea',\n",
              "   'title': 'Bacterial biofilms: from the Natural environment to infectious diseases',\n",
              "   'authors': ['Lao Tzu', 'L. Hall-Stoodley', 'J. Costerton', 'P. Stoodley'],\n",
              "   'year': 2004,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 6816,\n",
              "   'venue': 'Nature Reviews Microbiology',\n",
              "   'url': 'https://www.semanticscholar.org/paper/abb9c4e284e9eba6e8e9ed0d8ad65159b80beaea',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'cd0ee7e614f8f4a1c40ea1de4485e2aea6e2b9cb',\n",
              "   'title': 'Organizational Structure, Environment and Performance: The Role of Strategic Choice',\n",
              "   'authors': ['J. Child'],\n",
              "   'year': 1972,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 5075,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/cd0ee7e614f8f4a1c40ea1de4485e2aea6e2b9cb',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'ad497d6aa1dfcae07f4f441ce26c0b0af9cb14dd',\n",
              "   'title': 'Environment dominates over host genetics in shaping human gut microbiota',\n",
              "   'authors': ['Daphna Rothschild',\n",
              "    'O. Weissbrod',\n",
              "    'Elad Barkan',\n",
              "    'A. Kurilshikov',\n",
              "    'Tal Korem',\n",
              "    'D. Zeevi',\n",
              "    'P. Costea',\n",
              "    'A. Godneva',\n",
              "    'I. Kalka',\n",
              "    'N. Bar',\n",
              "    'S. Shilo',\n",
              "    'Dar Lador',\n",
              "    'A. V. Vila',\n",
              "    'N. Zmora',\n",
              "    'M. Pevsner-Fischer',\n",
              "    'D. Israeli',\n",
              "    'Noa Kosower',\n",
              "    'Gal Malka',\n",
              "    'B. Wolf',\n",
              "    'T. Avnit-Sagi',\n",
              "    'M. Lotan-Pompan',\n",
              "    'A. Weinberger',\n",
              "    'Z. Halpern',\n",
              "    'S. Carmi',\n",
              "    'Jingyuan Fu',\n",
              "    'C. Wijmenga',\n",
              "    'A. Zhernakova',\n",
              "    'E. Elinav',\n",
              "    'E. Segal'],\n",
              "   'year': 2018,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 2261,\n",
              "   'venue': 'Nature',\n",
              "   'url': 'https://www.semanticscholar.org/paper/ad497d6aa1dfcae07f4f441ce26c0b0af9cb14dd',\n",
              "   'pdf_url': 'https://pure.rug.nl/ws/files/56021462/Environment_dominates_over_host_genetics_in_shaping_human_gut_microbiota.pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'abf935fd95d15016d4cd93661a14a7ff25e8a73d',\n",
              "   'title': 'COVID-19 outbreak: Migration, effects on society, global environment and prevention',\n",
              "   'authors': ['I. Chakraborty', 'Prasenjit Maity'],\n",
              "   'year': 2020,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 1502,\n",
              "   'venue': 'Science of the Total Environment',\n",
              "   'url': 'https://www.semanticscholar.org/paper/abf935fd95d15016d4cd93661a14a7ff25e8a73d',\n",
              "   'pdf_url': 'https://europepmc.org/articles/pmc7175860?pdf=render',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'b7443e0dacfc28cada33bf64c217c3472ef371c4',\n",
              "   'title': 'Macrobenthic succession in relation to organic enrichment and pollution of the marine environment',\n",
              "   'authors': ['T. Pearson', 'R. Rosenberg'],\n",
              "   'year': 1978,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 3888,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/b7443e0dacfc28cada33bf64c217c3472ef371c4',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '52bcd412ec1a3e2eb2e18f5d72ba3d9684d13d88',\n",
              "   'title': 'Economic Growth and the Environment',\n",
              "   'authors': ['G. Grossman', 'A. Krueger'],\n",
              "   'year': 1994,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 6123,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/52bcd412ec1a3e2eb2e18f5d72ba3d9684d13d88',\n",
              "   'pdf_url': 'https://doi.org/10.3386/w4634',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '35a4a4f05694f2180995fb4d4f5eebf13fa396b7',\n",
              "   'title': 'The Optical Properties of Metal Nanoparticles: The Influence of Size, Shape, and Dielectric Environment',\n",
              "   'authors': ['G. Schatz',\n",
              "    'K. Kelly',\n",
              "    'E. Coronado',\n",
              "    'L. Zhao',\n",
              "    'D. Beljonne',\n",
              "    'C. Curutchet',\n",
              "    'G. Scholes',\n",
              "    'B. Dabbousi',\n",
              "    'J. Rodr√≠guez-Viejo',\n",
              "    'F. Mikulec',\n",
              "    'C. Murphy',\n",
              "    'T. Sau',\n",
              "    'A. Gole',\n",
              "    'C. Orendorff',\n",
              "    'Jinxin Gao',\n",
              "    'L. Gou',\n",
              "    'S. E. Hunyadi'],\n",
              "   'year': 2003,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 9531,\n",
              "   'venue': '',\n",
              "   'url': 'https://www.semanticscholar.org/paper/35a4a4f05694f2180995fb4d4f5eebf13fa396b7',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': 'd07a757cd1a360f2d0fbd99061dd9346fac7948e',\n",
              "   'title': 'Microplastics as contaminants in the marine environment: a review.',\n",
              "   'authors': ['M. Cole', 'P. Lindeque', 'C. Halsband', 'T. Galloway'],\n",
              "   'year': 2011,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 4848,\n",
              "   'venue': 'Marine Pollution Bulletin',\n",
              "   'url': 'https://www.semanticscholar.org/paper/d07a757cd1a360f2d0fbd99061dd9346fac7948e',\n",
              "   'pdf_url': 'https://doi.org/10.1016/j.marpolbul.2011.09.025',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '23a94ce42fe0d50f5c993f34d4c9602f8aeac507',\n",
              "   'title': 'Rational choice and the structure of the environment.',\n",
              "   'authors': ['H. Simon'],\n",
              "   'year': 1956,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 5365,\n",
              "   'venue': 'Psychology Review',\n",
              "   'url': 'https://www.semanticscholar.org/paper/23a94ce42fe0d50f5c993f34d4c9602f8aeac507',\n",
              "   'pdf_url': 'http://www.uk.sagepub.com/upm-data/25239_Chater~Vol_1~Ch_03.pdf',\n",
              "   'has_pdf': True},\n",
              "  {'paperId': '30978748fd810cd5b0f72eaf0ffa3bef468f8cf3',\n",
              "   'title': 'The NEURON Simulation Environment',\n",
              "   'authors': ['M. Hines', 'N. Carnevale'],\n",
              "   'year': 1997,\n",
              "   'abstract': 'No abstract available',\n",
              "   'citationCount': 2816,\n",
              "   'venue': 'Neural Computation',\n",
              "   'url': 'https://www.semanticscholar.org/paper/30978748fd810cd5b0f72eaf0ffa3bef468f8cf3',\n",
              "   'pdf_url': '',\n",
              "   'has_pdf': True}]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# MODULE 2: Paper Selection & PDF Download\n",
        "# (FIXED & ROBUST VERSION)\n",
        "# ============================================\n",
        "\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "\n",
        "SEARCH_DIR = \"data/search_results\"\n",
        "DOWNLOAD_DIR = \"downloads\"\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. LOAD SEARCH RESULTS (SAFE VERSION)\n",
        "# ------------------------------------------------------------\n",
        "def load_search_results():\n",
        "    if not os.path.exists(SEARCH_DIR):\n",
        "        raise FileNotFoundError(\n",
        "            \"Search results folder not found. Run Module 1 first.\"\n",
        "        )\n",
        "\n",
        "    files = [f for f in os.listdir(SEARCH_DIR) if f.endswith(\".json\")]\n",
        "\n",
        "    if not files:\n",
        "        raise FileNotFoundError(\n",
        "            \"No search result JSON found. Run Module 1 first.\"\n",
        "        )\n",
        "\n",
        "    # Pick latest file automatically\n",
        "    files.sort(key=lambda f: os.path.getmtime(os.path.join(SEARCH_DIR, f)), reverse=True)\n",
        "    filepath = os.path.join(SEARCH_DIR, files[0])\n",
        "\n",
        "    print(f\"üìÇ Loading search results from: {filepath}\")\n",
        "\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    return data\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. FILTER PAPERS WITH PDFs\n",
        "# ------------------------------------------------------------\n",
        "def filter_papers_with_pdf(papers):\n",
        "    pdf_papers = []\n",
        "    for p in papers:\n",
        "        if p.get(\"pdf_url\"):\n",
        "            pdf_papers.append(p)\n",
        "    print(f\"üìÑ Papers with PDF: {len(pdf_papers)}\")\n",
        "    return pdf_papers\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. DOWNLOAD PDFS\n",
        "# ------------------------------------------------------------\n",
        "def download_pdfs(limit=3):\n",
        "    data = load_search_results()\n",
        "    papers = filter_papers_with_pdf(data[\"papers\"])\n",
        "\n",
        "    downloaded = 0\n",
        "\n",
        "    for i, paper in enumerate(papers[:limit]):\n",
        "        pdf_url = paper[\"pdf_url\"]\n",
        "        title = paper[\"title\"][:50].replace(\" \", \"_\")\n",
        "\n",
        "        filename = f\"{DOWNLOAD_DIR}/{i+1}_{title}.pdf\"\n",
        "\n",
        "        try:\n",
        "            print(f\"\\n‚¨á Downloading: {paper['title'][:60]}...\")\n",
        "            r = requests.get(pdf_url, timeout=30)\n",
        "\n",
        "            if r.status_code == 200 and r.content[:4] == b\"%PDF\":\n",
        "                with open(filename, \"wb\") as f:\n",
        "                    f.write(r.content)\n",
        "                downloaded += 1\n",
        "                print(f\"   ‚úÖ Saved as {filename}\")\n",
        "            else:\n",
        "                print(\"   ‚ùå Not a valid PDF\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"   ‚ùå Download failed:\", e)\n",
        "\n",
        "    print(f\"\\nüìä Total PDFs downloaded: {downloaded}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# RUN MODULE 2\n",
        "# ------------------------------------------------------------\n",
        "download_pdfs(limit=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYVPe4Zc4qBF",
        "outputId": "f1424d18-df2b-472c-93a7-86b1eb11e3c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Loading search results from: data/search_results/environment.json\n",
            "üìÑ Papers with PDF: 27\n",
            "\n",
            "‚¨á Downloading: ape 5.0: an environment for modern phylogenetics and evoluti...\n",
            "   ‚ùå Not a valid PDF\n",
            "\n",
            "‚¨á Downloading: Cytoscape: a software environment for integrated models of b...\n",
            "   ‚úÖ Saved as downloads/2_Cytoscape:_a_software_environment_for_integrated_m.pdf\n",
            "\n",
            "‚¨á Downloading: WebArena: A Realistic Web Environment for Building Autonomou...\n",
            "   ‚úÖ Saved as downloads/3_WebArena:_A_Realistic_Web_Environment_for_Building.pdf\n",
            "\n",
            "üìä Total PDFs downloaded: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MILESTONE 2 (WEEK 3‚Äì4)\n",
        "# MODULE 3 + MODULE 4\n",
        "# PDF TEXT EXTRACTION, ANALYSIS & CROSS-PAPER COMPARISON\n",
        "# ============================================================\n",
        "\n",
        "\"\"\"\n",
        "WHAT THIS CODE DOES (HIGH-LEVEL):\n",
        "\n",
        "1. Loads research paper PDFs downloaded in Module 2\n",
        "2. Extracts text from each PDF using robust strategies\n",
        "3. Cleans noisy PDF text\n",
        "4. Extracts academic sections:\n",
        "   - Title\n",
        "   - Abstract\n",
        "   - Introduction\n",
        "   - Methods\n",
        "   - Results\n",
        "   - Conclusion\n",
        "5. Stores extracted content in structured JSON format\n",
        "6. Extracts key findings from Results & Conclusion\n",
        "7. Validates correctness and completeness\n",
        "8. Compares findings across all papers\n",
        "9. Generates summary + comparison reports\n",
        "\n",
        "THIS DIRECTLY SATISFIES:\n",
        "‚úî Text extraction module\n",
        "‚úî Section-wise parsing\n",
        "‚úî Structured storage\n",
        "‚úî Key finding extraction\n",
        "‚úî Cross-paper comparison\n",
        "‚úî Validation of correctness\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# INSTALL REQUIRED LIBRARIES\n",
        "# ------------------------------------------------------------\n",
        "!pip install pymupdf pymupdf4llm tqdm -q\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import fitz                     # PyMuPDF\n",
        "import pymupdf4llm              # Layout-aware extraction\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ------------------------------------------------------------\n",
        "DOWNLOAD_DIR = \"downloads\"          # PDFs from Module 2\n",
        "OUTPUT_DIR = \"data/extracted\"       # Structured output\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"üìÅ Using PDF directory:\", DOWNLOAD_DIR)\n",
        "print(\"üìÅ Extracted data stored in:\", OUTPUT_DIR)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. BASIC TEXT CLEANING (ENHANCEMENT)\n",
        "# ------------------------------------------------------------\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans raw PDF text:\n",
        "    - Removes excessive spaces\n",
        "    - Fixes broken hyphenated words\n",
        "    - Removes unreadable characters\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'-\\s+', '', text)   # Fix line-break hyphens\n",
        "    text = ''.join(c for c in text if ord(c) >= 32)\n",
        "    return text.strip()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. PDF TEXT EXTRACTION\n",
        "# ------------------------------------------------------------\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text using TWO strategies and select best output:\n",
        "    1) pymupdf4llm (layout-aware, better for research papers)\n",
        "    2) Standard PyMuPDF extraction (fallback)\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nüìÑ Extracting text from: {pdf_path.name}\")\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        extracted_versions = []\n",
        "\n",
        "        # Strategy 1: Layout-aware extraction\n",
        "        try:\n",
        "            md_text = pymupdf4llm.to_markdown(str(pdf_path))\n",
        "            if md_text and len(md_text) > 1000:\n",
        "                extracted_versions.append(md_text)\n",
        "                print(\"   ‚úî Used layout-aware extraction\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Strategy 2: Regular extraction (fallback)\n",
        "        raw_text = \"\"\n",
        "        for page in doc[:min(40, len(doc))]:\n",
        "            raw_text += page.get_text()\n",
        "\n",
        "        if len(raw_text) > 1000:\n",
        "            extracted_versions.append(raw_text)\n",
        "            print(\"   ‚úî Used standard text extraction\")\n",
        "\n",
        "        doc.close()\n",
        "\n",
        "        if not extracted_versions:\n",
        "            print(\"   ‚ùå No usable text found\")\n",
        "            return None\n",
        "\n",
        "        # Select longest version (most content)\n",
        "        best_text = max(extracted_versions, key=len)\n",
        "        return clean_text(best_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Extraction failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. SECTION-WISE TEXT EXTRACTION\n",
        "# ------------------------------------------------------------\n",
        "def extract_sections(text):\n",
        "    \"\"\"\n",
        "    Extracts standard research paper sections using regex patterns\n",
        "    \"\"\"\n",
        "\n",
        "    sections = {\n",
        "        \"title\": \"\",\n",
        "        \"abstract\": \"\",\n",
        "        \"introduction\": \"\",\n",
        "        \"methods\": \"\",\n",
        "        \"results\": \"\",\n",
        "        \"conclusion\": \"\"\n",
        "    }\n",
        "\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    patterns = {\n",
        "        \"abstract\": r\"\\babstract\\b\",\n",
        "        \"introduction\": r\"\\bintroduction\\b\",\n",
        "        \"methods\": r\"\\b(methods?|methodology|experiment)\\b\",\n",
        "        \"results\": r\"\\b(results?|findings)\\b\",\n",
        "        \"conclusion\": r\"\\b(conclusion|discussion)\\b\"\n",
        "    }\n",
        "\n",
        "    boundaries = {}\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                boundaries.setdefault(section, i)\n",
        "\n",
        "    sorted_sections = sorted(boundaries.items(), key=lambda x: x[1])\n",
        "\n",
        "    for idx, (section, start) in enumerate(sorted_sections):\n",
        "        end = sorted_sections[idx + 1][1] if idx + 1 < len(sorted_sections) else len(lines)\n",
        "        content = \" \".join(lines[start:end]).strip()\n",
        "        if len(content) > 200:\n",
        "            sections[section] = content[:5000]\n",
        "\n",
        "    # Title extraction (first meaningful line)\n",
        "    for line in lines[:10]:\n",
        "        if 20 < len(line) < 200:\n",
        "            sections[\"title\"] = line\n",
        "            break\n",
        "\n",
        "    return sections\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. KEY FINDINGS EXTRACTION (MODULE 4)\n",
        "# ------------------------------------------------------------\n",
        "def extract_key_findings(sections, max_findings=5):\n",
        "    \"\"\"\n",
        "    Extracts important conclusions from Results + Conclusion\n",
        "    \"\"\"\n",
        "\n",
        "    keywords = [\n",
        "        \"we found\", \"results show\", \"our results\",\n",
        "        \"significant\", \"improves\", \"outperforms\",\n",
        "        \"demonstrates\", \"indicates\", \"we conclude\"\n",
        "    ]\n",
        "\n",
        "    findings = []\n",
        "    combined = sections.get(\"results\", \"\") + \" \" + sections.get(\"conclusion\", \"\")\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', combined)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(k in sentence.lower() for k in keywords):\n",
        "            findings.append(sentence.strip())\n",
        "        if len(findings) >= max_findings:\n",
        "            break\n",
        "\n",
        "    return findings\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. VALIDATION OF CORRECTNESS\n",
        "# ------------------------------------------------------------\n",
        "def validate_extraction(paper):\n",
        "    \"\"\"\n",
        "    Ensures extracted data is complete and meaningful\n",
        "    \"\"\"\n",
        "\n",
        "    return {\n",
        "        \"has_abstract\": len(paper[\"sections\"].get(\"abstract\", \"\")) > 200,\n",
        "        \"has_multiple_sections\": sum(len(v) > 200 for v in paper[\"sections\"].values()) >= 2,\n",
        "        \"has_key_findings\": len(paper[\"key_findings\"]) > 0,\n",
        "        \"sufficient_text\": paper[\"stats\"][\"total_chars\"] > 1000\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6. PROCESS A SINGLE PDF (END-TO-END)\n",
        "# ------------------------------------------------------------\n",
        "def process_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Complete processing of ONE research paper\n",
        "    \"\"\"\n",
        "\n",
        "    raw_text = extract_text_from_pdf(pdf_path)\n",
        "    if not raw_text:\n",
        "        return None\n",
        "\n",
        "    sections = extract_sections(raw_text)\n",
        "    key_findings = extract_key_findings(sections)\n",
        "\n",
        "    paper = {\n",
        "        \"paper_id\": pdf_path.stem,\n",
        "        \"filename\": pdf_path.name,\n",
        "        \"sections\": sections,\n",
        "        \"key_findings\": key_findings,\n",
        "        \"stats\": {\n",
        "            \"total_chars\": len(raw_text),\n",
        "            \"sections_found\": sum(len(v) > 200 for v in sections.values())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    paper[\"validation\"] = validate_extraction(paper)\n",
        "    return paper\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7. CROSS-PAPER COMPARISON\n",
        "# ------------------------------------------------------------\n",
        "def compare_papers(papers):\n",
        "    \"\"\"\n",
        "    Finds common themes across all papers\n",
        "    \"\"\"\n",
        "\n",
        "    all_findings = []\n",
        "    for paper in papers:\n",
        "        all_findings.extend(paper[\"key_findings\"])\n",
        "\n",
        "    words = []\n",
        "    for finding in all_findings:\n",
        "        words.extend(re.findall(r'\\b[a-zA-Z]{4,}\\b', finding.lower()))\n",
        "\n",
        "    common_terms = Counter(words).most_common(10)\n",
        "\n",
        "    return {\n",
        "        \"total_papers\": len(papers),\n",
        "        \"total_findings\": len(all_findings),\n",
        "        \"common_terms\": common_terms\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8. RUN COMPLETE PIPELINE\n",
        "# ------------------------------------------------------------\n",
        "def run_milestone_2():\n",
        "    \"\"\"\n",
        "    Runs complete Milestone-2 pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    pdf_files = list(Path(DOWNLOAD_DIR).glob(\"*.pdf\"))\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"MILESTONE 2 PIPELINE STARTED\")\n",
        "    print(\"==============================\")\n",
        "    print(f\"üìÑ PDFs found: {len(pdf_files)}\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"‚ùå No PDFs found. Run Module 2 first.\")\n",
        "        return\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for pdf in tqdm(pdf_files):\n",
        "        paper = process_pdf(pdf)\n",
        "        if paper:\n",
        "            results.append(paper)\n",
        "            with open(f\"{OUTPUT_DIR}/{paper['paper_id']}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(paper, f, indent=2)\n",
        "\n",
        "    comparison = compare_papers(results)\n",
        "\n",
        "    with open(f\"{OUTPUT_DIR}/comparison_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(comparison, f, indent=2)\n",
        "\n",
        "    print(\"\\n‚úÖ MILESTONE 2 COMPLETED SUCCESSFULLY\")\n",
        "    print(f\"üìä Papers processed: {len(results)}\")\n",
        "    print(\"üìÅ Output stored in:\", OUTPUT_DIR)\n",
        "\n",
        "    return results, comparison\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# RUN\n",
        "# ------------------------------------------------------------\n",
        "results, comparison = run_milestone_2()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTgrSoyb4yqt",
        "outputId": "539db8b1-4d3e-4c29-8010-86f2b13f6685"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/24.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/24.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.7/24.1 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.2/24.1 MB\u001b[0m \u001b[31m217.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m23.7/24.1 MB\u001b[0m \u001b[31m234.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m228.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/68.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hConsider using the pymupdf_layout package for a greatly improved page layout analysis.\n",
            "üìÅ Using PDF directory: downloads\n",
            "üìÅ Extracted data stored in: data/extracted\n",
            "\n",
            "==============================\n",
            "MILESTONE 2 PIPELINE STARTED\n",
            "==============================\n",
            "üìÑ PDFs found: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÑ Extracting text from: 2_Cytoscape:_a_software_environment_for_integrated_m.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  3.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úî Used layout-aware extraction\n",
            "   ‚úî Used standard text extraction\n",
            "\n",
            "üìÑ Extracting text from: 3_WebArena:_A_Realistic_Web_Environment_for_Building.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:16<00:00,  8.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úî Used layout-aware extraction\n",
            "   ‚úî Used standard text extraction\n",
            "\n",
            "‚úÖ MILESTONE 2 COMPLETED SUCCESSFULLY\n",
            "üìä Papers processed: 2\n",
            "üìÅ Output stored in: data/extracted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MILESTONE 2 (WEEK 3‚Äì4)\n",
        "# PDF TEXT EXTRACTION, SECTION ANALYSIS & CROSS-PAPER COMPARISON\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 0. INSTALL REQUIRED LIBRARIES\n",
        "# ------------------------------------------------------------\n",
        "# PyMuPDF: robust PDF reading\n",
        "# pymupdf4llm: layout-aware extraction (works well for structured research PDFs)\n",
        "# tqdm: progress bar for loops\n",
        "!pip install pymupdf pymupdf4llm tqdm -q\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. IMPORT MODULES\n",
        "# ------------------------------------------------------------\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import fitz                # PyMuPDF\n",
        "import pymupdf4llm         # Layout-aware text extraction\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. CONFIGURATION\n",
        "# ------------------------------------------------------------\n",
        "# Directory where PDFs are stored (downloaded in Module 2)\n",
        "DOWNLOAD_DIR = \"downloads\"\n",
        "\n",
        "# Directory to save structured JSON outputs\n",
        "OUTPUT_DIR = \"data/extracted\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"üìÅ Using PDF directory:\", DOWNLOAD_DIR)\n",
        "print(\"üìÅ Extracted data stored in:\", OUTPUT_DIR)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. TEXT CLEANING FUNCTION\n",
        "# ------------------------------------------------------------\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans the raw PDF text.\n",
        "\n",
        "    Steps:\n",
        "    1. Remove multiple consecutive spaces.\n",
        "    2. Fix line-break hyphenation (e.g., \"ex-\\nample\" -> \"example\").\n",
        "    3. Remove non-printable/unreadable characters.\n",
        "\n",
        "    Returns:\n",
        "        Cleaned string.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Replace multiple spaces with single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove hyphenation at line breaks\n",
        "    text = re.sub(r'-\\s+', '', text)\n",
        "\n",
        "    # Remove unreadable characters\n",
        "    text = ''.join(c for c in text if ord(c) >= 32)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. PDF TEXT EXTRACTION FUNCTION\n",
        "# ------------------------------------------------------------\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text from PDF using TWO strategies:\n",
        "\n",
        "    Strategy 1: Layout-aware extraction (pymupdf4llm) ‚Üí better for academic PDFs.\n",
        "    Strategy 2: Standard PyMuPDF extraction ‚Üí fallback if layout-aware fails.\n",
        "\n",
        "    Returns:\n",
        "        Best cleaned text (string) or None if extraction fails.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìÑ Extracting text from: {pdf_path.name}\")\n",
        "\n",
        "    try:\n",
        "        # Open PDF with PyMuPDF\n",
        "        doc = fitz.open(pdf_path)\n",
        "        extracted_versions = []\n",
        "\n",
        "        # --- Strategy 1: Layout-aware extraction ---\n",
        "        try:\n",
        "            layout_text = pymupdf4llm.to_markdown(str(pdf_path))\n",
        "            if layout_text and len(layout_text) > 1000:  # Only accept meaningful content\n",
        "                extracted_versions.append(layout_text)\n",
        "                print(\"   ‚úî Used layout-aware extraction\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö† Layout-aware extraction failed: {e}\")\n",
        "\n",
        "        # --- Strategy 2: Standard PyMuPDF extraction (fallback) ---\n",
        "        raw_text = \"\"\n",
        "        for page in doc[:min(40, len(doc))]:  # Limit to first 40 pages for speed\n",
        "            raw_text += page.get_text()\n",
        "        if len(raw_text) > 1000:\n",
        "            extracted_versions.append(raw_text)\n",
        "            print(\"   ‚úî Used standard text extraction\")\n",
        "\n",
        "        doc.close()\n",
        "\n",
        "        if not extracted_versions:\n",
        "            print(\"   ‚ùå No usable text found\")\n",
        "            return None\n",
        "\n",
        "        # Select the longest text (most complete)\n",
        "        best_text = max(extracted_versions, key=len)\n",
        "        return clean_text(best_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Extraction failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. SECTION-WISE EXTRACTION FUNCTION\n",
        "# ------------------------------------------------------------\n",
        "def extract_sections(text):\n",
        "    \"\"\"\n",
        "    Extract standard research paper sections using regex.\n",
        "\n",
        "    Sections:\n",
        "        - Title: first meaningful line\n",
        "        - Abstract\n",
        "        - Introduction\n",
        "        - Methods/Methodology/Experiment\n",
        "        - Results/Findings\n",
        "        - Conclusion/Discussion\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of sections with content.\n",
        "    \"\"\"\n",
        "    sections = {\n",
        "        \"title\": \"\",\n",
        "        \"abstract\": \"\",\n",
        "        \"introduction\": \"\",\n",
        "        \"methods\": \"\",\n",
        "        \"results\": \"\",\n",
        "        \"conclusion\": \"\"\n",
        "    }\n",
        "\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    # Regex patterns for section headings\n",
        "    patterns = {\n",
        "        \"abstract\": r\"\\babstract\\b\",\n",
        "        \"introduction\": r\"\\bintroduction\\b\",\n",
        "        \"methods\": r\"\\b(methods?|methodology|experiment)\\b\",\n",
        "        \"results\": r\"\\b(results?|findings)\\b\",\n",
        "        \"conclusion\": r\"\\b(conclusion|discussion)\\b\"\n",
        "    }\n",
        "\n",
        "    boundaries = {}  # Stores line indices of section starts\n",
        "\n",
        "    # Detect section start lines\n",
        "    for i, line in enumerate(lines):\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(pattern, line.lower()):\n",
        "                boundaries.setdefault(section, i)\n",
        "\n",
        "    # Sort sections by start line\n",
        "    sorted_sections = sorted(boundaries.items(), key=lambda x: x[1])\n",
        "\n",
        "    # Extract section content based on line boundaries\n",
        "    for idx, (section, start) in enumerate(sorted_sections):\n",
        "        end = sorted_sections[idx + 1][1] if idx + 1 < len(sorted_sections) else len(lines)\n",
        "        content = \" \".join(lines[start:end]).strip()\n",
        "        if len(content) > 200:  # Only keep meaningful content\n",
        "            sections[section] = content[:5000]  # Limit size for JSON\n",
        "\n",
        "    # Title: first meaningful line in first 10 lines\n",
        "    for line in lines[:10]:\n",
        "        if 20 < len(line) < 200:\n",
        "            sections[\"title\"] = line.strip()\n",
        "            break\n",
        "\n",
        "    return sections\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6. KEY FINDINGS EXTRACTION FUNCTION\n",
        "# ------------------------------------------------------------\n",
        "def extract_key_findings(sections, max_findings=5):\n",
        "    \"\"\"\n",
        "    Extracts important results/conclusions from Results + Conclusion sections.\n",
        "\n",
        "    Looks for sentences containing keywords like:\n",
        "        - we found, results show, significant, improves, outperforms, indicates\n",
        "\n",
        "    Returns:\n",
        "        List of key sentences (max `max_findings`)\n",
        "    \"\"\"\n",
        "    keywords = [\n",
        "        \"we found\", \"results show\", \"our results\",\n",
        "        \"significant\", \"improves\", \"outperforms\",\n",
        "        \"demonstrates\", \"indicates\", \"we conclude\"\n",
        "    ]\n",
        "\n",
        "    findings = []\n",
        "    combined_text = sections.get(\"results\", \"\") + \" \" + sections.get(\"conclusion\", \"\")\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', combined_text)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(k in sentence.lower() for k in keywords):\n",
        "            findings.append(sentence.strip())\n",
        "        if len(findings) >= max_findings:\n",
        "            break\n",
        "\n",
        "    return findings\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7. VALIDATION FUNCTION\n",
        "# ------------------------------------------------------------\n",
        "def validate_extraction(paper):\n",
        "    \"\"\"\n",
        "    Checks whether extracted data is complete and meaningful.\n",
        "\n",
        "    Criteria:\n",
        "        - Abstract exists (>200 chars)\n",
        "        - At least 2 sections exist\n",
        "        - Key findings exist\n",
        "        - Total text is sufficiently long (>1000 chars)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of validation flags\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"has_abstract\": len(paper[\"sections\"].get(\"abstract\", \"\")) > 200,\n",
        "        \"has_multiple_sections\": sum(len(v) > 200 for v in paper[\"sections\"].values()) >= 2,\n",
        "        \"has_key_findings\": len(paper[\"key_findings\"]) > 0,\n",
        "        \"sufficient_text\": paper[\"stats\"][\"total_chars\"] > 1000\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8. PROCESS SINGLE PDF\n",
        "# ------------------------------------------------------------\n",
        "def process_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    End-to-end processing of one research paper:\n",
        "        1. Extract text\n",
        "        2. Extract sections\n",
        "        3. Extract key findings\n",
        "        4. Collect stats\n",
        "        5. Validate\n",
        "    \"\"\"\n",
        "    raw_text = extract_text_from_pdf(pdf_path)\n",
        "    if not raw_text:\n",
        "        return None\n",
        "\n",
        "    sections = extract_sections(raw_text)\n",
        "    key_findings = extract_key_findings(sections)\n",
        "\n",
        "    paper = {\n",
        "        \"paper_id\": pdf_path.stem,\n",
        "        \"filename\": pdf_path.name,\n",
        "        \"sections\": sections,\n",
        "        \"key_findings\": key_findings,\n",
        "        \"stats\": {\n",
        "            \"total_chars\": len(raw_text),\n",
        "            \"sections_found\": sum(len(v) > 200 for v in sections.values())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    paper[\"validation\"] = validate_extraction(paper)\n",
        "    return paper\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 9. CROSS-PAPER COMPARISON\n",
        "# ------------------------------------------------------------\n",
        "def compare_papers(papers):\n",
        "    \"\"\"\n",
        "    Finds common themes across multiple papers:\n",
        "        - Combines all key findings\n",
        "        - Extracts most frequent words (>3 letters)\n",
        "\n",
        "    Returns:\n",
        "        Summary dictionary with total papers, total findings, and top 10 common terms\n",
        "    \"\"\"\n",
        "    all_findings = []\n",
        "    for paper in papers:\n",
        "        all_findings.extend(paper[\"key_findings\"])\n",
        "\n",
        "    words = []\n",
        "    for finding in all_findings:\n",
        "        words.extend(re.findall(r'\\b[a-zA-Z]{4,}\\b', finding.lower()))\n",
        "\n",
        "    common_terms = Counter(words).most_common(10)\n",
        "\n",
        "    return {\n",
        "        \"total_papers\": len(papers),\n",
        "        \"total_findings\": len(all_findings),\n",
        "        \"common_terms\": common_terms\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 10. RUN COMPLETE PIPELINE\n",
        "# ------------------------------------------------------------\n",
        "def run_milestone_2():\n",
        "    \"\"\"\n",
        "    Runs the complete Milestone 2 pipeline:\n",
        "        1. Process all PDFs\n",
        "        2. Save JSONs for each paper\n",
        "        3. Generate comparison report\n",
        "    \"\"\"\n",
        "    pdf_files = list(Path(DOWNLOAD_DIR).glob(\"*.pdf\"))\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"MILESTONE 2 PIPELINE STARTED\")\n",
        "    print(\"==============================\")\n",
        "    print(f\"üìÑ PDFs found: {len(pdf_files)}\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"‚ùå No PDFs found. Run Module 2 first.\")\n",
        "        return\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for pdf in tqdm(pdf_files):\n",
        "        paper = process_pdf(pdf)\n",
        "        if paper:\n",
        "            results.append(paper)\n",
        "            # Save structured JSON for each paper\n",
        "            with open(f\"{OUTPUT_DIR}/{paper['paper_id']}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(paper, f, indent=2)\n",
        "\n",
        "    # Cross-paper comparison\n",
        "    comparison = compare_papers(results)\n",
        "    with open(f\"{OUTPUT_DIR}/comparison_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(comparison, f, indent=2)\n",
        "\n",
        "    print(\"\\n‚úÖ MILESTONE 2 COMPLETED SUCCESSFULLY\")\n",
        "    print(f\"üìä Papers processed: {len(results)}\")\n",
        "    print(\"üìÅ Output stored in:\", OUTPUT_DIR)\n",
        "\n",
        "    return results, comparison\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 11. EXECUTE PIPELINE\n",
        "# ------------------------------------------------------------\n",
        "results, comparison = run_milestone_2()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJvsep_n56VN",
        "outputId": "ebd06d76-c22d-4ddd-9d84-a4b0b01d62a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Using PDF directory: downloads\n",
            "üìÅ Extracted data stored in: data/extracted\n",
            "\n",
            "==============================\n",
            "MILESTONE 2 PIPELINE STARTED\n",
            "==============================\n",
            "üìÑ PDFs found: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÑ Extracting text from: 2_Cytoscape:_a_software_environment_for_integrated_m.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03<00:03,  3.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úî Used layout-aware extraction\n",
            "   ‚úî Used standard text extraction\n",
            "\n",
            "üìÑ Extracting text from: 3_WebArena:_A_Realistic_Web_Environment_for_Building.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:16<00:00,  8.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úî Used layout-aware extraction\n",
            "   ‚úî Used standard text extraction\n",
            "\n",
            "‚úÖ MILESTONE 2 COMPLETED SUCCESSFULLY\n",
            "üìä Papers processed: 2\n",
            "üìÅ Output stored in: data/extracted\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "=========================================================\n",
        "MILESTONE 3\n",
        "AI-BASED SUMMARIZATION, ANALYSIS & LITERATURE REVIEW\n",
        "=========================================================\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# --------------------------------------------------\n",
        "# IMPORTS\n",
        "# --------------------------------------------------\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# --------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# --------------------------------------------------\n",
        "EXTRACTED_DIR = \"data/extracted\"\n",
        "OUTPUT_DIR = \"data/milestone_3\"\n",
        "\n",
        "SUMMARY_FILE = f\"{OUTPUT_DIR}/paper_summaries.json\"\n",
        "ANALYSIS_FILE = f\"{OUTPUT_DIR}/paper_analysis.json\"\n",
        "FINAL_REVIEW_FILE = f\"{OUTPUT_DIR}/final_literature_review.txt\"\n",
        "\n",
        "MAX_SECTION_CHARS = 3500\n",
        "\n",
        "# --------------------------------------------------\n",
        "# INITIALIZATION (COLAB SECRETS SAFE)\n",
        "# --------------------------------------------------\n",
        "def initialize_environment():\n",
        "    api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "    if not api_key:\n",
        "        raise EnvironmentError(\n",
        "            \"OPENAI_API_KEY not found in Colab Secrets\"\n",
        "        )\n",
        "\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    print(\"‚úÖ OpenAI API key loaded securely from Colab Secrets\")\n",
        "    return OpenAI()\n",
        "\n",
        "client = initialize_environment()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# LLM CALL WRAPPER\n",
        "# --------------------------------------------------\n",
        "def call_llm(prompt: str, temperature: float = 0.3) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temperature\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# SECTION-WISE SUMMARIZATION\n",
        "# --------------------------------------------------\n",
        "def summarize_section(section_name: str, text: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "    You are an academic research assistant.\n",
        "\n",
        "    Summarize the {section_name} section of a research paper.\n",
        "    Focus on:\n",
        "    ‚Ä¢ Research problem\n",
        "    ‚Ä¢ Methodology\n",
        "    ‚Ä¢ Key results\n",
        "    ‚Ä¢ Technical contribution\n",
        "\n",
        "    Maintain formal academic tone.\n",
        "    Limit to 120 words.\n",
        "\n",
        "    TEXT:\n",
        "    {text[:MAX_SECTION_CHARS]}\n",
        "    \"\"\"\n",
        "    return call_llm(prompt, temperature=0.25)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# PAPER-LEVEL SUMMARIZATION\n",
        "# --------------------------------------------------\n",
        "def summarize_paper(sections: Dict[str, str]) -> Dict[str, str]:\n",
        "    summaries = {}\n",
        "\n",
        "    for section, content in sections.items():\n",
        "        if content and len(content.split()) > 80:\n",
        "            summaries[section] = summarize_section(section, content)\n",
        "        else:\n",
        "            summaries[section] = \"Section content insufficient for summarization.\"\n",
        "\n",
        "    return summaries\n",
        "\n",
        "# --------------------------------------------------\n",
        "# CRITICAL ANALYSIS (KEY CONTRIBUTION MODULE)\n",
        "# --------------------------------------------------\n",
        "def analyze_paper(summary: Dict[str, str]) -> str:\n",
        "    prompt = f\"\"\"\n",
        "    Perform a critical analysis of the following paper summary.\n",
        "\n",
        "    Identify:\n",
        "    1. Key Contributions\n",
        "    2. Strengths\n",
        "    3. Limitations\n",
        "    4. Research or practical implications\n",
        "\n",
        "    Provide concise bullet points.\n",
        "\n",
        "    SUMMARY:\n",
        "    {json.dumps(summary, indent=2)}\n",
        "    \"\"\"\n",
        "    return call_llm(prompt, temperature=0.35)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# PROCESS ALL PAPERS\n",
        "# --------------------------------------------------\n",
        "def process_all_papers():\n",
        "    extracted_files = list(Path(EXTRACTED_DIR).glob(\"*.json\"))\n",
        "\n",
        "    if not extracted_files:\n",
        "        raise FileNotFoundError(\n",
        "            \"No extracted JSON files found from Milestone 2\"\n",
        "        )\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\" MILESTONE 3 PIPELINE STARTED \")\n",
        "    print(\"==============================\")\n",
        "    print(f\"üìÑ Papers found: {len(extracted_files)}\")\n",
        "\n",
        "    all_summaries = {}\n",
        "    all_analyses = {}\n",
        "\n",
        "    for file in tqdm(extracted_files, desc=\"Processing papers\"):\n",
        "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "            paper_data = json.load(f)\n",
        "\n",
        "        paper_id = paper_data.get(\"paper_id\", file.stem)\n",
        "        sections = paper_data.get(\"sections\", {})\n",
        "\n",
        "        # ---- Summarization ----\n",
        "        paper_summary = summarize_paper(sections)\n",
        "        all_summaries[paper_id] = paper_summary\n",
        "\n",
        "        # ---- Analysis ----\n",
        "        paper_analysis = analyze_paper(paper_summary)\n",
        "        all_analyses[paper_id] = paper_analysis\n",
        "\n",
        "    with open(SUMMARY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_summaries, f, indent=2)\n",
        "\n",
        "    with open(ANALYSIS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_analyses, f, indent=2)\n",
        "\n",
        "    print(\"‚úÖ Paper-level summarization & analysis completed\")\n",
        "    return all_summaries, all_analyses\n",
        "\n",
        "# --------------------------------------------------\n",
        "# CROSS-PAPER SYNTHESIS (LITERATURE REVIEW)\n",
        "# --------------------------------------------------\n",
        "def generate_literature_review(\n",
        "    summaries: Dict[str, Dict[str, str]],\n",
        "    analyses: Dict[str, str]\n",
        ") -> str:\n",
        "\n",
        "    combined_text = \"\"\n",
        "\n",
        "    for paper_id in summaries:\n",
        "        combined_text += f\"\\nPAPER ID: {paper_id}\\n\"\n",
        "        combined_text += json.dumps(summaries[paper_id], indent=2)\n",
        "        combined_text += \"\\nANALYSIS:\\n\"\n",
        "        combined_text += analyses[paper_id]\n",
        "        combined_text += \"\\n\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert academic researcher.\n",
        "\n",
        "    Write a comprehensive literature review based on the papers below.\n",
        "\n",
        "    Your review must:\n",
        "    ‚Ä¢ Identify major research trends\n",
        "    ‚Ä¢ Compare methodologies\n",
        "    ‚Ä¢ Highlight common limitations\n",
        "    ‚Ä¢ Clearly state research gaps\n",
        "\n",
        "    Use IEEE-style academic tone.\n",
        "    Length: 800‚Äì1000 words.\n",
        "\n",
        "    CONTENT:\n",
        "    {combined_text[:12000]}\n",
        "    \"\"\"\n",
        "    return call_llm(prompt, temperature=0.3)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# MAIN PIPELINE\n",
        "# --------------------------------------------------\n",
        "def run_milestone_3():\n",
        "    summaries, analyses = process_all_papers()\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\" CROSS-PAPER SYNTHESIS STARTED \")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    final_review = generate_literature_review(summaries, analyses)\n",
        "\n",
        "    with open(FINAL_REVIEW_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(final_review)\n",
        "\n",
        "    print(\"‚úÖ Literature review generated successfully\")\n",
        "    print(\"üìÅ Output:\", FINAL_REVIEW_FILE)\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\" MILESTONE 3 COMPLETED \")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    return final_review\n",
        "\n",
        "# --------------------------------------------------\n",
        "# ENTRY POINT\n",
        "# --------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    run_milestone_3()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu4bW0NqAl2t",
        "outputId": "10e6409b-d75f-493a-af5a-e7a2f6360834"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API key loaded securely from Colab Secrets\n",
            "\n",
            "==============================\n",
            " MILESTONE 3 PIPELINE STARTED \n",
            "==============================\n",
            "üìÑ Papers found: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing papers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:56<00:00, 18.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Paper-level summarization & analysis completed\n",
            "\n",
            "==============================\n",
            " CROSS-PAPER SYNTHESIS STARTED \n",
            "==============================\n",
            "‚úÖ Literature review generated successfully\n",
            "üìÅ Output: data/milestone_3/final_literature_review.txt\n",
            "\n",
            "==============================\n",
            " MILESTONE 3 COMPLETED \n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "=========================================================\n",
        "MILESTONE 4 (FINAL ‚Äì STABLE VERSION)\n",
        "Review, Refinement, Quality Evaluation & Final Report\n",
        "=========================================================\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# --------------------------------------------------\n",
        "# IMPORTS\n",
        "# --------------------------------------------------\n",
        "import os\n",
        "import json\n",
        "from typing import Dict\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# --------------------------------------------------\n",
        "# PATHS\n",
        "# --------------------------------------------------\n",
        "M3_DIR = \"data/milestone_3\"\n",
        "M4_DIR = \"data/milestone_4\"\n",
        "\n",
        "SUMMARY_FILE = f\"{M3_DIR}/paper_summaries.json\"\n",
        "REVIEW_FILE = f\"{M3_DIR}/final_literature_review.txt\"\n",
        "\n",
        "REVISED_FILE = f\"{M4_DIR}/revised_sections.json\"\n",
        "QUALITY_FILE = f\"{M4_DIR}/quality_report.json\"\n",
        "FINAL_REPORT_FILE = f\"{M4_DIR}/final_report.txt\"\n",
        "\n",
        "os.makedirs(M4_DIR, exist_ok=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# OPENAI INITIALIZATION (COLAB SAFE)\n",
        "# --------------------------------------------------\n",
        "def init_client():\n",
        "    api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise EnvironmentError(\"OPENAI_API_KEY missing in Colab Secrets\")\n",
        "\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "    print(\"‚úÖ OpenAI API key loaded securely from Colab Secrets\")\n",
        "    return OpenAI()\n",
        "\n",
        "client = init_client()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# LLM CALL\n",
        "# --------------------------------------------------\n",
        "def call_llm(prompt, temperature=0.3):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temperature\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# --------------------------------------------------\n",
        "# REVIEW & REVISION\n",
        "# --------------------------------------------------\n",
        "def critique_and_revise(section, text):\n",
        "    prompt = f\"\"\"\n",
        "    You are a senior academic reviewer.\n",
        "\n",
        "    Critique the following {section} section and then provide\n",
        "    an improved revised version.\n",
        "\n",
        "    TEXT:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "    return call_llm(prompt, temperature=0.35)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# QUALITY EVALUATION\n",
        "# --------------------------------------------------\n",
        "def evaluate_quality(section, text):\n",
        "    prompt = f\"\"\"\n",
        "    Evaluate the quality of the following {section} section.\n",
        "\n",
        "    Score (1‚Äì10):\n",
        "    - Clarity\n",
        "    - Technical Depth\n",
        "    - Coherence\n",
        "    - Academic Tone\n",
        "\n",
        "    Provide short justification.\n",
        "\n",
        "    TEXT:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "    return call_llm(prompt, temperature=0.25)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# LOAD MILESTONE 3 OUTPUTS\n",
        "# --------------------------------------------------\n",
        "def load_m3():\n",
        "    with open(SUMMARY_FILE) as f:\n",
        "        summaries = json.load(f)\n",
        "\n",
        "    with open(REVIEW_FILE) as f:\n",
        "        review_text = f.read()\n",
        "\n",
        "    return summaries, review_text\n",
        "\n",
        "# --------------------------------------------------\n",
        "# MAIN REVIEW PIPELINE\n",
        "# --------------------------------------------------\n",
        "def run_milestone_4():\n",
        "    summaries, review_text = load_m3()\n",
        "\n",
        "    revised = {}\n",
        "    quality = {}\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\" MILESTONE 4 REVIEW STARTED \")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    for paper_id, sections in summaries.items():\n",
        "        revised[paper_id] = {}\n",
        "        quality[paper_id] = {}\n",
        "\n",
        "        for section, text in sections.items():\n",
        "            revised[paper_id][section] = critique_and_revise(section, text)\n",
        "            quality[paper_id][section] = evaluate_quality(section, text)\n",
        "\n",
        "    with open(REVISED_FILE, \"w\") as f:\n",
        "        json.dump(revised, f, indent=2)\n",
        "\n",
        "    with open(QUALITY_FILE, \"w\") as f:\n",
        "        json.dump(quality, f, indent=2)\n",
        "\n",
        "    final_report = generate_final_report(revised, review_text)\n",
        "\n",
        "    with open(FINAL_REPORT_FILE, \"w\") as f:\n",
        "        f.write(final_report)\n",
        "\n",
        "    print(\"‚úÖ MILESTONE 4 COMPLETED SUCCESSFULLY\")\n",
        "    print(\"üìÅ Output:\", FINAL_REPORT_FILE)\n",
        "\n",
        "    return final_report\n",
        "\n",
        "# --------------------------------------------------\n",
        "# FINAL REPORT\n",
        "# --------------------------------------------------\n",
        "def generate_final_report(revised, review_text):\n",
        "    report = \"FINAL AUTOMATED RESEARCH REVIEW REPORT\\n\"\n",
        "    report += \"=\" * 50 + \"\\n\\n\"\n",
        "\n",
        "    report += \"LITERATURE REVIEW\\n\\n\"\n",
        "    report += review_text + \"\\n\\n\"\n",
        "\n",
        "    report += \"REVISED SECTIONS\\n\\n\"\n",
        "\n",
        "    for paper_id, sections in revised.items():\n",
        "        report += f\"\\nPAPER ID: {paper_id}\\n\"\n",
        "        report += \"-\" * 30 + \"\\n\"\n",
        "        for sec, content in sections.items():\n",
        "            report += f\"\\n{sec.upper()}:\\n{content}\\n\"\n",
        "\n",
        "    return report\n",
        "\n",
        "# --------------------------------------------------\n",
        "# RUN\n",
        "# --------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    run_milestone_4()\n"
      ],
      "metadata": {
        "id": "s9ofgl6uGMJS",
        "outputId": "e34b36f0-2b1f-4113-95c6-090fbe6921e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API key loaded securely from Colab Secrets\n",
            "\n",
            "==============================\n",
            " MILESTONE 4 REVIEW STARTED \n",
            "==============================\n",
            "‚úÖ MILESTONE 4 COMPLETED SUCCESSFULLY\n",
            "üìÅ Output: data/milestone_4/final_report.txt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}