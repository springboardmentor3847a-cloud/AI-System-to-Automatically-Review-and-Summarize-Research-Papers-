{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927f9d0a",
   "metadata": {},
   "source": [
    "# Module 1 : Topic Input & Paper Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162bfa5d",
   "metadata": {},
   "source": [
    "### Install and Import required libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999f3643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\abhis\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# !pip install semanticscholar python-dotenv requests -q\n",
    "\n",
    "import json\n",
    "import os\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from semanticscholar import SemanticScholar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad56ec",
   "metadata": {},
   "source": [
    "### Setup API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc5594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_api_key() -> SemanticScholar:\n",
    "    \"\"\"\n",
    "    Initialize and return a SemanticScholar client.\n",
    "\n",
    "    Behavior:\n",
    "    - Attempts to load SEMANTIC_SCHOLAR_API_KEY from a .env file.\n",
    "    - If not found, does NOT write a real API key to disk (hard-coded keys removed).\n",
    "      Instead, continues without a key (limited rate) and prints clear instructions.\n",
    "    - Returns an initialized SemanticScholar client (with or without api_key).\n",
    "\n",
    "    Returns:\n",
    "        SemanticScholar: Initialized client object.\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "\n",
    "    if not api_key:\n",
    "        print(\n",
    "            \"SEMANTIC_SCHOLAR_API_KEY not found in environment. \"\n",
    "            \"Proceeding without API key (limited rate).\"\n",
    "        )\n",
    "        print(\n",
    "            \"To use a key: create a .env file with a line like:\\n\"\n",
    "            \"SEMANTIC_SCHOLAR_API_KEY=83rBkeaXb14D8vGpXJezU6nrCFFmyn5L8RCvT9MM\\n\"\n",
    "            \"Then re-run this script.\"\n",
    "        )\n",
    "        scholar_client = SemanticScholar()\n",
    "    else:\n",
    "        scholar_client = SemanticScholar(api_key=api_key)\n",
    "        print(\"Semantic Scholar initialized with API key.\")\n",
    "\n",
    "    return scholar_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b942c64",
   "metadata": {},
   "source": [
    "### Search Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1b881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(topic: str, limit: int = 20) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search Semantic Scholar for papers on a given topic.\n",
    "\n",
    "    Args:\n",
    "        topic (str): Topic/query string for searching papers.\n",
    "        limit (int): Maximum number of papers to request.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: Dictionary containing search metadata and papers list\n",
    "                      or None if an error occurred.\n",
    "    \"\"\"\n",
    "    if not topic or not topic.strip():\n",
    "        raise ValueError(\"search_papers requires a non-empty topic string.\")\n",
    "\n",
    "    print(f\"\\nSearching for papers on: '{topic}' (limit={limit})\")\n",
    "\n",
    "    scholar_client = setup_api_key()\n",
    "\n",
    "    try:\n",
    "        # Request fields that are useful downstream\n",
    "        results = scholar_client.search_paper(\n",
    "            query=topic,\n",
    "            limit=limit,\n",
    "            fields=[\n",
    "                \"paperId\", \"title\", \"abstract\", \"year\", \"authors\",\n",
    "                \"citationCount\", \"openAccessPdf\", \"url\", \"venue\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        papers: List[Dict[str, Any]] = []\n",
    "\n",
    "        for paper in results:\n",
    "            raw_authors = getattr(paper, \"authors\", []) or []\n",
    "            authors: List[str] = []\n",
    "            for a in raw_authors:\n",
    "                if hasattr(a, \"name\"):\n",
    "                    authors.append(getattr(a, \"name\"))\n",
    "                elif isinstance(a, dict) and \"name\" in a:\n",
    "                    authors.append(a[\"name\"])\n",
    "                else:\n",
    "                    authors.append(str(a))\n",
    "\n",
    "            open_access_pdf = getattr(paper, \"openAccessPdf\", None)\n",
    "            pdf_url = None\n",
    "            has_pdf = False\n",
    "            if open_access_pdf:\n",
    "                if isinstance(open_access_pdf, dict):\n",
    "                    pdf_url = open_access_pdf.get(\"url\")\n",
    "                else:\n",
    "                    pdf_url = getattr(open_access_pdf, \"get\", lambda x: None)(\"url\")\n",
    "                has_pdf = bool(pdf_url)\n",
    "\n",
    "            paper_entry = {\n",
    "                \"title\": getattr(paper, \"title\", \"\") or \"No title\",\n",
    "                \"authors\": authors,\n",
    "                \"year\": getattr(paper, \"year\", None),\n",
    "                \"paperId\": getattr(paper, \"paperId\", None),\n",
    "                \"abstract\": (getattr(paper, \"abstract\", \"\") or \"\")[:300] + (\"...\" if getattr(paper, \"abstract\", None) and len(getattr(paper, \"abstract\", \"\")) > 300 else \"\"),\n",
    "                \"citationCount\": getattr(paper, \"citationCount\", 0),\n",
    "                \"venue\": getattr(paper, \"venue\", None),\n",
    "                \"url\": getattr(paper, \"url\", None),\n",
    "                \"pdf_url\": pdf_url,\n",
    "                \"has_pdf\": has_pdf\n",
    "            }\n",
    "            papers.append(paper_entry)\n",
    "\n",
    "        papers_with_pdf = sum(1 for p in papers if p[\"has_pdf\"])\n",
    "\n",
    "        print(\"Search complete!\")\n",
    "        print(f\"  Total papers returned: {len(papers)}\")\n",
    "        print(f\"  Papers with PDF available: {papers_with_pdf}\")\n",
    "\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"search_timestamp\": datetime.now().isoformat(),\n",
    "            \"total_results\": len(papers),\n",
    "            \"papers_with_pdf\": papers_with_pdf,\n",
    "            \"papers\": papers\n",
    "        }\n",
    "\n",
    "    except Exception as exc:\n",
    "        print(f\"Error searching papers: {exc}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4386be9",
   "metadata": {},
   "source": [
    "### Save Searched Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "968ce73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_search_results(data: Dict[str, Any], filename: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Save search results dict to a JSON file under data/search_results.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Data returned by `search_papers`.\n",
    "        filename (str, optional): Custom filename. If None, generate from topic.\n",
    "\n",
    "    Returns:\n",
    "        str: Full path of the saved JSON file.\n",
    "    \"\"\"\n",
    "    if not data or \"topic\" not in data:\n",
    "        raise ValueError(\"save_search_results requires data dictionary with a 'topic' key.\")\n",
    "\n",
    "    # Create a filesystem-safe filename if not provided\n",
    "    if not filename:\n",
    "        safe_topic = \"\".join(c for c in data[\"topic\"] if c.isalnum() or c == \" \").strip()\n",
    "        safe_topic = safe_topic.replace(\" \", \"_\") or \"search\"\n",
    "        filename = f\"paper_search_results_{safe_topic}.json\"\n",
    "\n",
    "    os.makedirs(\"data/search_results\", exist_ok=True)\n",
    "    filepath = os.path.join(\"data/search_results\", filename)\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(data, fh, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Search results saved to: {filepath}\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72801b",
   "metadata": {},
   "source": [
    "### Display the Searched Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f476d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_search_results(data: Dict[str, Any], max_display: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    Display search results as a pandas DataFrame (table).\n",
    "\n",
    "    If running in a Jupyter / notebook environment the DataFrame will render\n",
    "    as a nice HTML table. In a plain console, the DataFrame will be printed\n",
    "    as text. Shows top `max_display` papers.\n",
    "    \"\"\"\n",
    "    if not data or \"papers\" not in data:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    papers = data[\"papers\"]\n",
    "    total = len(papers)\n",
    "    pdf_count = sum(1 for p in papers if p.get(\"has_pdf\"))\n",
    "    no_pdf_count = total - pdf_count\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(f\"SEARCH RESULTS: {data.get('topic', 'Unknown topic')}\")\n",
    "    print(\"=\" * 72)\n",
    "    print(\"\\nStatistics:\")\n",
    "    print(f\"  • Total papers: {total}\")\n",
    "    print(f\"  • Papers with PDF: {pdf_count}\")\n",
    "    print(f\"  • Papers without PDF: {no_pdf_count}\")\n",
    "\n",
    "    to_show = min(max_display, total)\n",
    "    if to_show == 0:\n",
    "        print(\"\\nNo papers to display.\")\n",
    "        return\n",
    "\n",
    "    # Build rows for DataFrame\n",
    "    rows = []\n",
    "    for idx, paper in enumerate(papers[:to_show], start=1):\n",
    "        title = paper.get(\"title\", \"\") or \"\"\n",
    "        authors = paper.get(\"authors\", []) or []\n",
    "        authors_display = \", \".join(authors)\n",
    "        year = paper.get(\"year\", \"\")\n",
    "        citations = paper.get(\"citationCount\", 0)\n",
    "        has_pdf = paper.get(\"has_pdf\", False)\n",
    "        pdf_url = paper.get(\"pdf_url\", \"\") or \"\"\n",
    "        url = paper.get(\"url\", \"\") or \"\"\n",
    "        abstract = (paper.get(\"abstract\") or \"\")\n",
    "        if len(abstract) > 300:\n",
    "            abstract = abstract[:297] + \"...\"\n",
    "\n",
    "        rows.append({\n",
    "            \"#\": idx,\n",
    "            \"Title\": title,\n",
    "            \"Authors\": authors_display,\n",
    "            \"Year\": year,\n",
    "            \"Citations\": citations,\n",
    "            \"Has PDF\": has_pdf,\n",
    "            \"PDF URL\": pdf_url,\n",
    "            \"URL\": url,\n",
    "            \"Abstract\": abstract\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    col_order = [\"#\", \"Title\", \"Authors\", \"Year\", \"Citations\", \"Has PDF\", \"PDF URL\", \"URL\", \"Abstract\"]\n",
    "    df = df[col_order]\n",
    "\n",
    "    try:\n",
    "        from IPython.display import display as _display, HTML\n",
    "        _display(df)\n",
    "    except Exception:\n",
    "        pd.set_option(\"display.max_colwidth\", 120)\n",
    "        print(\"\\nTop results (DataFrame):\\n\")\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "    print(f\"\\nShowing top {to_show} of {total} papers. Use `max_display` to change the table size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10a6fe",
   "metadata": {},
   "source": [
    "### Main Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c0a73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================\n",
      "MODULE 1: TOPIC INPUT & PAPER SEARCH\n",
      "========================================================================\n",
      "\n",
      "Enter research topic: Sun\n",
      "\n",
      "Searching for papers on: 'Sun' (limit=20)\n",
      "Semantic Scholar initialized with API key.\n",
      "Search complete!\n",
      "  Total papers returned: 1000\n",
      "  Papers with PDF available: 433\n",
      "Search results saved to: data/search_results\\paper_search_results_Sun.json\n",
      "\n",
      "========================================================================\n",
      "SEARCH RESULTS: Sun\n",
      "========================================================================\n",
      "\n",
      "Statistics:\n",
      "  • Total papers: 1000\n",
      "  • Papers with PDF: 433\n",
      "  • Papers without PDF: 567\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Citations</th>\n",
       "      <th>Has PDF</th>\n",
       "      <th>PDF URL</th>\n",
       "      <th>URL</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The chemical composition of the Sun</td>\n",
       "      <td>N. Grevesse, Martin Asplund, A. Sauval, Pat Scott</td>\n",
       "      <td>2009</td>\n",
       "      <td>6661</td>\n",
       "      <td>True</td>\n",
       "      <td>https://arxiv.org/pdf/0909.0948</td>\n",
       "      <td>https://www.semanticscholar.org/paper/abcacba6...</td>\n",
       "      <td>We present a redetermination of the solar abun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SUN database: Large-scale scene recognition fr...</td>\n",
       "      <td>Jianxiong Xiao, James Hays, Krista A. Ehinger,...</td>\n",
       "      <td>2010</td>\n",
       "      <td>4079</td>\n",
       "      <td>True</td>\n",
       "      <td>https://dspace.mit.edu/bitstream/1721.1/60690/...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/908091b4...</td>\n",
       "      <td>Scene categorization is a fundamental problem ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>IEEE Standard for Low-Rate Wireless Networks A...</td>\n",
       "      <td></td>\n",
       "      <td>2022</td>\n",
       "      <td>510</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>https://www.semanticscholar.org/paper/3546467e...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The validity and practicality of sun-reactive ...</td>\n",
       "      <td>T. Fitzpatrick</td>\n",
       "      <td>1988</td>\n",
       "      <td>4019</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>https://www.semanticscholar.org/paper/e5739d50...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SUN RGB-D: A RGB-D scene understanding benchma...</td>\n",
       "      <td>Shuran Song, Samuel P. Lichtenberg, Jianxiong ...</td>\n",
       "      <td>2015</td>\n",
       "      <td>1964</td>\n",
       "      <td>True</td>\n",
       "      <td>http://vision.cs.princeton.edu/projects/2015/S...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/b73e2d40...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>A flexible inversion algorithm for retrieval o...</td>\n",
       "      <td>O. Dubovik, M. King</td>\n",
       "      <td>2000</td>\n",
       "      <td>2354</td>\n",
       "      <td>True</td>\n",
       "      <td>https://onlinelibrary.wiley.com/doi/pdfdirect/...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/18a91fec...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>The chemical make-up of the Sun: A 2020 vision</td>\n",
       "      <td>M. Asplund, A. Amarsi, N. Grevesse</td>\n",
       "      <td>2021</td>\n",
       "      <td>327</td>\n",
       "      <td>True</td>\n",
       "      <td>https://www.aanda.org/articles/aa/pdf/2021/09/...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/1d6e72f6...</td>\n",
       "      <td>Context. The chemical composition of the Sun i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Sun Microsystems</td>\n",
       "      <td>Elena Loutskina, Eric Varney</td>\n",
       "      <td>2017</td>\n",
       "      <td>615</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>https://www.semanticscholar.org/paper/60393847...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Accuracy assessments of aerosol optical proper...</td>\n",
       "      <td>O. Dubovik, A. Smirnov, B. Holben, M. King, Y....</td>\n",
       "      <td>2000</td>\n",
       "      <td>1664</td>\n",
       "      <td>True</td>\n",
       "      <td>https://onlinelibrary.wiley.com/doi/pdfdirect/...</td>\n",
       "      <td>https://www.semanticscholar.org/paper/5ac95c69...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Tailoring Graphene Oxide‐Based Aerogels for Ef...</td>\n",
       "      <td>Xiaozhen Hu, Weichao Xu, Lin Zhou, Yingling Ta...</td>\n",
       "      <td>2017</td>\n",
       "      <td>852</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>https://www.semanticscholar.org/paper/80c59640...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    #                                              Title  \\\n",
       "0   1                The chemical composition of the Sun   \n",
       "1   2  SUN database: Large-scale scene recognition fr...   \n",
       "2   3  IEEE Standard for Low-Rate Wireless Networks A...   \n",
       "3   4  The validity and practicality of sun-reactive ...   \n",
       "4   5  SUN RGB-D: A RGB-D scene understanding benchma...   \n",
       "5   6  A flexible inversion algorithm for retrieval o...   \n",
       "6   7     The chemical make-up of the Sun: A 2020 vision   \n",
       "7   8                                   Sun Microsystems   \n",
       "8   9  Accuracy assessments of aerosol optical proper...   \n",
       "9  10  Tailoring Graphene Oxide‐Based Aerogels for Ef...   \n",
       "\n",
       "                                             Authors  Year  Citations  \\\n",
       "0  N. Grevesse, Martin Asplund, A. Sauval, Pat Scott  2009       6661   \n",
       "1  Jianxiong Xiao, James Hays, Krista A. Ehinger,...  2010       4079   \n",
       "2                                                     2022        510   \n",
       "3                                     T. Fitzpatrick  1988       4019   \n",
       "4  Shuran Song, Samuel P. Lichtenberg, Jianxiong ...  2015       1964   \n",
       "5                                O. Dubovik, M. King  2000       2354   \n",
       "6                 M. Asplund, A. Amarsi, N. Grevesse  2021        327   \n",
       "7                       Elena Loutskina, Eric Varney  2017        615   \n",
       "8  O. Dubovik, A. Smirnov, B. Holben, M. King, Y....  2000       1664   \n",
       "9  Xiaozhen Hu, Weichao Xu, Lin Zhou, Yingling Ta...  2017        852   \n",
       "\n",
       "   Has PDF                                            PDF URL  \\\n",
       "0     True                    https://arxiv.org/pdf/0909.0948   \n",
       "1     True  https://dspace.mit.edu/bitstream/1721.1/60690/...   \n",
       "2    False                                                      \n",
       "3    False                                                      \n",
       "4     True  http://vision.cs.princeton.edu/projects/2015/S...   \n",
       "5     True  https://onlinelibrary.wiley.com/doi/pdfdirect/...   \n",
       "6     True  https://www.aanda.org/articles/aa/pdf/2021/09/...   \n",
       "7    False                                                      \n",
       "8     True  https://onlinelibrary.wiley.com/doi/pdfdirect/...   \n",
       "9    False                                                      \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.semanticscholar.org/paper/abcacba6...   \n",
       "1  https://www.semanticscholar.org/paper/908091b4...   \n",
       "2  https://www.semanticscholar.org/paper/3546467e...   \n",
       "3  https://www.semanticscholar.org/paper/e5739d50...   \n",
       "4  https://www.semanticscholar.org/paper/b73e2d40...   \n",
       "5  https://www.semanticscholar.org/paper/18a91fec...   \n",
       "6  https://www.semanticscholar.org/paper/1d6e72f6...   \n",
       "7  https://www.semanticscholar.org/paper/60393847...   \n",
       "8  https://www.semanticscholar.org/paper/5ac95c69...   \n",
       "9  https://www.semanticscholar.org/paper/80c59640...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  We present a redetermination of the solar abun...  \n",
       "1  Scene categorization is a fundamental problem ...  \n",
       "2                                                     \n",
       "3                                                     \n",
       "4                                                     \n",
       "5                                                     \n",
       "6  Context. The chemical composition of the Sun i...  \n",
       "7                                                     \n",
       "8                                                     \n",
       "9                                                     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing top 10 of 1000 papers. Use `max_display` to change the table size.\n",
      "\n",
      "Module 1 complete. Results saved to: data/search_results\\paper_search_results_Sun.json\n",
      "Proceed to Module 2 for paper selection and PDF download.\n"
     ]
    }
   ],
   "source": [
    "def main_search() -> (Optional[Dict[str, Any]], Optional[str]):\n",
    "    \"\"\"\n",
    "    Interactive main entry for Module 1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (results dict or None, path to saved file or None).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"MODULE 1: TOPIC INPUT & PAPER SEARCH\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    try:\n",
    "        topic = input(\"\\nEnter research topic: \").strip()\n",
    "    except Exception:\n",
    "        topic = \"\"\n",
    "\n",
    "    if not topic:\n",
    "        topic = \"artificial intelligence\"\n",
    "\n",
    "    results = search_papers(topic, limit=20)\n",
    "    if not results:\n",
    "        print(\"No results found or an error occurred during search.\")\n",
    "        return None, None\n",
    "\n",
    "    save_path = save_search_results(results)\n",
    "    display_search_results(results)\n",
    "\n",
    "    print(\"\\nModule 1 complete. Results saved to:\", save_path)\n",
    "    print(\"Proceed to Module 2 for paper selection and PDF download.\")\n",
    "    return results, save_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fdf8c4",
   "metadata": {},
   "source": [
    "# Module 2 : Paper Selection & Pdf Downloads\n",
    "### install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ba0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyMuPDF requests -q\n",
    "\n",
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "import requests\n",
    "import fitz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b709e",
   "metadata": {},
   "source": [
    "### Load the Previous Seached Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946120aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_search_results(filepath: Optional[str] = None) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load previously saved search results JSON.\n",
    "\n",
    "    If filepath is None, find the newest JSON in data/search_results.\n",
    "\n",
    "    Args:\n",
    "        filepath: Optional path to a specific results file.\n",
    "\n",
    "    Returns:\n",
    "        Parsed JSON dict or None if loading fails.\n",
    "    \"\"\"\n",
    "    if filepath:\n",
    "        results_path = Path(filepath)\n",
    "    else:\n",
    "        results_dir = Path(\"data/search_results\")\n",
    "        if not results_dir.exists():\n",
    "            print(\"Search results directory not found. Run Module 1 first.\")\n",
    "            return None\n",
    "\n",
    "        json_files = sorted(\n",
    "            (f for f in results_dir.iterdir() if f.suffix == \".json\"),\n",
    "            key=lambda p: p.stat().st_mtime,\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        if not json_files:\n",
    "            print(\"No search results found. Run Module 1 first.\")\n",
    "            return None\n",
    "\n",
    "        results_path = json_files[0]\n",
    "        print(f\"Loading most recent search results: {results_path.name}\")\n",
    "\n",
    "    try:\n",
    "        with results_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "            data = json.load(fh)\n",
    "        papers = data.get(\"papers\", [])\n",
    "        print(f\"Loaded {len(papers)} papers on '{data.get('topic', 'Unknown')}'\")\n",
    "        return data\n",
    "    except Exception as exc:\n",
    "        print(f\"Error loading file {results_path}: {exc}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418023c",
   "metadata": {},
   "source": [
    "### Filter Research Papers as PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c0758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_papers_with_pdfs(papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return subset of papers that contain a plausible PDF URL.\n",
    "\n",
    "    Args:\n",
    "        papers: list of paper dicts (from Module 1 save format).\n",
    "\n",
    "    Returns:\n",
    "        List of papers that likely have PDF links.\n",
    "    \"\"\"\n",
    "    papers_with_pdf: List[Dict[str, Any]] = []\n",
    "    for paper in papers:\n",
    "        pdf_url = (paper.get(\"pdf_url\") or \"\").strip()\n",
    "        if not pdf_url:\n",
    "            continue\n",
    "\n",
    "        lower = pdf_url.lower()\n",
    "        if lower.endswith(\".pdf\") or \".pdf?\" in lower or \"pdf\" in lower:\n",
    "            papers_with_pdf.append(paper)\n",
    "\n",
    "    print(f\"\\nPDF Availability:\")\n",
    "    print(f\"  • Total papers checked: {len(papers)}\")\n",
    "    print(f\"  • Papers with PDF URLs: {len(papers_with_pdf)}\")\n",
    "\n",
    "    return papers_with_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daef81d",
   "metadata": {},
   "source": [
    "### Rank the Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b5d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_papers(papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Rank papers by citation count (desc) then year (desc).\n",
    "\n",
    "    Args:\n",
    "        papers: list of paper dicts.\n",
    "\n",
    "    Returns:\n",
    "        Sorted list (highest ranked first).\n",
    "    \"\"\"\n",
    "    valid = [p for p in papers if p.get(\"citationCount\") is not None]\n",
    "    ranked = sorted(\n",
    "        valid,\n",
    "        key=lambda x: (int(x.get(\"citationCount\", 0)), int(x.get(\"year\", 0) or 0)),\n",
    "        reverse=True\n",
    "    )\n",
    "    return ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2b62a",
   "metadata": {},
   "source": [
    "### Select the Top Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce7f16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_papers(papers: List[Dict[str, Any]], count: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Select top N papers (by ranking) that have PDFs.\n",
    "\n",
    "    Args:\n",
    "        papers: All papers (from search results).\n",
    "        count: Number of papers to select.\n",
    "\n",
    "    Returns:\n",
    "        Selected papers list.\n",
    "    \"\"\"\n",
    "    papers_with_pdf = filter_papers_with_pdfs(papers)\n",
    "    ranked = rank_papers(papers_with_pdf)\n",
    "    selected = ranked[:count]\n",
    "\n",
    "    print(f\"\\nSelected top {len(selected)} papers for download:\")\n",
    "    for i, p in enumerate(selected, 1):\n",
    "        title = p.get(\"title\", \"\")[:70]\n",
    "        print(f\"\\n{i}. {title}{'...' if len(p.get('title', '')) > 70 else ''}\")\n",
    "        print(f\"   Citations: {p.get('citationCount', 0)}\")\n",
    "        print(f\"   Year: {p.get('year', 'N/A')}\")\n",
    "        authors = \", \".join(p.get(\"authors\", [])[:2])\n",
    "        print(f\"   Authors: {authors}\")\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dca2a7",
   "metadata": {},
   "source": [
    "### Verification the Research Papers for PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e6739b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_response_pdf(response: requests.Response) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic check if a requests response looks like a PDF.\n",
    "\n",
    "    Args:\n",
    "        response: requests.Response object.\n",
    "\n",
    "    Returns:\n",
    "        True if response content-type or initial bytes indicate PDF.\n",
    "    \"\"\"\n",
    "    content_type = response.headers.get(\"content-type\", \"\").lower()\n",
    "    if \"pdf\" in content_type:\n",
    "        return True\n",
    "    # Check first bytes (PDF files start with '%PDF')\n",
    "    start = response.content[:4]\n",
    "    return start == b\"%PDF\"\n",
    "\n",
    "\n",
    "\n",
    "def download_pdf_with_verification(url: str, filename: str, max_retries: int = 2, chunk_size: int = 1024*32) -> bool:\n",
    "    \"\"\"\n",
    "    Download a PDF URL to `filename` with verification.\n",
    "\n",
    "    - Streams to disk using iter_content (no resp.raw.seek usage).\n",
    "    - Checks initial bytes for '%PDF' and content-type hints.\n",
    "    - Retries on transient errors with simple backoff.\n",
    "    - Returns True on success (valid PDF), False otherwise.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
    "    }\n",
    "\n",
    "    target = Path(filename)\n",
    "    target.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"  Attempt {attempt}/{max_retries}: {url}\")\n",
    "            with session.get(url, headers=headers, timeout=30, stream=True, allow_redirects=True) as resp:\n",
    "                status = resp.status_code\n",
    "\n",
    "                if status == 403:\n",
    "                    print(f\"    HTTP 403 - Forbidden (site blocked this request).\")\n",
    "                    return False\n",
    "\n",
    "                if status >= 400:\n",
    "                    print(f\"    HTTP {status} - skipping this attempt\")\n",
    "                    time.sleep(1 * attempt)\n",
    "                    continue\n",
    "\n",
    "                # Basic content-type check\n",
    "                content_type = resp.headers.get(\"content-type\", \"\").lower()\n",
    "                looks_like_pdf_by_ct = \"pdf\" in content_type\n",
    "\n",
    "                first_bytes = b\"\"\n",
    "                bytes_written = 0\n",
    "\n",
    "                with open(target, \"wb\") as fh:\n",
    "                    for chunk in resp.iter_content(chunk_size=chunk_size):\n",
    "                        if not chunk:\n",
    "                            continue\n",
    "\n",
    "                        if bytes_written < 8:\n",
    "                            need = 8 - len(first_bytes)\n",
    "                            first_bytes += chunk[:need]\n",
    "                        fh.write(chunk)\n",
    "                        bytes_written += len(chunk)\n",
    "\n",
    "                if first_bytes.startswith(b\"%PDF\") or looks_like_pdf_by_ct:\n",
    "                    # Final verification using PyMuPDF (fitz)\n",
    "                    try:\n",
    "                        import fitz\n",
    "                        with fitz.open(str(target)) as doc:\n",
    "                            if len(doc) > 0:\n",
    "                                size = target.stat().st_size\n",
    "                                print(f\"    Downloaded: {size:,} bytes -> {target.name}\")\n",
    "                                return True\n",
    "                            else:\n",
    "                                print(\"    Downloaded file opened but has zero pages.\")\n",
    "                                target.unlink(missing_ok=True)\n",
    "                                continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"    PDF verification failed (PyMuPDF): {e}\")\n",
    "                        target.unlink(missing_ok=True)\n",
    "                        continue\n",
    "                else:\n",
    "                    print(\"    File does not look like a PDF (missing %PDF signature and content-type not PDF).\")\n",
    "                    target.unlink(missing_ok=True)\n",
    "                    continue\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"    Timeout during download attempt.\")\n",
    "            time.sleep(1 * attempt)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # handle other network errors\n",
    "            print(f\"    Network error during download: {e}\")\n",
    "            time.sleep(1 * attempt)\n",
    "        except Exception as e:\n",
    "            print(f\"    Unexpected error during download: {e}\")\n",
    "            time.sleep(1 * attempt)\n",
    "\n",
    "    # All attempts failed\n",
    "    return False\n",
    "\n",
    "def get_pdf_info(filepath: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Return basic metadata about a PDF file.\n",
    "\n",
    "    Args:\n",
    "        filepath: path to PDF.\n",
    "\n",
    "    Returns:\n",
    "        Dict with pages, size_bytes, size_mb, is_valid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = Path(filepath)\n",
    "        if not p.exists():\n",
    "            return {\"is_valid\": False}\n",
    "\n",
    "        size_bytes = p.stat().st_size\n",
    "        with fitz.open(str(p)) as doc:\n",
    "            pages = len(doc)\n",
    "        return {\n",
    "            \"pages\": pages,\n",
    "            \"size_bytes\": size_bytes,\n",
    "            \"size_mb\": round(size_bytes / (1024 * 1024), 2),\n",
    "            \"is_valid\": True\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"is_valid\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea98e2d",
   "metadata": {},
   "source": [
    "### Download the Selected Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1c3275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_selected_papers(selected_papers: List[Dict[str, Any]], output_dir: str = \"downloads\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Download selected papers to output directory and collect metadata.\n",
    "\n",
    "    Args:\n",
    "        selected_papers: list of paper dicts with keys 'title' and 'pdf_url'.\n",
    "        output_dir: directory to place downloaded PDFs.\n",
    "\n",
    "    Returns:\n",
    "        List of paper dicts augmented with download metadata.\n",
    "    \"\"\"\n",
    "    out_path = Path(output_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    downloaded: List[Dict[str, Any]] = []\n",
    "    print(f\"\\nStarting PDF downloads to: {out_path}/\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for idx, paper in enumerate(selected_papers, start=1):\n",
    "        title = paper.get(\"title\", \"untitled\")\n",
    "        print(f\"\\n[{idx}/{len(selected_papers)}] Downloading: {title[:60]}{'...' if len(title) > 60 else ''}\")\n",
    "\n",
    "        # Create safe filename derived from title and a short hash to avoid collisions\n",
    "        safe_title = \"\".join(c for c in title if c.isalnum() or c in (\" \", \"-\", \"_\")).strip()\n",
    "        safe_title = safe_title[:50] if len(safe_title) > 50 else safe_title\n",
    "        short_hash = hashlib.md5(safe_title.encode(\"utf-8\")).hexdigest()[:8]\n",
    "        filename = out_path / f\"paper_{idx}_{short_hash}.pdf\"\n",
    "\n",
    "        pdf_url = paper.get(\"pdf_url\")\n",
    "        success = False\n",
    "        if pdf_url:\n",
    "            success = download_pdf_with_verification(str(pdf_url), str(filename))\n",
    "\n",
    "        if success:\n",
    "            pdf_info = get_pdf_info(str(filename))\n",
    "            paper[\"downloaded\"] = True\n",
    "            paper[\"local_path\"] = str(filename)\n",
    "            paper[\"download_time\"] = datetime.now().isoformat()\n",
    "            paper[\"pdf_info\"] = pdf_info\n",
    "            downloaded.append(paper)\n",
    "            print(f\"    Success! {pdf_info.get('pages', 'N/A')} pages, {pdf_info.get('size_mb', 'N/A')} MB\")\n",
    "        else:\n",
    "            paper[\"downloaded\"] = False\n",
    "            print(\"    Failed to download.\")\n",
    "\n",
    "    return downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57411bc6",
   "metadata": {},
   "source": [
    "### Save & Verify the Downloaded Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9bc2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_download_report(downloaded_papers: List[Dict[str, Any]], topic: str, output_dir: str = \"downloads\") -> str:\n",
    "    \"\"\"\n",
    "    Save a detailed download report and a simple list of downloaded files.\n",
    "\n",
    "    Args:\n",
    "        downloaded_papers: list returned from download_selected_papers.\n",
    "        topic: research topic string for report context.\n",
    "        output_dir: directory where downloaded files are located.\n",
    "\n",
    "    Returns:\n",
    "        Path to the saved JSON report.\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        \"topic\": topic,\n",
    "        \"download_timestamp\": datetime.now().isoformat(),\n",
    "        \"total_selected\": len(downloaded_papers),\n",
    "        \"successful_downloads\": sum(1 for p in downloaded_papers if p.get(\"downloaded\")),\n",
    "        \"failed_downloads\": sum(1 for p in downloaded_papers if not p.get(\"downloaded\")),\n",
    "        \"papers\": downloaded_papers\n",
    "    }\n",
    "\n",
    "    Path(\"data/reports\").mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    report_file = Path(\"data/reports\") / f\"download_report_{timestamp}.json\"\n",
    "\n",
    "    with report_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(report, fh, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nDownload report saved to: {report_file}\")\n",
    "\n",
    "    download_list = []\n",
    "    for p in downloaded_papers:\n",
    "        if p.get(\"downloaded\"):\n",
    "            pdf_info = p.get(\"pdf_info\", {})\n",
    "            download_list.append({\n",
    "                \"title\": p.get(\"title\"),\n",
    "                \"local_file\": p.get(\"local_path\"),\n",
    "                \"size_mb\": pdf_info.get(\"size_mb\"),\n",
    "                \"pages\": pdf_info.get(\"pages\")\n",
    "            })\n",
    "\n",
    "    list_file = Path(output_dir) / \"downloaded_papers_list.json\"\n",
    "    with list_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(download_list, fh, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return str(report_file)\n",
    "\n",
    "\n",
    "def verify_downloads(output_dir: str = \"downloads\") -> int:\n",
    "    \"\"\"\n",
    "    Verify all PDFs in the output directory and print a summary.\n",
    "\n",
    "    Args:\n",
    "        output_dir: directory containing downloaded PDFs.\n",
    "\n",
    "    Returns:\n",
    "        Number of valid PDF files.\n",
    "    \"\"\"\n",
    "    out_path = Path(output_dir)\n",
    "    if not out_path.exists():\n",
    "        print(f\"Directory '{output_dir}' does not exist!\")\n",
    "        return 0\n",
    "\n",
    "    pdf_files = sorted(out_path.glob(\"*.pdf\"))\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VERIFICATION OF DOWNLOADS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nDirectory: {out_path.resolve()}\")\n",
    "    print(f\"PDF files found: {len(pdf_files)}\")\n",
    "\n",
    "    total_size = 0\n",
    "    valid_count = 0\n",
    "\n",
    "    if pdf_files:\n",
    "        print(\"\\nFile Details:\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for pdf in pdf_files:\n",
    "            size = pdf.stat().st_size\n",
    "            total_size += size\n",
    "            info = get_pdf_info(str(pdf))\n",
    "            is_valid = info.get(\"is_valid\", False)\n",
    "            if is_valid:\n",
    "                valid_count += 1\n",
    "                try:\n",
    "                    with fitz.open(str(pdf)) as doc:\n",
    "                        pages = len(doc)\n",
    "                except Exception:\n",
    "                    pages = \"N/A\"\n",
    "                print(f\" {pdf.name}\")\n",
    "                print(f\"   Size: {size:,} bytes ({size / (1024 * 1024):.2f} MB)\")\n",
    "                print(f\"   Pages: {pages}\")\n",
    "            else:\n",
    "                print(f\" {pdf.name} - INVALID PDF\")\n",
    "                print(f\"   Size: {size:,} bytes\")\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  • Total PDF files: {len(pdf_files)}\")\n",
    "    print(f\"  • Valid PDFs: {valid_count}\")\n",
    "    print(f\"  • Total size: {total_size / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    return valid_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50aa87",
   "metadata": {},
   "source": [
    "### Main Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b8382b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================\n",
      "MODULE 2: PAPER SELECTION & PDF DOWNLOAD\n",
      "========================================================================\n",
      "Loading most recent search results: paper_search_results_Sun.json\n",
      "Loaded 1000 papers on 'Sun'\n",
      "\n",
      "PDF Availability:\n",
      "  • Total papers checked: 1000\n",
      "  • Papers with PDF URLs: 344\n",
      "\n",
      "Selected top 3 papers for download:\n",
      "\n",
      "1. Microsoft COCO: Common Objects in Context\n",
      "   Citations: 48983\n",
      "   Year: 2014\n",
      "   Authors: Tsung-Yi Lin, M. Maire\n",
      "\n",
      "2. The CLUSTAL_X windows interface: flexible strategies for multiple sequ...\n",
      "   Citations: 39945\n",
      "   Year: 1997\n",
      "   Authors: J. Thompson, T. Gibson\n",
      "\n",
      "3. Solar water splitting cells.\n",
      "   Citations: 8319\n",
      "   Year: 2010\n",
      "   Authors: M. Walter, E. Warren\n",
      "\n",
      "Starting PDF downloads to: downloads/\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1/3] Downloading: Microsoft COCO: Common Objects in Context\n",
      "  Attempt 1/2: https://link.springer.com/content/pdf/10.1007%2F978-3-319-10602-1_48.pdf\n",
      "    Downloaded: 2,887,169 bytes -> paper_1_b6ad4f9b.pdf\n",
      "    Success! 16 pages, 2.75 MB\n",
      "\n",
      "[2/3] Downloading: The CLUSTAL_X windows interface: flexible strategies for mul...\n",
      "  Attempt 1/2: https://europepmc.org/articles/pmc147148?pdf=render\n",
      "    Downloaded: 365,554 bytes -> paper_2_19dc86c4.pdf\n",
      "    Success! 7 pages, 0.35 MB\n",
      "\n",
      "[3/3] Downloading: Solar water splitting cells.\n",
      "  Attempt 1/2: https://pubs.acs.org/doi/pdf/10.1021/cr1002326\n",
      "    HTTP 403 - Forbidden (site blocked this request).\n",
      "    Failed to download.\n",
      "\n",
      "Download report saved to: data\\reports\\download_report_20251224_230836.json\n",
      "\n",
      "============================================================\n",
      "VERIFICATION OF DOWNLOADS\n",
      "============================================================\n",
      "\n",
      "Directory: C:\\Users\\abhis\\Downloads\n",
      "PDF files found: 169\n",
      "\n",
      "File Details:\n",
      "------------------------------------------------------------\n",
      " 1-13b14a12-c985-4f52-bbfc-39b514484867.pdf\n",
      "   Size: 119,362 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " 1-5ec73332-9ef2-4a09-a34a-1e302d50c9bc.pdf\n",
      "   Size: 120,953 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " 1-6f868a5f-2513-4341-8bd1-b271a25e6b11.pdf\n",
      "   Size: 119,125 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " 1-9ff001e7-9a8b-4908-9642-765c3b4c2041.pdf\n",
      "   Size: 121,815 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " 3rd_year_results.pdf\n",
      "   Size: 1,447,581 bytes (1.38 MB)\n",
      "   Pages: 2\n",
      " 4th sem dmc.pdf\n",
      "   Size: 209,228 bytes (0.20 MB)\n",
      "   Pages: 1\n",
      " 6773934a7cb3e_faraz_sde2_amazon.pdf\n",
      "   Size: 146,004 bytes (0.14 MB)\n",
      "   Pages: 2\n",
      " 6th_sem_results.pdf\n",
      "   Size: 641,138 bytes (0.61 MB)\n",
      "   Pages: 1\n",
      " 7-Day No-Oven Meal Plan.pdf\n",
      "   Size: 228,021 bytes (0.22 MB)\n",
      "   Pages: 14\n",
      " aadhaar (1).pdf\n",
      "   Size: 167,021 bytes (0.16 MB)\n",
      "   Pages: 1\n",
      " aadhaar.pdf\n",
      "   Size: 167,021 bytes (0.16 MB)\n",
      "   Pages: 1\n",
      " Aadhar Card.pdf\n",
      "   Size: 120,295 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " abhinav final.pdf\n",
      "   Size: 887,277 bytes (0.85 MB)\n",
      "   Pages: 17\n",
      " Abhinav Sharma.pdf\n",
      "   Size: 64,594 bytes (0.06 MB)\n",
      "   Pages: 1\n",
      " Abhinav.pdf\n",
      "   Size: 69,176 bytes (0.07 MB)\n",
      "   Pages: 1\n",
      " Abhinav_Aug_2024_internship_40430.pdf\n",
      "   Size: 221,860 bytes (0.21 MB)\n",
      "   Pages: 1\n",
      " Abhinav_Jan_2023_training_40430.pdf\n",
      "   Size: 528,427 bytes (0.50 MB)\n",
      "   Pages: 1\n",
      " Abhinav_Sharma_AI_ResearchBrief.pdf\n",
      "   Size: 122,950 bytes (0.12 MB)\n",
      "   Pages: 2\n",
      " Abhinav_Sharma_IBM_Cloud_Project.pdf\n",
      "   Size: 798,637 bytes (0.76 MB)\n",
      "   Pages: 17\n",
      " AbhinavSharma_AI-AgentAssignment.pdf\n",
      "   Size: 222,217 bytes (0.21 MB)\n",
      "   Pages: 8\n",
      " ADE.pdf\n",
      "   Size: 226,238 bytes (0.22 MB)\n",
      "   Pages: 1\n",
      " Agile Scrum In Practice.pdf\n",
      "   Size: 121,112 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " AI System to Automatically Review and Summarize Research Papers.pdf\n",
      "   Size: 220,432 bytes (0.21 MB)\n",
      "   Pages: 3\n",
      " AI_INT_ABHINAV_SHARMA.pdf\n",
      "   Size: 280,652 bytes (0.27 MB)\n",
      "   Pages: 11\n",
      " Akan icar.pdf\n",
      "   Size: 436,367 bytes (0.42 MB)\n",
      "   Pages: 3\n",
      " Akan Rank card.pdf\n",
      "   Size: 190,077 bytes (0.18 MB)\n",
      "   Pages: 1\n",
      " Akanksha_Sharma.pdf\n",
      "   Size: 3,183,287 bytes (3.04 MB)\n",
      "   Pages: 18\n",
      " answers1.pdf\n",
      "   Size: 244,009 bytes (0.23 MB)\n",
      "   Pages: 6\n",
      " ARJUN PRASAD BILL.pdf\n",
      "   Size: 130,556 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " ARTIFICIAL INTELLIGENCE 1.pdf\n",
      "   Size: 2,228,903 bytes (2.13 MB)\n",
      "   Pages: 19\n",
      " ARTIFICIAL INTELLIGENCE 12.pdf\n",
      "   Size: 1,350,572 bytes (1.29 MB)\n",
      "   Pages: 12\n",
      " Artificial Intelligence.pdf\n",
      "   Size: 121,444 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " automted_cpu_designing.pdf\n",
      "   Size: 2,827,298 bytes (2.70 MB)\n",
      "   Pages: 11\n",
      " Azure AI Fundamentals.pdf\n",
      "   Size: 297,422 bytes (0.28 MB)\n",
      "   Pages: 1\n",
      " Bitcoin_Market_Analysis.pdf\n",
      "   Size: 532,225 bytes (0.51 MB)\n",
      "   Pages: 8\n",
      " Bonafide Certificate .pdf\n",
      "   Size: 787,206 bytes (0.75 MB)\n",
      "   Pages: 1\n",
      " Bonafide certificate.pdf\n",
      "   Size: 271,883 bytes (0.26 MB)\n",
      "   Pages: 1\n",
      " Bonafied Certificate 2.pdf\n",
      "   Size: 702,439 bytes (0.67 MB)\n",
      "   Pages: 1\n",
      " BRCCO_2024_11051989.pdf\n",
      "   Size: 81,205 bytes (0.08 MB)\n",
      "   Pages: 2\n",
      " CAIR.pdf\n",
      "   Size: 227,160 bytes (0.22 MB)\n",
      "   Pages: 1\n",
      " case_study_1_2243021_GEC_Abhinav_Sharma.pdf\n",
      "   Size: 149,612 bytes (0.14 MB)\n",
      "   Pages: 5\n",
      " case_study_2_2243021_GEC_Abhinav_Sharma.pdf\n",
      "   Size: 383,373 bytes (0.37 MB)\n",
      "   Pages: 7\n",
      " case_study_3_2243021_GEC_Abhinav_Sharma.pdf\n",
      "   Size: 169,655 bytes (0.16 MB)\n",
      "   Pages: 6\n",
      " Certificate EWS.pdf\n",
      "   Size: 129,078 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " Certificate.pdf\n",
      "   Size: 606,740 bytes (0.58 MB)\n",
      "   Pages: 1\n",
      " Class X Certificate (1).pdf\n",
      "   Size: 286,564 bytes (0.27 MB)\n",
      "   Pages: 1\n",
      " Class X Certificate.pdf\n",
      "   Size: 286,564 bytes (0.27 MB)\n",
      "   Pages: 1\n",
      " Class XII Certificate (1).pdf\n",
      "   Size: 113,399 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " Class XII Certificate.pdf\n",
      "   Size: 113,399 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " Coincent AI Certificate.pdf\n",
      "   Size: 409,662 bytes (0.39 MB)\n",
      "   Pages: 1\n",
      " Coincent Assignment 2.pdf\n",
      "   Size: 527,039 bytes (0.50 MB)\n",
      "   Pages: 7\n",
      " collections problems.pdf\n",
      "   Size: 166,761 bytes (0.16 MB)\n",
      "   Pages: 2\n",
      " collections.pdf\n",
      "   Size: 354,907 bytes (0.34 MB)\n",
      "   Pages: 60\n",
      " College_ID (1).pdf\n",
      "   Size: 337,331 bytes (0.32 MB)\n",
      "   Pages: 1\n",
      " College_ID.pdf\n",
      "   Size: 337,331 bytes (0.32 MB)\n",
      "   Pages: 1\n",
      " collegeid_yogi.pdf\n",
      "   Size: 835,123 bytes (0.80 MB)\n",
      "   Pages: 1\n",
      " Completion Certificate 1 _ SkillsBuild.pdf\n",
      "   Size: 25,013 bytes (0.02 MB)\n",
      "   Pages: 1\n",
      " Completion Certificate _ SkillsBuild.pdf\n",
      "   Size: 23,843 bytes (0.02 MB)\n",
      "   Pages: 1\n",
      " counceling.pdf\n",
      "   Size: 436,368 bytes (0.42 MB)\n",
      "   Pages: 3\n",
      " cover_letter_abhinav_sharma.pdf\n",
      "   Size: 104,497 bytes (0.10 MB)\n",
      "   Pages: 1\n",
      " cover_letter_abhinav_sharma_a.pdf\n",
      "   Size: 94,739 bytes (0.09 MB)\n",
      "   Pages: 1\n",
      " cover_letter_abhinav_sharma_fam.pdf\n",
      "   Size: 97,819 bytes (0.09 MB)\n",
      "   Pages: 1\n",
      " cover_letter_abhinav_sharma_m.pdf\n",
      "   Size: 105,232 bytes (0.10 MB)\n",
      "   Pages: 1\n",
      " cover_letter_abhinavsharma.pdf\n",
      "   Size: 57,130 bytes (0.05 MB)\n",
      "   Pages: 1\n",
      " cover_letter_h.pdf\n",
      "   Size: 115,050 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " Cover_letter_TGI.pdf\n",
      "   Size: 63,617 bytes (0.06 MB)\n",
      "   Pages: 1\n",
      " CreaNova.pdf\n",
      "   Size: 936,159 bytes (0.89 MB)\n",
      "   Pages: 12\n",
      " daily_dairy_aditya.pdf\n",
      "   Size: 223,819 bytes (0.21 MB)\n",
      "   Pages: 12\n",
      " day_1.pdf\n",
      "   Size: 163,873 bytes (0.16 MB)\n",
      "   Pages: 10\n",
      " Deep Learning for developers.pdf\n",
      "   Size: 121,212 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " Deloitte_certificate.pdf\n",
      "   Size: 123,359 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " Diode Characteristics.pdf\n",
      "   Size: 377,364 bytes (0.36 MB)\n",
      "   Pages: 5\n",
      " DocScanner Sep 19, 2025 3-14 PM.pdf\n",
      "   Size: 2,062,931 bytes (1.97 MB)\n",
      "   Pages: 3\n",
      " domicile cert.pdf\n",
      "   Size: 141,550 bytes (0.13 MB)\n",
      "   Pages: 1\n",
      " DSA_routine.pdf\n",
      "   Size: 255,575 bytes (0.24 MB)\n",
      "   Pages: 5\n",
      " Edunet_Certificate.pdf\n",
      "   Size: 383,123 bytes (0.37 MB)\n",
      "   Pages: 1\n",
      " Edunet_Intern_Certificate.pdf\n",
      "   Size: 585,614 bytes (0.56 MB)\n",
      "   Pages: 1\n",
      " G438U85ApplicationForm.pdf\n",
      "   Size: 353,683 bytes (0.34 MB)\n",
      "   Pages: 1\n",
      " Gahi public.pdf\n",
      "   Size: 23,806 bytes (0.02 MB)\n",
      "   Pages: 1\n",
      " GATE.pdf\n",
      "   Size: 363,889 bytes (0.35 MB)\n",
      "   Pages: 2\n",
      " Gen AI Unleashing.pdf\n",
      "   Size: 121,219 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " Generative models for developers.pdf\n",
      "   Size: 121,362 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " HP Extended Warranty Card.pdf\n",
      "   Size: 68,643 bytes (0.07 MB)\n",
      "   Pages: 2\n",
      " HR predection.pdf\n",
      "   Size: 2,054,144 bytes (1.96 MB)\n",
      "   Pages: 18\n",
      " IBMDesign20250716-24-nywcqy.pdf\n",
      "   Size: 75,177 bytes (0.07 MB)\n",
      "   Pages: 1\n",
      " IBMDesign20250717-27-9bj7o7.pdf\n",
      "   Size: 73,524 bytes (0.07 MB)\n",
      "   Pages: 1\n",
      " ID card 2.pdf\n",
      "   Size: 197,205 bytes (0.19 MB)\n",
      "   Pages: 1\n",
      " Income Certificate.pdf\n",
      "   Size: 139,525 bytes (0.13 MB)\n",
      "   Pages: 1\n",
      " Indian Council of Agricultural Research report.pdf\n",
      "   Size: 241,567 bytes (0.23 MB)\n",
      "   Pages: 1\n",
      " INFOSYS -APTITUDE-MODEL PAPERS_1473177928759.pdf\n",
      "   Size: 351,028 bytes (0.33 MB)\n",
      "   Pages: 25\n",
      " Infosys Aptitude — Complete Cheat-sheet.pdf\n",
      "   Size: 40,801 bytes (0.04 MB)\n",
      "   Pages: 7\n",
      " Infosys-Previous-Year-Placement-Papers-skillvertex-1.pdf\n",
      "   Size: 118,862 bytes (0.11 MB)\n",
      "   Pages: 18\n",
      " Intern Assignment.pdf\n",
      "   Size: 114,989 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " invitation.pdf\n",
      "   Size: 465,648 bytes (0.44 MB)\n",
      "   Pages: 1\n",
      " ISTRAC.pdf\n",
      "   Size: 228,461 bytes (0.22 MB)\n",
      "   Pages: 1\n",
      " java notes.pdf\n",
      "   Size: 4,202,931 bytes (4.01 MB)\n",
      "   Pages: 11\n",
      " Java_DSA_routine.pdf\n",
      "   Size: 278,014 bytes (0.27 MB)\n",
      "   Pages: 7\n",
      " Lunch Recipes.pdf\n",
      "   Size: 222,554 bytes (0.21 MB)\n",
      "   Pages: 6\n",
      " Microsoft Azure AI-900 Certificate.pdf\n",
      "   Size: 70,128 bytes (0.07 MB)\n",
      "   Pages: 1\n",
      " Microsoft Azure Certificate.pdf\n",
      "   Size: 297,418 bytes (0.28 MB)\n",
      "   Pages: 1\n",
      " nikhil final.pdf\n",
      "   Size: 731,347 bytes (0.70 MB)\n",
      "   Pages: 17\n",
      " offer_letter.pdf\n",
      "   Size: 241,853 bytes (0.23 MB)\n",
      "   Pages: 3\n",
      " OOP.pdf\n",
      "   Size: 4,100,917 bytes (3.91 MB)\n",
      "   Pages: 7\n",
      " OpenAI GPT 3.pdf\n",
      "   Size: 122,554 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " pan_card (1).pdf\n",
      "   Size: 78,109 bytes (0.07 MB)\n",
      "   Pages: 1\n",
      " pan_card (2).pdf\n",
      "   Size: 143,971 bytes (0.14 MB)\n",
      "   Pages: 1\n",
      "MuPDF error: format error: corrupt object stream (11 0 R)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: format error: corrupt object stream (11 0 R)\n",
      "\n",
      " pan_card.pdf\n",
      "   Size: 722,939 bytes (0.69 MB)\n",
      "   Pages: 0\n",
      " paper_1_b6ad4f9b.pdf\n",
      "   Size: 2,887,169 bytes (2.75 MB)\n",
      "   Pages: 16\n",
      " paper_2_19dc86c4.pdf\n",
      "   Size: 365,554 bytes (0.35 MB)\n",
      "   Pages: 7\n",
      " paper_2_1c08f086.pdf\n",
      "   Size: 710,934 bytes (0.68 MB)\n",
      "   Pages: 7\n",
      " paper_3_829b5d7b.pdf\n",
      "   Size: 184,494 bytes (0.18 MB)\n",
      "   Pages: 1\n",
      " passbook.pdf\n",
      "   Size: 131,244 bytes (0.13 MB)\n",
      "   Pages: 1\n",
      " Policy.pdf\n",
      "   Size: 422,201 bytes (0.40 MB)\n",
      "   Pages: 6\n",
      " Practice problems.pdf\n",
      "   Size: 4,021,126 bytes (3.83 MB)\n",
      "   Pages: 2\n",
      " practice questions.pdf\n",
      "   Size: 155,295 bytes (0.15 MB)\n",
      "   Pages: 3\n",
      " practice_ques.pdf\n",
      "   Size: 176,469 bytes (0.17 MB)\n",
      "   Pages: 1\n",
      " prompt engineering.pdf\n",
      "   Size: 121,171 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " Provisional Allotment Letter.pdf\n",
      "   Size: 202,707 bytes (0.19 MB)\n",
      "   Pages: 1\n",
      " Python_Complete_Notes.pdf\n",
      "   Size: 27,330,548 bytes (26.06 MB)\n",
      "   Pages: 64\n",
      " questions.pdf\n",
      "   Size: 180,938 bytes (0.17 MB)\n",
      "   Pages: 2\n",
      " questionsa.pdf\n",
      "   Size: 203,489 bytes (0.19 MB)\n",
      "   Pages: 1\n",
      " questionsb.pdf\n",
      "   Size: 221,520 bytes (0.21 MB)\n",
      "   Pages: 1\n",
      " questionsc.pdf\n",
      "   Size: 199,770 bytes (0.19 MB)\n",
      "   Pages: 2\n",
      " RADAR_Grp12.pdf\n",
      "   Size: 5,951,863 bytes (5.68 MB)\n",
      "   Pages: 13\n",
      " Reciept.pdf\n",
      "   Size: 124,400 bytes (0.12 MB)\n",
      "   Pages: 1\n",
      " Recommendation letter  Abinav 3rd year.pdf\n",
      "   Size: 266,028 bytes (0.25 MB)\n",
      "   Pages: 1\n",
      " Reliance Scholarship Application.pdf\n",
      "   Size: 563,901 bytes (0.54 MB)\n",
      "   Pages: 3\n",
      " reply.pdf\n",
      "   Size: 193,510 bytes (0.18 MB)\n",
      "   Pages: 2\n",
      " report_cover.pdf\n",
      "   Size: 274,060 bytes (0.26 MB)\n",
      "   Pages: 1\n",
      " Request Letter ISRO.pdf\n",
      "   Size: 558,810 bytes (0.53 MB)\n",
      "   Pages: 2\n",
      " request_letter_ADE.pdf\n",
      "   Size: 560,025 bytes (0.53 MB)\n",
      "   Pages: 1\n",
      " Request_letter_CAIR.pdf\n",
      "   Size: 560,404 bytes (0.53 MB)\n",
      "   Pages: 1\n",
      " Request_Letter_URSC.pdf\n",
      "   Size: 560,501 bytes (0.53 MB)\n",
      "   Pages: 1\n",
      " Results.pdf\n",
      "   Size: 5,153,884 bytes (4.92 MB)\n",
      "   Pages: 6\n",
      " Results1.pdf\n",
      "   Size: 1,383,467 bytes (1.32 MB)\n",
      "   Pages: 6\n",
      " Resume_1.pdf\n",
      "   Size: 109,803 bytes (0.10 MB)\n",
      "   Pages: 1\n",
      " Resume_Abhi.pdf\n",
      "   Size: 109,967 bytes (0.10 MB)\n",
      "   Pages: 1\n",
      " Resume_Abhi1.pdf\n",
      "   Size: 109,967 bytes (0.10 MB)\n",
      "   Pages: 1\n",
      " resume_abhinav_sharma.pdf\n",
      "   Size: 115,605 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " resume_abhinav_sharma_.pdf\n",
      "   Size: 114,950 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " resume_abhinav_sharma_da.pdf\n",
      "   Size: 144,766 bytes (0.14 MB)\n",
      "   Pages: 1\n",
      " resume_abhinav_Sharma_da1.pdf\n",
      "   Size: 144,947 bytes (0.14 MB)\n",
      "   Pages: 1\n",
      " resume_abhinav_sharma_sd.pdf\n",
      "   Size: 114,166 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " resume_abhinav_sharma_sde.pdf\n",
      "   Size: 119,429 bytes (0.11 MB)\n",
      "   Pages: 1\n",
      " RFSCH230100016009.pdf\n",
      "   Size: 546,493 bytes (0.52 MB)\n",
      "   Pages: 9\n",
      " round 3.pdf\n",
      "   Size: 437,203 bytes (0.42 MB)\n",
      "   Pages: 3\n",
      " scholar_report.pdf\n",
      "   Size: 259,136 bytes (0.25 MB)\n",
      "   Pages: 1\n",
      " school leaving.pdf\n",
      "   Size: 381,927 bytes (0.36 MB)\n",
      "   Pages: 1\n",
      " sem_res.pdf\n",
      "   Size: 799,146 bytes (0.76 MB)\n",
      "   Pages: 1\n",
      " Sem_results.pdf\n",
      "   Size: 3,986,811 bytes (3.80 MB)\n",
      "   Pages: 5\n",
      " semester_results.pdf\n",
      "   Size: 2,180,514 bytes (2.08 MB)\n",
      "   Pages: 6\n",
      " SIH Project.pdf\n",
      "   Size: 282,363 bytes (0.27 MB)\n",
      "   Pages: 13\n",
      " SIH2024_IDEA_Presentation.pdf\n",
      "   Size: 415,390 bytes (0.40 MB)\n",
      "   Pages: 6\n",
      " SIH_Nomination24-pages-13.pdf\n",
      "   Size: 365,376 bytes (0.35 MB)\n",
      "   Pages: 1\n",
      " solutions.pdf\n",
      "   Size: 265,527 bytes (0.25 MB)\n",
      "   Pages: 12\n",
      " sql_advanced certificate.pdf\n",
      "   Size: 7,683,641 bytes (7.33 MB)\n",
      "   Pages: 1\n",
      " survey_report[1].pdf\n",
      "   Size: 1,001,267 bytes (0.95 MB)\n",
      "   Pages: 18\n",
      " The Art of Computer Programming (vol. 1_ Fundamental Algorithms) (3rd ed.) [Knuth 1997-07-17].pdf\n",
      "   Size: 38,607,723 bytes (36.82 MB)\n",
      "   Pages: 666\n",
      " The Ultimate Python Handbook.pdf\n",
      "   Size: 1,728,887 bytes (1.65 MB)\n",
      "   Pages: 61\n",
      " Training_Repoert.pdf\n",
      "   Size: 525,073 bytes (0.50 MB)\n",
      "   Pages: 20\n",
      " training_reportfergf.pdf\n",
      "   Size: 487,179 bytes (0.46 MB)\n",
      "   Pages: 40\n",
      " UNIT-I (1).pdf\n",
      "   Size: 1,701,831 bytes (1.62 MB)\n",
      "   Pages: 83\n",
      " UNIT-II (1).pdf\n",
      "   Size: 762,777 bytes (0.73 MB)\n",
      "   Pages: 35\n",
      " UNIT-III (1).pdf\n",
      "   Size: 1,087,828 bytes (1.04 MB)\n",
      "   Pages: 32\n",
      " UNIT-IV (1).pdf\n",
      "   Size: 803,816 bytes (0.77 MB)\n",
      "   Pages: 31\n",
      " UNIT-V (1).pdf\n",
      "   Size: 1,025,925 bytes (0.98 MB)\n",
      "   Pages: 32\n",
      " URSC.pdf\n",
      "   Size: 227,205 bytes (0.22 MB)\n",
      "   Pages: 1\n",
      " Variables.pdf\n",
      "   Size: 4,042,146 bytes (3.85 MB)\n",
      "   Pages: 2\n",
      " YC.pdf\n",
      "   Size: 4,237,285 bytes (4.04 MB)\n",
      "   Pages: 13\n",
      "\n",
      "Summary:\n",
      "  • Total PDF files: 169\n",
      "  • Valid PDFs: 169\n",
      "  • Total size: 169.42 MB\n",
      "\n",
      "Module 2 complete!\n",
      "  Downloaded papers are in: downloads/\n",
      "  Report saved to: data\\reports\\download_report_20251224_230836.json\n"
     ]
    }
   ],
   "source": [
    "def main_download(filepath: Optional[str] = None, download_count: int = 3) -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Main entry for Module 2: load search results, select top papers, download them, save a report.\n",
    "\n",
    "    Args:lepath: optional specific search results JSON file to load.\n",
    "        download_count: number of top papers to download.\n",
    "\n",
    "    Returns:\n",
    "        List of downloaded paper metadata, or None if pipeline could not proceed.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"MODULE 2: PAPER SELECTION & PDF DOWNLOAD\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    data = load_search_results(filepath)\n",
    "    if not data:\n",
    "        return None\n",
    "\n",
    "    selected_papers = select_top_papers(data.get(\"papers\", []), count=download_count)\n",
    "    if not selected_papers:\n",
    "        print(\"No papers with PDFs available for download.\")\n",
    "        return None\n",
    "\n",
    "    downloaded = download_selected_papers(selected_papers)\n",
    "    report_file = save_download_report(downloaded, data.get(\"topic\", \"unknown\"))\n",
    "    verify_downloads()\n",
    "\n",
    "    print(f\"\\nModule 2 complete!\")\n",
    "    print(f\"  Downloaded papers are in: downloads/\")\n",
    "    print(f\"  Report saved to: {report_file}\")\n",
    "\n",
    "    return downloaded\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_download(download_count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5dcc8c",
   "metadata": {},
   "source": [
    "# Module 3 : PDF Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e331ce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================\n",
      "PDF TEXT EXTRACTION MODULE\n",
      "========================================================================\n",
      "\n",
      "STEP 1: Extracting text from PDFs...\n",
      "\n",
      "========================================================================\n",
      "MODULE 3: PDF TEXT EXTRACTION\n",
      "========================================================================\n",
      "Using PDFs from: C:\\Users\\abhis\\Downloads\\research_papers\n",
      "\n",
      "Processing 2 PDF file(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing PDFs:   0%|                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: paper_1_b6ad4f9b.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing PDFs:  50%|█████████████████████████████████▌                                 | 1/2 [00:04<00:04,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 44,237 characters\n",
      "   Found 5 meaningful sections\n",
      "    • abstract: 2,779 chars\n",
      "    • introduction: 4,303 chars\n",
      "    • methods: 5,000 chars\n",
      "\n",
      "Processing: paper_2_19dc86c4.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 35,094 characters\n",
      "   Found 5 meaningful sections\n",
      "    • abstract: 1,045 chars\n",
      "    • introduction: 3,243 chars\n",
      "    • methods: 5,000 chars\n",
      "   Saved: paper_1_b6ad4f9b_extracted.json\n",
      "   Saved: paper_2_19dc86c4_extracted.json\n",
      "\n",
      "Summary saved to: data\\extracted\\extraction_summary.json\n",
      "\n",
      "Extraction complete!\n",
      "  Successfully processed: 2\n",
      "  Skipped: 0\n",
      "\n",
      "STEP 2: Analyzing extraction quality...\n",
      "\n",
      "========================================================================\n",
      "EXTRACTION ANALYSIS\n",
      "========================================================================\n",
      "\n",
      "Found 4 extracted papers:\n",
      "\n",
      " paper_1_b6ad4f9b\n",
      "   Size: 2,887,169 bytes\n",
      "   Text: 44,237 chars\n",
      "   Sections found: 5\n",
      "   Abstract: # **Microsoft COCO: Common Objects in Context** Tsung-Yi Lin [1], Michael Maire [2], Serge Belongie [1], James Hays [3], Pietro Perona [2], Deva Raman...\n",
      "\n",
      " paper_2_19dc86c4\n",
      "   Size: 365,554 bytes\n",
      "   Text: 35,094 chars\n",
      "   Sections found: 5\n",
      "   Abstract: Thompson, Toby J. Gibson [1], Frédéric Plewniak, François Jeanmougin* and** **Desmond G. Higgins** **[2]** Institut de Genetique et de Biologie Molecu...\n",
      "\n",
      " paper_2_1c08f086\n",
      "   Size: 710,934 bytes\n",
      "   Text: 48,752 chars\n",
      "   Sections found: 5\n",
      "   Abstract: Energy security, national security, environmental security, and economic security can likely be met only through addressing the energy problem within ...\n",
      "\n",
      " paper_3_829b5d7b\n",
      "   Size: 184,494 bytes\n",
      "   Text: 1,281 chars\n",
      "   Sections found: 1\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXTRACTION SUMMARY\n",
      "============================================================\n",
      "Total papers processed: 4\n",
      "Total characters extracted: 129,364\n",
      "Papers with abstract: 3/4\n",
      "Papers with multiple sections: 3/4\n",
      "\n",
      "STEP 3: Generating review report...\n",
      "\n",
      "========================================================================\n",
      "  REVIEW REPORT\n",
      "========================================================================\n",
      "\n",
      "report generated!\n",
      "  Overall score: 14/16\n",
      "  Success rate: 87.5%\n",
      "  Report saved to: data\\extracted\\_review_report.json\n",
      "\n",
      "QUALITY CHECK SUMMARY:\n",
      "----------------------------------------\n",
      "✅ text_clean: 3/4 (75%)\n",
      "✅ sections_correct: 3/4 (75%)\n",
      "✅ no_hallucinations: 4/4 (100%)\n",
      "✅ no_missing_chunks: 4/4 (100%)\n",
      "\n",
      "========================================================================\n",
      "COMPLETE!\n",
      "========================================================================\n",
      "\n",
      "========================================================================\n",
      "EXAMPLE OF EXTRACTED CONTENT\n",
      "========================================================================\n",
      "\n",
      "Paper: paper_1_b6ad4f9b\n",
      "\n",
      "ABSTRACT:\n",
      "----------------------------------------\n",
      "# **Microsoft COCO: Common Objects in Context** Tsung-Yi Lin [1], Michael Maire [2], Serge Belongie [1], James Hays [3], Pietro Perona [2], Deva Ramanan [4], Piotr Doll´ar [5], and C. Lawrence Zitnick [5] 1 Cornell 2 Caltech 3 Brown 4 UC Irvine 5 Microsoft Research **Abstract.** We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved...\n",
      "[Total length: 2,779 characters]\n",
      "\n",
      "INTRODUCTION:\n",
      "----------------------------------------\n",
      "We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. **1** **Introduction** One of the primary goals of computer vision is the understanding of visual scenes. Scene understanding involves numerous tasks including recognizing what objects are present, localizing the objects in 2D and 3D, determining the objects’ and ...\n",
      "[Total length: 4,303 characters]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MODULE 3: PDF TEXT EXTRACTION\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    import fitz\n",
    "    pymupdf = fitz\n",
    "except Exception as exc:\n",
    "    raise ImportError(\"PyMuPDF (fitz) is required for Module 3. Install with `pip install pymupdf`.\") from exc\n",
    "\n",
    "try:\n",
    "    import pymupdf4llm\n",
    "    HAS_PYMUPDF4LLM = True\n",
    "except Exception:\n",
    "    pymupdf4llm = None\n",
    "    HAS_PYMUPDF4LLM = False\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. TEXT EXTRACTION\n",
    "# -------------------------\n",
    "def extract_text_improved(pdf_path: Path) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Improved text extraction that tries layout-aware markdown first (if available),\n",
    "    otherwise performs robust plain-text extraction with page limits and heuristics.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (Path): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str or None: Extracted text (possibly markdown). None if extraction failed or PDF appears restricted.\n",
    "    \"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    if not pdf_path.exists():\n",
    "        print(f\"  File not found: {pdf_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        doc = pymupdf.open(str(pdf_path))\n",
    "\n",
    "        if getattr(doc, \"isEncrypted\", False):\n",
    "            print(f\"  PDF appears encrypted: {pdf_path.name}. Attempting extraction...\")\n",
    "\n",
    "        first_page_text = \"\"\n",
    "        if len(doc) > 0:\n",
    "            try:\n",
    "                first_page_text = doc[0].get_text().strip()\n",
    "            except Exception:\n",
    "                first_page_text = \"\"\n",
    "\n",
    "        copyright_indicators = [\"removed\", \"deleted\", \"takedown\", \"not available\"]\n",
    "        if first_page_text and any(keyword in first_page_text.lower() for keyword in copyright_indicators):\n",
    "            print(f\"  PDF appears to contain takedown/copyright notice: {pdf_path.name}\")\n",
    "            doc.close()\n",
    "            return None\n",
    "\n",
    "        extracted_candidates: List[tuple] = []\n",
    "\n",
    "        if HAS_PYMUPDF4LLM:\n",
    "            try:\n",
    "                md_text = pymupdf4llm.to_markdown(str(pdf_path))\n",
    "                if md_text and len(md_text) > 500:\n",
    "                    extracted_candidates.append((\"markdown\", md_text))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        plain_text = []\n",
    "\n",
    "        pages_to_process = min(50, len(doc))\n",
    "        for page_no in range(pages_to_process):\n",
    "            try:\n",
    "                page = doc[page_no]\n",
    "                p_text = page.get_text()\n",
    "                if p_text:\n",
    "                    plain_text.append(p_text)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        full_text = \"\\n\".join(plain_text).strip()\n",
    "        if full_text and len(full_text) > 500:\n",
    "            extracted_candidates.append((\"regular\", full_text))\n",
    "\n",
    "        doc.close()\n",
    "\n",
    "        if not extracted_candidates:\n",
    "            return None\n",
    "\n",
    "        for method, text in extracted_candidates:\n",
    "            if method == \"markdown\" and len(text) > 1000:\n",
    "                return text\n",
    "\n",
    "        best_text = max(extracted_candidates, key=lambda x: len(x[1]))[1]\n",
    "        return best_text\n",
    "\n",
    "    except Exception as exc:\n",
    "        print(f\"  Extraction error for {pdf_path.name}: {str(exc)[:200]}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. SECTION EXTRACTION\n",
    "# -------------------------\n",
    "def extract_sections_improved(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract standard paper sections (title, abstract, introduction, methods, results, conclusion, references)\n",
    "    using header heuristics and keyword fallbacks.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full extracted text from a PDF.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping of section names to content plus an 'extracted_text' preview.\n",
    "    \"\"\"\n",
    "    sections = {\n",
    "        \"title\": \"\",\n",
    "        \"abstract\": \"\",\n",
    "        \"introduction\": \"\",\n",
    "        \"methods\": \"\",\n",
    "        \"results\": \"\",\n",
    "        \"conclusion\": \"\",\n",
    "        \"references\": \"\",\n",
    "        \"extracted_text\": text[:20000] if text else \"\"\n",
    "    }\n",
    "\n",
    "    if not text or len(text) < 500:\n",
    "        return sections\n",
    "\n",
    "    clean = clean_text_basic(text)\n",
    "    lines = clean.splitlines()\n",
    "\n",
    "    header_patterns = {\n",
    "        \"abstract\": [r'\\babstract\\b', r'\\bsummary\\b'],\n",
    "        \"introduction\": [r'^\\d+\\.\\s*introduction\\b', r'\\bintroduction\\b', r'\\bbackground\\b'],\n",
    "        \"methods\": [r'^\\d+\\.\\s*methods?\\b', r'\\bmethods?\\b', r'\\bmethodology\\b', r'\\bexperimental\\b'],\n",
    "        \"results\": [r'^\\d+\\.\\s*results?\\b', r'\\bresults?\\b', r'\\bfindings?\\b'],\n",
    "        \"conclusion\": [r'^\\d+\\.\\s*conclusions?\\b', r'\\bconclusions?\\b', r'\\bdiscussion\\b'],\n",
    "        \"references\": [r'^\\s*references\\s*$', r'\\bbibliography\\b']\n",
    "    }\n",
    "\n",
    "    # Find line indices for section headers\n",
    "    boundaries: Dict[str, int] = {}\n",
    "    for idx, raw_line in enumerate(lines):\n",
    "        line = raw_line.strip()\n",
    "        low = line.lower()\n",
    "\n",
    "        if len(line) > 200:\n",
    "            continue\n",
    "        for section_key, patterns in header_patterns.items():\n",
    "            for pat in patterns:\n",
    "                try:\n",
    "                    if re.search(pat, low):\n",
    "                        # Record first occurrence\n",
    "                        if section_key not in boundaries:\n",
    "                            boundaries[section_key] = idx\n",
    "                            break\n",
    "                except re.error:\n",
    "                    continue\n",
    "\n",
    "    if boundaries:\n",
    "        sorted_sections = sorted(boundaries.items(), key=lambda x: x[1])\n",
    "        for i, (section_name, start_idx) in enumerate(sorted_sections):\n",
    "            start = start_idx + 1\n",
    "            if i + 1 < len(sorted_sections):\n",
    "                end = sorted_sections[i + 1][1]\n",
    "            else:\n",
    "                end = len(lines)\n",
    "            content = \"\\n\".join(lines[start:end]).strip()\n",
    "            if len(content) > 100:\n",
    "                sections[section_name] = content[:5000] \n",
    "\n",
    "    for line in lines[:10]:\n",
    "        line_stripped = line.strip()\n",
    "        if 20 < len(line_stripped) < 200 and not line_stripped.lower().startswith(\"http\"):\n",
    "            sections[\"title\"] = line_stripped\n",
    "            break\n",
    "\n",
    "    major_keys = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]\n",
    "    if not any(len(sections[k]) > 200 for k in major_keys):\n",
    "        sections = extract_by_keywords_fallback(clean, sections)\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "def extract_by_keywords_fallback(text: str, existing_sections: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fallback strategy: look for keywords for each section and return contextual snippets.\n",
    "\n",
    "    Args:\n",
    "        text (str): Full text.\n",
    "        existing_sections (dict): Current sections dict to update.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated sections mapping.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    section_keywords = {\n",
    "        \"abstract\": [\"abstract\", \"summary\", \"we present\", \"this paper\"],\n",
    "        \"introduction\": [\"introduction\", \"background\", \"motivation\", \"related work\"],\n",
    "        \"methods\": [\"method\", \"experiment\", \"procedure\", \"dataset\", \"implementation\"],\n",
    "        \"results\": [\"result\", \"finding\", \"table\", \"figure\", \"experiment shows\"],\n",
    "        \"conclusion\": [\"conclusion\", \"discussion\", \"future work\", \"limitations\", \"summary\"]\n",
    "    }\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "    for section, keywords in section_keywords.items():\n",
    "        if existing_sections.get(section):\n",
    "            continue\n",
    "\n",
    "        contexts: List[str] = []\n",
    "        for idx, sentence in enumerate(sentences):\n",
    "            s_low = sentence.lower()\n",
    "            if any(kw in s_low for kw in keywords):\n",
    "                start = max(0, idx - 2)\n",
    "                end = min(len(sentences), idx + 5)\n",
    "                context = \" \".join(sentences[start:end])\n",
    "                contexts.append(context.strip())\n",
    "\n",
    "        if contexts:\n",
    "            existing_sections[section] = \" \".join(contexts)[:5000]\n",
    "\n",
    "    return existing_sections\n",
    "\n",
    "\n",
    "def clean_text_basic(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic cleaning to reduce PDF extraction noise:\n",
    "    - Normalize whitespace\n",
    "    - Fix common hyphenation across line breaks\n",
    "    - Remove non-printable characters (except newline)\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw extracted text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize whitespace and remove excessive breaks\n",
    "    t = re.sub(r'\\r\\n?', '\\n', text)\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "\n",
    "    # Fix hyphenation at end-of-line (common in PDFs)\n",
    "    t = re.sub(r'-\\s+', '', t)\n",
    "    t = re.sub(r'\\s*-\\s*', '-', t)\n",
    "\n",
    "    # Remove control characters except newline\n",
    "    t = \"\".join(ch for ch in t if ch == '\\n' or ord(ch) >= 32)\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. PAPER PROCESSING\n",
    "# -------------------------\n",
    "def process_paper_smart(pdf_path: Path) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validate PDF size, extract text, detect meaningful sections, and return structured metadata.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (Path): Path to a single PDF file.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: Result dict with metadata and sections, or None if skipped/failed.\n",
    "    \"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    print(f\"\\nProcessing: {pdf_path.name}\")\n",
    "\n",
    "    try:\n",
    "        file_size = pdf_path.stat().st_size\n",
    "    except Exception as exc:\n",
    "        print(f\"  Could not read file size: {exc}\")\n",
    "        return None\n",
    "\n",
    "    if file_size < 10_240:  # 10 KB\n",
    "        print(f\"  File too small ({file_size:,} bytes) — skipping\")\n",
    "        return None\n",
    "\n",
    "    raw_text = extract_text_improved(pdf_path)\n",
    "    if raw_text is None:\n",
    "        print(\"  Skipping — empty or restricted PDF\")\n",
    "        return None\n",
    "\n",
    "    if len(raw_text) < 1000:\n",
    "        print(f\"  Warning: extracted text very short ({len(raw_text):,} chars) — may be incomplete\")\n",
    "\n",
    "    print(f\"  Extracted {len(raw_text):,} characters\")\n",
    "\n",
    "    sections = extract_sections_improved(raw_text)\n",
    "\n",
    "    meaningful_sections = [\n",
    "        name for name, content in sections.items()\n",
    "        if name != \"extracted_text\" and content and len(content) > 200\n",
    "    ]\n",
    "\n",
    "    print(f\"   Found {len(meaningful_sections)} meaningful sections\")\n",
    "    for s in meaningful_sections[:3]:\n",
    "        print(f\"    • {s}: {len(sections[s]):,} chars\")\n",
    "\n",
    "    result = {\n",
    "        \"paper_id\": pdf_path.stem,\n",
    "        \"filename\": pdf_path.name,\n",
    "        \"file_size_bytes\": file_size,\n",
    "        \"total_characters\": len(raw_text),\n",
    "        \"meaningful_sections\": meaningful_sections,\n",
    "        \"sections\": sections,\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. MAIN EXTRACTION\n",
    "# -------------------------\n",
    "def extract_all_papers(download_dir: str = \"downloads/research_papers\", max_papers: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extract text and sections from all PDFs found in `download_dir`.\n",
    "\n",
    "    Default directory now looks in downloads/research_papers for organized storage.\n",
    "    Falls back to 'downloads' if the specified directory is missing or empty.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"MODULE 3: PDF TEXT EXTRACTION\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    pdf_files = get_downloaded_papers(download_dir)\n",
    "    if not pdf_files:\n",
    "        print(\"No PDFs found in the target directories. Run Module 2 or check paths.\")\n",
    "        return []\n",
    "\n",
    "    if max_papers:\n",
    "        pdf_files = pdf_files[:max_papers]\n",
    "\n",
    "    print(f\"\\nProcessing {len(pdf_files)} PDF file(s)...\")\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    skipped = 0\n",
    "\n",
    "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        try:\n",
    "            res = process_paper_smart(pdf_file)\n",
    "            if res:\n",
    "                results.append(res)\n",
    "            else:\n",
    "                skipped += 1\n",
    "        except Exception as exc:\n",
    "            print(f\"  Unexpected error processing {pdf_file.name}: {str(exc)[:200]}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "    if results:\n",
    "        save_results_final(results)\n",
    "\n",
    "    print(\"\\nExtraction complete!\")\n",
    "    print(f\"  Successfully processed: {len(results)}\")\n",
    "    print(f\"  Skipped: {skipped}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_downloaded_papers(download_dir: str = \"downloads/research_papers\") -> List[Path]:\n",
    "    \"\"\"\n",
    "    Return list of PDF paths.\n",
    "\n",
    "    Behavior:\n",
    "      - First checks the preferred directory (default 'downloads/research_papers').\n",
    "      - If that directory doesn't exist or contains no PDFs, falls back to 'downloads'.\n",
    "      - Returns a sorted list of Path objects for PDF files.\n",
    "    \"\"\"\n",
    "    preferred = Path(download_dir)\n",
    "    fallback = Path(\"downloads\")\n",
    "\n",
    "    def _list_pdfs(path: Path) -> List[Path]:\n",
    "        if not path.exists():\n",
    "            return []\n",
    "        return sorted(path.glob(\"*.pdf\"))\n",
    "\n",
    "    pdf_list = _list_pdfs(preferred)\n",
    "    if pdf_list:\n",
    "        print(f\"Using PDFs from: {preferred.resolve()}\")\n",
    "        return pdf_list\n",
    "\n",
    "    pdf_list = _list_pdfs(fallback)\n",
    "    if pdf_list:\n",
    "        print(f\"No PDFs in {preferred}. Falling back to: {fallback.resolve()}\")\n",
    "        return pdf_list\n",
    "\n",
    "    print(f\"No PDF files found in either {preferred} or {fallback}.\")\n",
    "    return []\n",
    "\n",
    "def save_results_final(results: List[Dict[str, Any]], output_dir: str = \"data/extracted\") -> None:\n",
    "    \"\"\"\n",
    "    Save per-paper JSON files and a summary extraction file.\n",
    "\n",
    "    Args:\n",
    "        results (list): List of result dictionaries.\n",
    "        output_dir (str): Directory to save outputs.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for res in results:\n",
    "        paper_id = res.get(\"paper_id\", \"unknown\")\n",
    "        output_file = output_path / f\"{paper_id}_extracted.json\"\n",
    "\n",
    "        if \"extracted_text\" in res.get(\"sections\", {}) and len(res[\"sections\"][\"extracted_text\"]) > 10_000:\n",
    "            res[\"sections\"][\"extracted_text\"] = res[\"sections\"][\"extracted_text\"][:10_000] + \"...[truncated]\"\n",
    "\n",
    "        with output_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(res, fh, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"   Saved: {output_file.name}\")\n",
    "\n",
    "    # Save a summary file\n",
    "    summary = {\n",
    "        \"extraction_date\": datetime.now().isoformat(),\n",
    "        \"total_papers\": len(results),\n",
    "        \"papers\": [\n",
    "            {\n",
    "                \"paper_id\": r[\"paper_id\"],\n",
    "                \"filename\": r[\"filename\"],\n",
    "                \"file_size\": r[\"file_size_bytes\"],\n",
    "                \"total_chars\": r[\"total_characters\"],\n",
    "                \"sections_found\": r[\"meaningful_sections\"]\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    summary_file = output_path / \"extraction_summary.json\"\n",
    "    with summary_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(summary, fh, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nSummary saved to: {summary_file}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. ANALYZE RESULTS\n",
    "# -------------------------\n",
    "def analyze_extraction_results() -> None:\n",
    "    \"\"\"\n",
    "    Read saved extracted JSON files and print an analysis summary.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"EXTRACTION ANALYSIS\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    data_path = Path(\"data/extracted\")\n",
    "    if not data_path.exists():\n",
    "        print(\"No extraction directory found\")\n",
    "        return\n",
    "\n",
    "    json_files = sorted(data_path.glob(\"*_extracted.json\"))\n",
    "    if not json_files:\n",
    "        print(\"No extracted paper files found\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFound {len(json_files)} extracted papers:\\n\")\n",
    "\n",
    "    total_chars = 0\n",
    "    papers_with_abstract = 0\n",
    "    papers_with_multiple_sections = 0\n",
    "\n",
    "    for jf in json_files:\n",
    "        try:\n",
    "            with jf.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                data = json.load(fh)\n",
    "\n",
    "            paper_id = data.get(\"paper_id\", \"Unknown\")\n",
    "            total_chars += data.get(\"total_characters\", 0)\n",
    "\n",
    "            sections = data.get(\"sections\", {})\n",
    "            meaningful_sections = data.get(\"meaningful_sections\", [])\n",
    "\n",
    "            if sections.get(\"abstract\") and len(sections[\"abstract\"]) > 200:\n",
    "                papers_with_abstract += 1\n",
    "\n",
    "            if len(meaningful_sections) >= 2:\n",
    "                papers_with_multiple_sections += 1\n",
    "\n",
    "            print(f\" {paper_id}\")\n",
    "            print(f\"   Size: {data.get('file_size_bytes', 0):,} bytes\")\n",
    "            print(f\"   Text: {data.get('total_characters', 0):,} chars\")\n",
    "            print(f\"   Sections found: {len(meaningful_sections)}\")\n",
    "\n",
    "            if sections.get(\"title\"):\n",
    "                print(f\"   Title: {sections['title'][:80]}\")\n",
    "\n",
    "            if sections.get(\"abstract\"):\n",
    "                print(f\"   Abstract: {sections['abstract'][:150]}...\")\n",
    "\n",
    "            print()\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\" Error reading {jf.name}: {str(exc)[:200]}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXTRACTION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total papers processed: {len(json_files)}\")\n",
    "    print(f\"Total characters extracted: {total_chars:,}\")\n",
    "    print(f\"Papers with abstract: {papers_with_abstract}/{len(json_files)}\")\n",
    "    print(f\"Papers with multiple sections: {papers_with_multiple_sections}/{len(json_files)}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. GENERATE REPORT\n",
    "# -------------------------\n",
    "def generate_report() -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a per-paper quality report and an aggregated review JSON that a mentor can inspect.\n",
    "\n",
    "    Returns:\n",
    "        dict: Report dictionary (also saved to data/extracted/_review_report.json)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"  REVIEW REPORT\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    data_path = Path(\"data/extracted\")\n",
    "    if not data_path.exists():\n",
    "        print(\" No extraction directory found\")\n",
    "        return None\n",
    "\n",
    "    json_files = sorted(data_path.glob(\"*_extracted.json\"))\n",
    "    if not json_files:\n",
    "        print(\" No extracted papers found\")\n",
    "        return None\n",
    "\n",
    "    report: Dict[str, Any] = {\n",
    "        \"generated_date\": datetime.now().isoformat(),\n",
    "        \"total_papers\": len(json_files),\n",
    "        \"quality_checks\": [],\n",
    "        \"papers\": []\n",
    "    }\n",
    "\n",
    "    for jf in json_files:\n",
    "        try:\n",
    "            with jf.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                data = json.load(fh)\n",
    "\n",
    "            sections = data.get(\"sections\", {})\n",
    "            paper_report = {\n",
    "                \"paper_id\": data.get(\"paper_id\"),\n",
    "                \"filename\": data.get(\"filename\"),\n",
    "                \"checks\": {\n",
    "                    \"text_clean\": False,\n",
    "                    \"sections_correct\": False,\n",
    "                    \"no_hallucinations\": False,\n",
    "                    \"no_missing_chunks\": False\n",
    "                },\n",
    "                \"section_lengths\": {},\n",
    "                \"issues\": []\n",
    "            }\n",
    "\n",
    "            sample_text = sections.get(\"abstract\") or sections.get(\"extracted_text\", \"\")\n",
    "            artifacts = ['�', '\\x00', '[?]', '[ ]']\n",
    "            has_artifacts = any(art in sample_text for art in artifacts)\n",
    "            paper_report[\"checks\"][\"text_clean\"] = not has_artifacts\n",
    "            if has_artifacts:\n",
    "                paper_report[\"issues\"].append(\"Text contains extraction artifacts\")\n",
    "\n",
    "            major_sections = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]\n",
    "            found_major = [s for s in major_sections if sections.get(s) and len(sections[s]) > 200]\n",
    "            paper_report[\"checks\"][\"sections_correct\"] = len(found_major) >= 2\n",
    "            if len(found_major) < 2:\n",
    "                paper_report[\"issues\"].append(f\"Only found {len(found_major)} major sections\")\n",
    "\n",
    "            total_chars = data.get(\"total_characters\", 0)\n",
    "            paper_report[\"checks\"][\"no_hallucinations\"] = 1000 <= total_chars <= 500_000\n",
    "            if total_chars < 1000:\n",
    "                paper_report[\"issues\"].append(f\"Text too short: {total_chars} chars\")\n",
    "            elif total_chars > 500_000:\n",
    "                paper_report[\"issues\"].append(f\"Text suspiciously long: {total_chars} chars\")\n",
    "\n",
    "            section_lengths_sum = sum(len(str(v)) for v in sections.values() if v)\n",
    "            coverage = section_lengths_sum / total_chars if total_chars > 0 else 0\n",
    "            paper_report[\"checks\"][\"no_missing_chunks\"] = coverage >= 0.3\n",
    "            if coverage < 0.3:\n",
    "                paper_report[\"issues\"].append(f\"Low coverage: {coverage:.1%}\")\n",
    "\n",
    "            for sec_name, content in sections.items():\n",
    "                if content and len(str(content)) > 50:\n",
    "                    paper_report[\"section_lengths\"][sec_name] = len(str(content))\n",
    "\n",
    "            report[\"papers\"].append(paper_report)\n",
    "        except Exception as exc:\n",
    "            print(f\"Error processing {jf.name}: {str(exc)[:200]}\")\n",
    "\n",
    "    # Aggregate overall scores\n",
    "    total_checks = 0\n",
    "    passed_checks = 0\n",
    "    for paper in report[\"papers\"]:\n",
    "        for check_name, passed in paper[\"checks\"].items():\n",
    "            total_checks += 1\n",
    "            if passed:\n",
    "                passed_checks += 1\n",
    "\n",
    "    report[\"overall_score\"] = f\"{passed_checks}/{total_checks}\" if total_checks > 0 else \"N/A\"\n",
    "    report[\"success_rate\"] = (passed_checks / total_checks) if total_checks > 0 else 0\n",
    "\n",
    "    report_file = data_path / \"_review_report.json\"\n",
    "    with report_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(report, fh, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nreport generated!\")\n",
    "    print(f\"  Overall score: {report['overall_score']}\")\n",
    "    print(f\"  Success rate: {report['success_rate']:.1%}\")\n",
    "    print(f\"  Report saved to: {report_file}\")\n",
    "\n",
    "    print(\"\\nQUALITY CHECK SUMMARY:\")\n",
    "    print(\"-\" * 40)\n",
    "    check_names = [\"text_clean\", \"sections_correct\", \"no_hallucinations\", \"no_missing_chunks\"]\n",
    "    for check_name in check_names:\n",
    "        passed_count = sum(1 for paper in report[\"papers\"] if paper[\"checks\"].get(check_name, False))\n",
    "        total = len(report[\"papers\"])\n",
    "        percentage = (passed_count / total * 100) if total > 0 else 0\n",
    "        status = \"✅\" if percentage >= 70 else \"⚠️\" if percentage >= 50 else \"❌\"\n",
    "        print(f\"{status} {check_name}: {passed_count}/{total} ({percentage:.0f}%)\")\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 7. RUN COMPLETE PIPELINE\n",
    "# -------------------------\n",
    "def run_complete_extraction() -> (List[Dict[str, Any]], Optional[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Full extraction pipeline entrypoint:\n",
    "      - Extract text from PDFs (up to a limit)\n",
    "      - Analyze saved results\n",
    "      - Generate mentor review report\n",
    "\n",
    "    Returns:\n",
    "        (results_list, report_dict)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"PDF TEXT EXTRACTION MODULE\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    print(\"\\nSTEP 1: Extracting text from PDFs...\")\n",
    "    results = extract_all_papers(max_papers=5)\n",
    "\n",
    "    if not results:\n",
    "        print(\"No papers extracted successfully.\")\n",
    "        return [], None\n",
    "\n",
    "    print(\"\\nSTEP 2: Analyzing extraction quality...\")\n",
    "    analyze_extraction_results()\n",
    "\n",
    "    print(\"\\nSTEP 3: Generating review report...\")\n",
    "    report = generate_report()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"COMPLETE!\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    return results, report\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, report = run_complete_extraction()\n",
    "\n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\" * 72)\n",
    "        print(\"EXAMPLE OF EXTRACTED CONTENT\")\n",
    "        print(\"=\" * 72)\n",
    "\n",
    "        first_paper = results[0]\n",
    "        sections = first_paper.get(\"sections\", {})\n",
    "\n",
    "        print(f\"\\nPaper: {first_paper.get('paper_id')}\")\n",
    "\n",
    "        for section_name in [\"title\", \"abstract\", \"introduction\"]:\n",
    "            content = sections.get(section_name)\n",
    "            if content and len(content) > 50:\n",
    "                preview = content[:500] + (\"...\" if len(content) > 500 else \"\")\n",
    "                print(f\"\\n{section_name.upper()}:\")\n",
    "                print(\"-\" * 40)\n",
    "                print(preview)\n",
    "                print(f\"[Total length: {len(content):,} characters]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad7cbf",
   "metadata": {},
   "source": [
    "#  Module 4 : Key-finding Extraction Logic and Cross-paper Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cc2d45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PAPER ANALYSIS MODULE\n",
      "================================================================================\n",
      "\n",
      "STEP 1: Loading extracted papers...\n",
      "Loading 4 extracted papers from data\\extracted...\n",
      "  ✓ paper_1_b6ad4f9b: 44,237 chars\n",
      "  ✓ paper_2_19dc86c4: 35,094 chars\n",
      "  ✓ paper_2_1c08f086: 48,752 chars\n",
      "  ✓ paper_3_829b5d7b: 1,281 chars\n",
      "\n",
      "STEP 2: Analyzing 4 papers for comparison...\n",
      "  ✓ paper_1_b6ad4f9b: 4 methods, 4 findings\n",
      "  ✓ paper_2_19dc86c4: 5 methods, 5 findings\n",
      "  ✓ paper_2_1c08f086: 4 methods, 5 findings\n",
      "  ✓ paper_3_829b5d7b: 0 methods, 0 findings\n",
      "\n",
      "STEP 3: Comparing papers...\n",
      "\n",
      "Comparing 4 papers...\n",
      "\n",
      "STEP 4: Calculating similarity scores...\n",
      "\n",
      "STEP 5: Saving comparison results...\n",
      "  Comparison saved to: data\\analysis\\comparison.json\n",
      "   Similarity scores saved to: data\\analysis\\similarity_scores.json\n",
      "  Comparison report saved to: data\\analysis\\comparison_report.txt\n",
      "\n",
      "CROSS-PAPER ANALYSIS COMPLETE!\n",
      "Files saved to: data\\analysis\n",
      "\n",
      "Comparison analysis completed.\n"
     ]
    }
   ],
   "source": [
    "# pip install scikit-learn numpy\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Optional, Tuple, Set\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. LOAD EXTRACTED PAPERS\n",
    "# -------------------------\n",
    "def load_extracted_papers(data_dir: str = \"data/extracted\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load all extracted paper JSON files from the given directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir: Directory containing `*_extracted.json` files.\n",
    "\n",
    "    Returns:\n",
    "        List of paper dictionaries (parsed JSON).\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    papers: List[Dict[str, Any]] = []\n",
    "\n",
    "    json_files = sorted(data_path.glob(\"*_extracted.json\"))\n",
    "    if not json_files:\n",
    "        print(\"No extracted papers found. Run Module 3 first.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Loading {len(json_files)} extracted papers from {data_path}...\")\n",
    "\n",
    "    for jf in json_files:\n",
    "        try:\n",
    "            with jf.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                data = json.load(fh)\n",
    "                papers.append(data)\n",
    "                print(f\"  ✓ {data.get('paper_id', jf.stem)}: {data.get('total_characters', 0):,} chars\")\n",
    "        except Exception as exc:\n",
    "            print(f\"  Error loading {jf.name}: {exc}\")\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. SINGLE PAPER ANALYSIS\n",
    "# -------------------------\n",
    "def analyze_single_paper(paper: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform an in-depth analysis of a single extracted paper.\n",
    "\n",
    "    Args:\n",
    "        paper: A single paper dict (as produced by Module 3).\n",
    "\n",
    "    Returns:\n",
    "        A structured analysis dictionary with extracted insights.\n",
    "    \"\"\"\n",
    "    print(\"\\nPerforming deep analysis of a single paper...\")\n",
    "\n",
    "    info = extract_key_information(paper)\n",
    "\n",
    "    analysis = {\n",
    "        \"paper_id\": info.get(\"paper_id\"),\n",
    "        \"title\": info.get(\"title\"),\n",
    "        \"year\": info.get(\"year\"),\n",
    "        \"methods_used\": info.get(\"methods\", []),\n",
    "        \"datasets_mentioned\": info.get(\"datasets\", []),\n",
    "        \"key_findings\": info.get(\"key_findings\", []),\n",
    "        \"limitations\": info.get(\"limitations\", []),\n",
    "        \"contributions\": info.get(\"contributions\", []),\n",
    "        \"metrics_reported\": info.get(\"metrics\", []),\n",
    "        \"paper_structure\": analyze_paper_structure(paper),\n",
    "        \"research_quality_indicators\": assess_research_quality(info),\n",
    "        \"recommendations_for_future_research\": generate_recommendations(info)\n",
    "    }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "def analyze_paper_structure(paper: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze which standard sections are present and their lengths.\n",
    "\n",
    "    Args:\n",
    "        paper: extracted paper dict.\n",
    "\n",
    "    Returns:\n",
    "        Dict describing present/missing sections and lengths.\n",
    "    \"\"\"\n",
    "    sections = paper.get(\"sections\", {})\n",
    "    structure: Dict[str, Any] = {\n",
    "        \"sections_present\": [],\n",
    "        \"sections_missing\": [],\n",
    "        \"section_lengths\": {}\n",
    "    }\n",
    "\n",
    "    expected_sections = [\"title\", \"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]\n",
    "\n",
    "    for sec in expected_sections:\n",
    "        content = sections.get(sec, \"\")\n",
    "        if content and len(content.split()) > 10:\n",
    "            structure[\"sections_present\"].append(sec)\n",
    "            structure[\"section_lengths\"][sec] = len(content.split())\n",
    "        else:\n",
    "            structure[\"sections_missing\"].append(sec)\n",
    "\n",
    "    return structure\n",
    "\n",
    "\n",
    "def assess_research_quality(info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Assess research quality using simple heuristic indicators.\n",
    "\n",
    "    Args:\n",
    "        info: result of extract_key_information.\n",
    "\n",
    "    Returns:\n",
    "        Dict with quality indicators and an overall score.\n",
    "    \"\"\"\n",
    "    quality_indicators: Dict[str, Any] = {\n",
    "        \"has_methods\": bool(info.get(\"methods\")),\n",
    "        \"has_datasets\": bool(info.get(\"datasets\")),\n",
    "        \"has_findings\": bool(info.get(\"key_findings\")),\n",
    "        \"has_limitations\": bool(info.get(\"limitations\")),\n",
    "        \"has_metrics\": bool(info.get(\"metrics\")),\n",
    "        \"method_diversity\": len(info.get(\"methods\", [])),\n",
    "        \"finding_clarity\": len(info.get(\"key_findings\", []))\n",
    "    }\n",
    "\n",
    "    score = 0\n",
    "    max_score = 7\n",
    "    if quality_indicators[\"has_methods\"]:\n",
    "        score += 1\n",
    "    if quality_indicators[\"has_datasets\"]:\n",
    "        score += 1\n",
    "    if quality_indicators[\"has_findings\"]:\n",
    "        score += 1\n",
    "    if quality_indicators[\"has_limitations\"]:\n",
    "        score += 1\n",
    "    if quality_indicators[\"has_metrics\"]:\n",
    "        score += 1\n",
    "    if quality_indicators[\"method_diversity\"] >= 2:\n",
    "        score += 1\n",
    "    if quality_indicators[\"finding_clarity\"] >= 2:\n",
    "        score += 1\n",
    "\n",
    "    quality_indicators[\"overall_score\"] = f\"{score}/{max_score}\"\n",
    "    quality_indicators[\"percentage\"] = (score / max_score) * 100\n",
    "\n",
    "    return quality_indicators\n",
    "\n",
    "\n",
    "def generate_recommendations(info: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Produce short recommendations for future research based on extracted info.\n",
    "\n",
    "    Args:\n",
    "        info: key information extracted from a paper.\n",
    "\n",
    "    Returns:\n",
    "        List of recommendation strings.\n",
    "    \"\"\"\n",
    "    recommendations: List[str] = []\n",
    "\n",
    "    methods = info.get(\"methods\", [])\n",
    "    if methods:\n",
    "        recommendations.append(f\"Compare with other studies using: {methods[0]}\")\n",
    "\n",
    "    limitations = info.get(\"limitations\", [])\n",
    "    if limitations:\n",
    "        recommendations.append(f\"Address limitations such as: {limitations[0][:120]}...\")\n",
    "\n",
    "    datasets = info.get(\"datasets\", [])\n",
    "    if datasets:\n",
    "        recommendations.append(\"Explore additional datasets to validate findings\")\n",
    "\n",
    "    recommendations.append(\"Compare with recent papers in the same domain\")\n",
    "    recommendations.append(\"Consider using alternative methodologies referenced in related work\")\n",
    "\n",
    "    return recommendations[:3]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. KEY INFORMATION EXTRACTION\n",
    "# -------------------------\n",
    "def extract_key_information(paper: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract structured key information from a paper.\n",
    "\n",
    "    Args:\n",
    "        paper: paper dict with 'sections' etc.\n",
    "\n",
    "    Returns:\n",
    "        Dict with fields: paper_id, title, year, methods, datasets, key_findings, limitations, contributions, metrics.\n",
    "    \"\"\"\n",
    "    sections = paper.get(\"sections\", {})\n",
    "    info = {\n",
    "        \"paper_id\": paper.get(\"paper_id\", \"unknown\"),\n",
    "        \"title\": sections.get(\"title\", \"Unknown\"),\n",
    "        \"year\": extract_year(paper),\n",
    "        \"methods\": extract_methods(paper),\n",
    "        \"datasets\": extract_datasets(paper),\n",
    "        \"key_findings\": extract_key_findings(paper),\n",
    "        \"limitations\": extract_limitations(paper),\n",
    "        \"contributions\": extract_contributions(paper),\n",
    "        \"metrics\": extract_metrics(paper)\n",
    "    }\n",
    "    return info\n",
    "\n",
    "\n",
    "def extract_year(paper: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Try to extract a 4-digit year from the title or extracted_text.\n",
    "\n",
    "    Returns year as string or 'Unknown'.\n",
    "    \"\"\"\n",
    "    title = paper.get(\"sections\", {}).get(\"title\", \"\") or \"\"\n",
    "    match = re.search(r\"\\b(19|20)\\d{2}\\b\", title)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "\n",
    "    text = paper.get(\"sections\", {}).get(\"extracted_text\", \"\") or \"\"\n",
    "    match = re.search(r\"\\b(19|20)\\d{2}\\b\", text[:5000])\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "\n",
    "    if paper.get(\"year\"):\n",
    "        return str(paper.get(\"year\"))\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def extract_methods(paper: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract likely methods mentioned in methods/results/conclusion sections using keywords.\n",
    "\n",
    "    Returns up to 5 method snippets.\n",
    "    \"\"\"\n",
    "    methods_text = paper.get(\"sections\", {}).get(\"methods\", \"\") or \"\"\n",
    "    if not methods_text:\n",
    "        methods_text = (paper.get(\"sections\", {}).get(\"extracted_text\", \"\") or \"\")[:5000]\n",
    "\n",
    "    method_keywords = [\n",
    "        \"deep learning\", \"machine learning\", \"neural network\", \"transformer\",\n",
    "        \"cnn\", \"rnn\", \"lstm\", \"bert\", \"gpt\", \"reinforcement learning\",\n",
    "        \"statistical\", \"regression\", \"classification\", \"clustering\",\n",
    "        \"svm\", \"random forest\", \"xgboost\", \"bayesian\", \"monte carlo\",\n",
    "        \"simulation\", \"experiment\", \"analysis\", \"framework\", \"model\",\n",
    "        \"algorithm\", \"approach\", \"technique\", \"methodology\"\n",
    "    ]\n",
    "\n",
    "    found: List[str] = []\n",
    "    sentences = re.split(r\"[.!?]+\", methods_text.lower())\n",
    "\n",
    "    for sent in sentences:\n",
    "        for kw in method_keywords:\n",
    "            if kw in sent and len(sent.strip()) > 20:\n",
    "                clean = re.sub(r\"\\s+\", \" \", sent).strip()\n",
    "                if clean not in found:\n",
    "                    found.append(clean[:200])\n",
    "                    break\n",
    "        if len(found) >= 5:\n",
    "            break\n",
    "\n",
    "    # Fallback: look in results/conclusion\n",
    "    if not found:\n",
    "        combined = (paper.get(\"sections\", {}).get(\"results\", \"\") or \"\") + \" \" + (paper.get(\"sections\", {}).get(\"conclusion\", \"\") or \"\")\n",
    "        for sent in re.split(r\"[.!?]+\", combined.lower()):\n",
    "            for kw in method_keywords[:10]:\n",
    "                if kw in sent and len(sent.strip()) > 20:\n",
    "                    clean = re.sub(r\"\\s+\", \" \", sent).strip()\n",
    "                    if clean not in found:\n",
    "                        found.append(clean[:200])\n",
    "                        break\n",
    "            if len(found) >= 5:\n",
    "                break\n",
    "\n",
    "    return found[:5]\n",
    "\n",
    "\n",
    "def extract_datasets(paper: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find mentions of common datasets or dataset-like phrases within the extracted_text.\n",
    "    \"\"\"\n",
    "    text = (paper.get(\"sections\", {}).get(\"extracted_text\", \"\") or \"\").lower()[:10000]\n",
    "\n",
    "    dataset_patterns = [\n",
    "        r\"imagenet\", r\"cifar\", r\"mnist\", r\"coco\", r\"pascal\", r\"wikitext\",\n",
    "        r\"bookcorpus\", r\"squad\", r\"glue\", r\"superglue\", r\"kaggle\", r\"uci\",\n",
    "        r\"pubmed\", r\"arxiv\", r\"dataset\", r\"corpus\", r\"benchmark\", r\"repository\"\n",
    "    ]\n",
    "    found: List[str] = []\n",
    "\n",
    "    for pat in dataset_patterns:\n",
    "        if re.search(pat, text):\n",
    "            found.append(pat)\n",
    "\n",
    "    sentences = re.split(r\"[.!?]+\", text)\n",
    "    for sent in sentences:\n",
    "        if any(k in sent for k in [\"dataset\", \"corpus\", \"benchmark\", \"collection\"]):\n",
    "            clean = re.sub(r\"\\s+\", \" \", sent).strip()[:150]\n",
    "            if clean and clean not in found:\n",
    "                found.append(clean)\n",
    "\n",
    "    unique = []\n",
    "    for x in found:\n",
    "        if x not in unique:\n",
    "            unique.append(x)\n",
    "        if len(unique) >= 5:\n",
    "            break\n",
    "\n",
    "    return unique\n",
    "\n",
    "\n",
    "def extract_key_findings(paper: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Heuristically extract sentences that appear to describe findings/results.\n",
    "    \"\"\"\n",
    "    text = (paper.get(\"sections\", {}).get(\"results\", \"\") or \"\") or (paper.get(\"sections\", {}).get(\"conclusion\", \"\") or \"\")\n",
    "    if not text:\n",
    "        text = (paper.get(\"sections\", {}).get(\"extracted_text\", \"\") or \"\")[:3000]\n",
    "\n",
    "    result_keywords = [\n",
    "        \"result shows\", \"findings show\", \"we found\", \"we demonstrate\",\n",
    "        \"achieves\", \"outperforms\", \"improves\", \"increases\", \"reduces\",\n",
    "        \"accuracy\", \"precision\", \"recall\", \"f1\", \"score\", \"performance\",\n",
    "        \"significant\", \"better than\", \"compared to\", \"surpasses\"\n",
    "    ]\n",
    "\n",
    "    findings: List[str] = []\n",
    "    sentences = re.split(r\"[.!?]+\", text.lower())\n",
    "\n",
    "    for sent in sentences:\n",
    "        if any(kw in sent for kw in result_keywords) and len(sent.strip()) > 30:\n",
    "            clean = re.sub(r\"\\s+\", \" \", sent).strip()\n",
    "            if clean not in findings:\n",
    "                findings.append(clean[:300])\n",
    "        if len(findings) >= 5:\n",
    "            break\n",
    "\n",
    "    if len(findings) < 2:\n",
    "        conclusion = (paper.get(\"sections\", {}).get(\"conclusion\", \"\") or \"\")\n",
    "        for sent in re.split(r\"[.!?]+\", conclusion.lower())[:5]:\n",
    "            if len(sent.strip()) > 50:\n",
    "                findings.append(re.sub(r\"\\s+\", \" \", sent).strip()[:300])\n",
    "            if len(findings) >= 5:\n",
    "                break\n",
    "\n",
    "    return findings[:5]\n",
    "\n",
    "\n",
    "def extract_limitations(paper: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract sentences indicating limitations or future work.\n",
    "    \"\"\"\n",
    "    text = (paper.get(\"sections\", {}).get(\"conclusion\", \"\") or \"\") or (paper.get(\"sections\", {}).get(\"extracted_text\", \"\") or \"\")[:5000]\n",
    "\n",
    "    limitation_keywords = [\n",
    "        \"limitation\", \"drawback\", \"shortcoming\", \"weakness\",\n",
    "        \"future work\", \"further research\", \"need to\", \"could be improved\",\n",
    "        \"challenge\", \"difficulty\", \"issue\", \"problem\", \"not consider\",\n",
    "        \"assumption\", \"restriction\", \"constraint\", \"only work\"\n",
    "    ]\n",
    "\n",
    "    limitations: List[str] = []\n",
    "    sentences = re.split(r\"[.!?]+\", text.lower())\n",
    "\n",
    "    for sent in sentences:\n",
    "        if any(kw in sent for kw in limitation_keywords) and len(sent.strip()) > 30:\n",
    "            clean = re.sub(r\"\\s+\", \" \", sent).strip()\n",
    "            if clean not in limitations:\n",
    "                limitations.append(clean[:300])\n",
    "        if len(limitations) >= 3:\n",
    "            break\n",
    "\n",
    "    return limitations[:3]\n",
    "\n",
    "\n",
    "def extract_contributions(paper: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract statements of contribution from abstract/introduction.\n",
    "    \"\"\"\n",
    "    abstract = (paper.get(\"sections\", {}).get(\"abstract\", \"\") or \"\")[:1000]\n",
    "    intro = (paper.get(\"sections\", {}).get(\"introduction\", \"\") or \"\")[:1000]\n",
    "    text = (abstract + \" \" + intro).lower()\n",
    "\n",
    "    contribution_keywords = [\n",
    "        \"contribution\", \"contribute\", \"propose\", \"introduce\",\n",
    "        \"novel\", \"new method\", \"new approach\", \"we present\",\n",
    "        \"this paper\", \"our work\", \"main contribution\", \"key contribution\"\n",
    "    ]\n",
    "\n",
    "    contributions: List[str] = []\n",
    "    for sent in re.split(r\"[.!?]+\", text):\n",
    "        if any(kw in sent for kw in contribution_keywords) and len(sent.strip()) > 30:\n",
    "            clean = re.sub(r\"\\s+\", \" \", sent).strip()\n",
    "            if clean not in contributions:\n",
    "                contributions.append(clean[:300])\n",
    "        if len(contributions) >= 3:\n",
    "            break\n",
    "\n",
    "    return contributions[:3]\n",
    "\n",
    "\n",
    "def extract_metrics(paper: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract numeric metric mentions from the results section.\n",
    "    \"\"\"\n",
    "    results_text = paper.get(\"sections\", {}).get(\"results\", \"\") or \"\"\n",
    "    if not results_text:\n",
    "        return []\n",
    "\n",
    "    metric_patterns = [\n",
    "        r\"accuracy\\s*[:=]\\s*\\d+\\.?\\d*%?\", r\"precision\\s*[:=]\\s*\\d+\\.?\\d*%?\",\n",
    "        r\"recall\\s*[:=]\\s*\\d+\\.?\\d*%?\", r\"f1[\\s\\-]?score\\s*[:=]\\s*\\d+\\.?\\d*%?\",\n",
    "        r\"auc\\s*[:=]\\s*\\d+\\.?\\d*\", r\"mae\\s*[:=]\\s*\\d+\\.?\\d*\", r\"rmse\\s*[:=]\\s*\\d+\\.?\\d*\",\n",
    "        r\"\\d+\\.?\\d*\\s*%\"\n",
    "    ]\n",
    "\n",
    "    metrics: List[str] = []\n",
    "    for pat in metric_patterns:\n",
    "        matches = re.findall(pat, results_text.lower())\n",
    "        for m in matches:\n",
    "            if m not in metrics:\n",
    "                metrics.append(m)\n",
    "\n",
    "    return metrics[:5]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. COMPARISON FUNCTIONS\n",
    "# -------------------------\n",
    "def compare_papers(papers_info: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compare multiple paper summaries to find similarities, differences, and gaps.\n",
    "\n",
    "    Args:\n",
    "        papers_info: list of paper summary dicts (from extract_key_information).\n",
    "\n",
    "    Returns:\n",
    "        Dict containing comparison results.\n",
    "    \"\"\"\n",
    "    print(f\"\\nComparing {len(papers_info)} papers...\")\n",
    "\n",
    "    comparison = {\n",
    "        \"total_papers\": len(papers_info),\n",
    "        \"papers\": papers_info,\n",
    "        \"similarities\": find_similarities(papers_info),\n",
    "        \"differences\": find_differences(papers_info),\n",
    "        \"common_methods\": find_common_elements(papers_info, \"methods\"),\n",
    "        \"common_datasets\": find_common_elements(papers_info, \"datasets\"),\n",
    "        \"timeline_analysis\": analyze_timeline(papers_info),\n",
    "        \"research_gaps\": identify_research_gaps(papers_info)\n",
    "    }\n",
    "\n",
    "    return comparison\n",
    "\n",
    "\n",
    "def find_similarities(papers_info: List[Dict[str, Any]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Identify methods/datasets/findings that appear in more than one paper.\n",
    "    \"\"\"\n",
    "    methods_count: defaultdict = defaultdict(int)\n",
    "    datasets_count: defaultdict = defaultdict(int)\n",
    "    findings_count: defaultdict = defaultdict(int)\n",
    "\n",
    "    for paper in papers_info:\n",
    "        for m in paper.get(\"methods\", []):\n",
    "            key = m[:50].lower()\n",
    "            methods_count[key] += 1\n",
    "        for d in paper.get(\"datasets\", []):\n",
    "            key = d[:50].lower()\n",
    "            datasets_count[key] += 1\n",
    "        for f in paper.get(\"key_findings\", []):\n",
    "            key = f[:50].lower()\n",
    "            findings_count[key] += 1\n",
    "\n",
    "    similar_items = {\n",
    "        \"methods\": [item for item, cnt in methods_count.items() if cnt > 1 and len(item) > 10],\n",
    "        \"datasets\": [item for item, cnt in datasets_count.items() if cnt > 1 and len(item) > 10],\n",
    "        \"findings\": [item for item, cnt in findings_count.items() if cnt > 1 and len(item) > 10]\n",
    "    }\n",
    "\n",
    "    return similar_items\n",
    "\n",
    "\n",
    "def find_differences(papers_info: List[Dict[str, Any]]) -> Dict[str, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    For each paper, find unique methods/datasets/findings compared to other papers.\n",
    "    \"\"\"\n",
    "    differences = {\n",
    "        \"unique_methods\": defaultdict(list),\n",
    "        \"unique_datasets\": defaultdict(list),\n",
    "        \"unique_findings\": defaultdict(list)\n",
    "    }\n",
    "\n",
    "    paper_methods: Dict[str, Set[str]] = defaultdict(set)\n",
    "    paper_datasets: Dict[str, Set[str]] = defaultdict(set)\n",
    "    paper_findings: Dict[str, Set[str]] = defaultdict(set)\n",
    "    all_methods: Set[str] = set()\n",
    "    all_datasets: Set[str] = set()\n",
    "    all_findings: Set[str] = set()\n",
    "\n",
    "    for paper in papers_info:\n",
    "        pid = paper.get(\"paper_id\", \"\")\n",
    "        for m in paper.get(\"methods\", []):\n",
    "            key = m[:50].lower()\n",
    "            paper_methods[pid].add(key)\n",
    "            all_methods.add(key)\n",
    "        for d in paper.get(\"datasets\", []):\n",
    "            key = d[:50].lower()\n",
    "            paper_datasets[pid].add(key)\n",
    "            all_datasets.add(key)\n",
    "        for f in paper.get(\"key_findings\", []):\n",
    "            key = f[:50].lower()\n",
    "            paper_findings[pid].add(key)\n",
    "            all_findings.add(key)\n",
    "\n",
    "    for pid in paper_methods:\n",
    "        unique_methods = paper_methods[pid] - set().union(*(paper_methods[qid] for qid in paper_methods if qid != pid))\n",
    "        if unique_methods:\n",
    "            differences[\"unique_methods\"][pid] = list(unique_methods)[:3]\n",
    "\n",
    "        unique_datasets = paper_datasets[pid] - set().union(*(paper_datasets[qid] for qid in paper_datasets if qid != pid))\n",
    "        if unique_datasets:\n",
    "            differences[\"unique_datasets\"][pid] = list(unique_datasets)[:3]\n",
    "\n",
    "        unique_findings = paper_findings[pid] - set().union(*(paper_findings[qid] for qid in paper_findings if qid != pid))\n",
    "        if unique_findings:\n",
    "            differences[\"unique_findings\"][pid] = list(unique_findings)[:3]\n",
    "\n",
    "    return {\n",
    "        \"unique_methods\": dict(differences[\"unique_methods\"]),\n",
    "        \"unique_datasets\": dict(differences[\"unique_datasets\"]),\n",
    "        \"unique_findings\": dict(differences[\"unique_findings\"])\n",
    "    }\n",
    "\n",
    "\n",
    "def find_common_elements(papers_info: List[Dict[str, Any]], element_type: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find elements (methods/datasets) that appear in every paper.\n",
    "\n",
    "    Args:\n",
    "        papers_info: list of paper info dicts.\n",
    "        element_type: 'methods' or 'datasets' or other list-key.\n",
    "\n",
    "    Returns:\n",
    "        List of common elements (first 5).\n",
    "    \"\"\"\n",
    "    element_sets: List[Set[str]] = []\n",
    "    for paper in papers_info:\n",
    "        elements = paper.get(element_type, [])\n",
    "        element_sets.append({e[:50].lower() for e in elements if len(e) > 10})\n",
    "\n",
    "    if not element_sets:\n",
    "        return []\n",
    "\n",
    "    common = set.intersection(*element_sets)\n",
    "    return list(common)[:5]\n",
    "\n",
    "\n",
    "def analyze_timeline(papers_info: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze temporal distribution of papers (earliest, latest, range, counts).\n",
    "\n",
    "    Returns:\n",
    "        timeline dict or note if insufficient data.\n",
    "    \"\"\"\n",
    "    years: List[int] = []\n",
    "    for p in papers_info:\n",
    "        y = p.get(\"year\", \"Unknown\")\n",
    "        try:\n",
    "            if isinstance(y, str) and y.isdigit():\n",
    "                yi = int(y)\n",
    "            elif isinstance(y, int):\n",
    "                yi = y\n",
    "            else:\n",
    "                continue\n",
    "            if 1900 <= yi <= 2100:\n",
    "                years.append(yi)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if len(years) >= 2:\n",
    "        timeline = {\n",
    "            \"earliest\": min(years),\n",
    "            \"latest\": max(years),\n",
    "            \"range\": max(years) - min(years),\n",
    "            \"count_by_year\": {str(year): years.count(year) for year in sorted(set(years))}\n",
    "        }\n",
    "    else:\n",
    "        timeline = {\"note\": \"Insufficient year data\"}\n",
    "\n",
    "    return timeline\n",
    "\n",
    "\n",
    "def identify_research_gaps(papers_info: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Heuristically identify research gaps from aggregated limitations and un-used popular methods.\n",
    "    \"\"\"\n",
    "    gaps: List[str] = []\n",
    "\n",
    "    all_limitations: List[str] = []\n",
    "    for p in papers_info:\n",
    "        all_limitations.extend(p.get(\"limitations\", []))\n",
    "\n",
    "    limitation_counts: defaultdict = defaultdict(int)\n",
    "    for lim in all_limitations:\n",
    "        key = lim[:100].lower()\n",
    "        limitation_counts[key] += 1\n",
    "\n",
    "    frequent_limitations = [lim for lim, cnt in limitation_counts.items() if cnt > 1 and len(lim) > 20]\n",
    "    if frequent_limitations:\n",
    "        gaps.append(\"Common limitations across papers:\")\n",
    "        gaps.extend(frequent_limitations[:3])\n",
    "\n",
    "    methods_used = {m.lower() for p in papers_info for m in p.get(\"methods\", [])}\n",
    "    datasets_used = {d.lower() for p in papers_info for d in p.get(\"datasets\", [])}\n",
    "\n",
    "    common_methods_in_field = [\n",
    "        \"deep learning\", \"transfer learning\", \"reinforcement learning\",\n",
    "        \"explainable ai\", \"few-shot learning\", \"meta learning\"\n",
    "    ]\n",
    "\n",
    "    missing_methods = [m for m in common_methods_in_field if m not in methods_used]\n",
    "    if missing_methods:\n",
    "        gaps.append(\"Potentially unexplored methods in this set of papers:\")\n",
    "        gaps.extend(missing_methods[:3])\n",
    "\n",
    "    return gaps[:5]\n",
    "\n",
    "\n",
    "def calculate_similarity_scores(papers_info: List[Dict[str, Any]]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute pairwise cosine similarity between papers using TF-IDF on title+abstract+findings.\n",
    "\n",
    "    Returns:\n",
    "        Nested dict mapping paper_id -> other_paper_id -> similarity score (0..1).\n",
    "    \"\"\"\n",
    "    texts: List[str] = []\n",
    "    paper_ids: List[str] = []\n",
    "\n",
    "    for p in papers_info:\n",
    "        pid = p.get(\"paper_id\", \"\")\n",
    "        paper_ids.append(pid)\n",
    "        title = p.get(\"title\", \"\")\n",
    "        abstract = p.get(\"sections\", {}).get(\"abstract\", \"\")[:1000]\n",
    "        findings = \" \".join(p.get(\"key_findings\", []))[:1000]\n",
    "        combined = \" \".join([title, abstract, findings])\n",
    "        texts.append(combined)\n",
    "\n",
    "    if not texts:\n",
    "        return {}\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=2000)\n",
    "    try:\n",
    "        tfidf = vectorizer.fit_transform(texts)\n",
    "        sim_matrix = cosine_similarity(tfidf)\n",
    "    except Exception as exc:\n",
    "        print(f\"Error computing similarity matrix: {exc}\")\n",
    "        # Return empty similarity if failure\n",
    "        return {}\n",
    "\n",
    "    similarity_scores: Dict[str, Dict[str, float]] = {}\n",
    "    n = len(paper_ids)\n",
    "    for i in range(n):\n",
    "        similarity_scores[paper_ids[i]] = {}\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            similarity_scores[paper_ids[i]][paper_ids[j]] = float(f\"{sim_matrix[i, j]:.3f}\")\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. SAVE RESULTS\n",
    "# -------------------------\n",
    "def save_results(analysis_type: str, data: Dict[str, Any], output_dir: str = \"data/analysis\") -> str:\n",
    "    \"\"\"\n",
    "    Save analysis results to JSON and generate human-readable reports.\n",
    "\n",
    "    Args:\n",
    "        analysis_type: \"single\" or \"comparison\"\n",
    "        data: analysis data dict\n",
    "        output_dir: directory to save results\n",
    "\n",
    "    Returns:\n",
    "        Path to output directory as string.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if analysis_type == \"single\":\n",
    "        output_file = output_path / \"single_paper_analysis.json\"\n",
    "        with output_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(data, fh, indent=2, ensure_ascii=False)\n",
    "        print(f\"   Single paper analysis saved to: {output_file}\")\n",
    "        generate_single_paper_report(data, output_path)\n",
    "\n",
    "    elif analysis_type == \"comparison\":\n",
    "        comparison_file = output_path / \"comparison.json\"\n",
    "        with comparison_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(data.get(\"comparison\", {}), fh, indent=2, ensure_ascii=False)\n",
    "        print(f\"  Comparison saved to: {comparison_file}\")\n",
    "\n",
    "        similarity_file = output_path / \"similarity_scores.json\"\n",
    "        with similarity_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(data.get(\"similarity_scores\", {}), fh, indent=2, ensure_ascii=False)\n",
    "        print(f\"   Similarity scores saved to: {similarity_file}\")\n",
    "\n",
    "        generate_comparison_report(data, output_path)\n",
    "\n",
    "    return str(output_path)\n",
    "\n",
    "\n",
    "def generate_single_paper_report(analysis: Dict[str, Any], output_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Create a human-readable text report for a single-paper analysis.\n",
    "    \"\"\"\n",
    "    report_lines: List[str] = []\n",
    "    report_lines.append(\"=\" * 80)\n",
    "    report_lines.append(\"SINGLE PAPER IN-DEPTH ANALYSIS REPORT\")\n",
    "    report_lines.append(\"=\" * 80)\n",
    "    report_lines.append(f\"\\n PAPER: {analysis.get('paper_id')}\")\n",
    "    report_lines.append(f\" Title: {analysis.get('title')}\")\n",
    "    report_lines.append(f\" Year: {analysis.get('year')}\\n\")\n",
    "\n",
    "    report_lines.append(\"METHODS IDENTIFIED:\")\n",
    "    report_lines.append(\"-\" * 40)\n",
    "    if analysis.get(\"methods_used\"):\n",
    "        for m in analysis[\"methods_used\"]:\n",
    "            report_lines.append(f\"• {m}\")\n",
    "    else:\n",
    "        report_lines.append(\"No specific methods identified\")\n",
    "\n",
    "    report_lines.append(\"\\nKEY FINDINGS:\")\n",
    "    report_lines.append(\"-\" * 40)\n",
    "    if analysis.get(\"key_findings\"):\n",
    "        for f in analysis[\"key_findings\"]:\n",
    "            report_lines.append(f\"• {f}\")\n",
    "    else:\n",
    "        report_lines.append(\"No key findings extracted\")\n",
    "\n",
    "    report_lines.append(\"\\nLIMITATIONS:\")\n",
    "    report_lines.append(\"-\" * 40)\n",
    "    if analysis.get(\"limitations\"):\n",
    "        for lim in analysis[\"limitations\"]:\n",
    "            report_lines.append(f\"• {lim}\")\n",
    "    else:\n",
    "        report_lines.append(\"No limitations mentioned\")\n",
    "\n",
    "    report_lines.append(\"\\nRESEARCH QUALITY:\")\n",
    "    report_lines.append(\"-\" * 40)\n",
    "    quality = analysis.get(\"research_quality_indicators\", {})\n",
    "    report_lines.append(f\"Overall Score: {quality.get('overall_score', 'N/A')} ({quality.get('percentage', 0):.1f}%)\")\n",
    "    report_lines.append(f\"Has Methods: {'✅' if quality.get('has_methods') else '❌'}\")\n",
    "    report_lines.append(f\"Has Datasets: {'✅' if quality.get('has_datasets') else '❌'}\")\n",
    "    report_lines.append(f\"Has Findings: {'✅' if quality.get('has_findings') else '❌'}\")\n",
    "    report_lines.append(f\"Has Limitations: {'✅' if quality.get('has_limitations') else '❌'}\")\n",
    "\n",
    "    report_lines.append(\"\\nRECOMMENDATIONS FOR FUTURE RESEARCH:\")\n",
    "    report_lines.append(\"-\" * 40)\n",
    "    for r in analysis.get(\"recommendations_for_future_research\", []):\n",
    "        report_lines.append(f\"• {r}\")\n",
    "\n",
    "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
    "    report_lines.append(\"ANALYSIS COMPLETE\")\n",
    "    report_lines.append(\"=\" * 80)\n",
    "\n",
    "    report_file = output_path / \"single_paper_report.txt\"\n",
    "    with report_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(\"\\n\".join(report_lines))\n",
    "\n",
    "    print(f\"   Summary report saved to: {report_file}\")\n",
    "\n",
    "\n",
    "def generate_comparison_report(data: Dict[str, Any], output_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Create a human-readable comparison report for multiple papers.\n",
    "    \"\"\"\n",
    "    comparison = data.get(\"comparison\", {})\n",
    "    similarity_scores = data.get(\"similarity_scores\", {})\n",
    "\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"=\" * 80)\n",
    "    lines.append(\"CROSS-PAPER COMPARISON REPORT\")\n",
    "    lines.append(\"=\" * 80)\n",
    "    lines.append(f\"\\nTotal papers analyzed: {comparison.get('total_papers', 0)}\\n\")\n",
    "\n",
    "    lines.append(\"PAPERS ANALYZED:\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    for paper in comparison.get(\"papers\", []):\n",
    "        lines.append(f\"\\n• {paper.get('paper_id', 'unknown')}\")\n",
    "        lines.append(f\"  Title: {paper.get('title', 'Unknown')}\")\n",
    "        lines.append(f\"  Year: {paper.get('year', 'Unknown')}\")\n",
    "        lines.append(f\"  Methods: {len(paper.get('methods', []))} found\")\n",
    "        lines.append(f\"  Datasets: {len(paper.get('datasets', []))} found\")\n",
    "\n",
    "    lines.append(\"\\nKEY SIMILARITIES:\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    sim = comparison.get(\"similarities\", {})\n",
    "    if sim.get(\"methods\"):\n",
    "        lines.append(\"\\nCommon Methods:\")\n",
    "        for m in sim[\"methods\"]:\n",
    "            lines.append(f\"  • {m}\")\n",
    "    if sim.get(\"datasets\"):\n",
    "        lines.append(\"\\nCommon Datasets:\")\n",
    "        for d in sim[\"datasets\"]:\n",
    "            lines.append(f\"  • {d}\")\n",
    "\n",
    "    lines.append(\"\\nPAPER SIMILARITY SCORES:\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    for pid, scores in similarity_scores.items():\n",
    "        lines.append(f\"\\n{pid}:\")\n",
    "        for other, score in scores.items():\n",
    "            lines.append(f\"  vs {other}: {score:.3f}\")\n",
    "\n",
    "    if comparison.get(\"research_gaps\"):\n",
    "        lines.append(\"\\nIDENTIFIED RESEARCH GAPS:\")\n",
    "        lines.append(\"-\" * 40)\n",
    "        for gap in comparison[\"research_gaps\"]:\n",
    "            lines.append(f\"• {gap}\")\n",
    "\n",
    "    lines.append(\"\\n\" + \"=\" * 80)\n",
    "    lines.append(\"COMPARISON COMPLETE\")\n",
    "    lines.append(\"=\" * 80)\n",
    "\n",
    "    report_file = output_path / \"comparison_report.txt\"\n",
    "    with report_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(\"\\n\".join(lines))\n",
    "\n",
    "    print(f\"  Comparison report saved to: {report_file}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. MAIN ANALYSIS PIPELINE\n",
    "# -------------------------\n",
    "def run_analysis() -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Main pipeline entrypoint for Module 4.\n",
    "\n",
    "    - Loads extracted papers\n",
    "    - If a single paper: run deep single-paper analysis\n",
    "    - If multiple papers: extract key info, compare, compute similarities, and save\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PAPER ANALYSIS MODULE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nSTEP 1: Loading extracted papers...\")\n",
    "    papers = load_extracted_papers()\n",
    "    if not papers:\n",
    "        print(\" No papers to analyze\")\n",
    "        return None\n",
    "\n",
    "    if len(papers) == 1:\n",
    "        print(\"\\nℹ Only 1 paper found. Performing in-depth single paper analysis...\")\n",
    "        paper = papers[0]\n",
    "        analysis = analyze_single_paper(paper)\n",
    "        info = extract_key_information(paper)\n",
    "\n",
    "        print(\"\\nSTEP 2: Saving analysis results...\")\n",
    "        save_path = save_results(\"single\", analysis)\n",
    "\n",
    "        print(\"\\nSINGLE PAPER ANALYSIS COMPLETE!\")\n",
    "        print(f\"Files saved to: {save_path}\")\n",
    "        return {\"type\": \"single\", \"analysis\": analysis, \"paper_info\": info}\n",
    "\n",
    "    else:\n",
    "        print(f\"\\nSTEP 2: Analyzing {len(papers)} papers for comparison...\")\n",
    "        papers_info: List[Dict[str, Any]] = []\n",
    "        for p in papers:\n",
    "            info = extract_key_information(p)\n",
    "            papers_info.append(info)\n",
    "            print(f\"  ✓ {info.get('paper_id')}: {len(info.get('methods', []))} methods, {len(info.get('key_findings', []))} findings\")\n",
    "\n",
    "        print(\"\\nSTEP 3: Comparing papers...\")\n",
    "        comparison = compare_papers(papers_info)\n",
    "\n",
    "        print(\"\\nSTEP 4: Calculating similarity scores...\")\n",
    "        similarity_scores = calculate_similarity_scores(papers_info)\n",
    "\n",
    "        print(\"\\nSTEP 5: Saving comparison results...\")\n",
    "        data = {\n",
    "            \"comparison\": comparison,\n",
    "            \"similarity_scores\": similarity_scores\n",
    "        }\n",
    "        save_path = save_results(\"comparison\", data)\n",
    "\n",
    "        print(\"\\nCROSS-PAPER ANALYSIS COMPLETE!\")\n",
    "        print(f\"Files saved to: {save_path}\")\n",
    "\n",
    "        return {\"type\": \"comparison\", \"data\": data, \"papers_info\": papers_info}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 7. DEMO / TEST HELPERS\n",
    "# -------------------------\n",
    "def create_demo_paper_for_testing() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a small demo paper info dict for testing comparison features.\n",
    "    \"\"\"\n",
    "    demo_paper = {\n",
    "        \"paper_id\": \"demo_paper_ai_ethics\",\n",
    "        \"title\": \"Ethical Considerations in Artificial Intelligence Systems\",\n",
    "        \"year\": \"2023\",\n",
    "        \"methods\": [\"machine learning\", \"ethical framework analysis\", \"case studies\"],\n",
    "        \"datasets\": [\"AI ethics guidelines corpus\", \"public opinion surveys\"],\n",
    "        \"key_findings\": [\n",
    "            \"AI systems show bias in 78% of tested scenarios\",\n",
    "            \"Current ethical frameworks lack enforcement mechanisms\",\n",
    "            \"Transparency is the most cited ethical concern\"\n",
    "        ],\n",
    "        \"limitations\": [\n",
    "            \"Study limited to Western ethical frameworks\",\n",
    "            \"Small sample size for public opinion data\"\n",
    "        ],\n",
    "        \"contributions\": [\n",
    "            \"Proposes new AI ethics assessment framework\",\n",
    "            \"Identifies key gaps in current regulations\"\n",
    "        ],\n",
    "        \"metrics\": [\"accuracy: 85%\", \"f1-score: 0.82\"]\n",
    "    }\n",
    "    return demo_paper\n",
    "\n",
    "\n",
    "def run_with_demo_data() -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Example function showing how to run comparison with a real paper + a demo paper.\n",
    "    \"\"\"\n",
    "    real_papers = load_extracted_papers()\n",
    "    if not real_papers:\n",
    "        raise RuntimeError(\"No real papers found to demo with.\")\n",
    "\n",
    "    demo = create_demo_paper_for_testing()\n",
    "    real_info = extract_key_information(real_papers[0])\n",
    "\n",
    "    papers_info = [real_info, demo]\n",
    "    comparison = compare_papers(papers_info)\n",
    "    similarity_scores = calculate_similarity_scores(papers_info)\n",
    "\n",
    "    print(\"\\nDemo comparison complete.\")\n",
    "    return comparison, similarity_scores\n",
    "\n",
    "\n",
    "# -------------\n",
    "# Entry point\n",
    "# -------------\n",
    "if __name__ == \"__main__\":\n",
    "    result = run_analysis()\n",
    "    if result:\n",
    "        if result.get(\"type\") == \"single\":\n",
    "            print(\"\\nSingle paper analysis completed.\")\n",
    "        else:\n",
    "            print(\"\\nComparison analysis completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1bd22",
   "metadata": {},
   "source": [
    "# Module 5: Generate Draft Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d35101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================\n",
      "GENERATE DRAFT SECTIONS WITH GPT (Pipeline)\n",
      "========================================================================\n",
      " Loaded analysis data for 4 paper(s)\n",
      "\n",
      "========================================================================\n",
      " GENERATING ACADEMIC DRAFT SECTIONS\n",
      "========================================================================\n",
      " GPTSectionGenerator initialized (simulated, model=gpt-3.5-turbo)\n",
      "\n",
      " Generating sections for 4 paper(s)...\n",
      "  - Generating: Abstract (100 words max)...\n",
      "    ✓ abstract: 17 words, 25 tokens\n",
      "  - Generating: Introduction...\n",
      "    ✓ introduction: 16 words, 20 tokens\n",
      "  - Generating: Methods Comparison...\n",
      "    ✓ methods: 17 words, 21 tokens\n",
      "  - Generating: Results Synthesis...\n",
      "    ✓ results: 16 words, 23 tokens\n",
      "  - Generating: Conclusion...\n",
      "    ✓ conclusion: 30 words, 37 tokens\n",
      "  - Generating: APA References...\n",
      "    ✓ references: 20 words, 85 tokens\n",
      "\n",
      "========================================================================\n",
      " VALIDATING GENERATED SECTIONS\n",
      "========================================================================\n",
      " Abstract word count OK: 17/100\n",
      "\n",
      "Validation summary:\n",
      " Checks passed: 3/4\n",
      " Issues found:\n",
      "  - References may not follow basic APA structure\n",
      "\n",
      " Saving outputs to: C:\\Users\\abhis\\outputs\n",
      "  Saved: abstract_20251224_233458.txt\n",
      "  Saved: introduction_20251224_233458.txt\n",
      "  Saved: methods_20251224_233458.txt\n",
      "  Saved: results_20251224_233458.txt\n",
      "  Saved: conclusion_20251224_233458.txt\n",
      "  Saved: references_20251224_233458.txt\n",
      "  Saved complete draft: complete_draft_20251224_233458.txt\n",
      "  Saved metadata: draft_metadata_20251224_233458.json\n",
      "\n",
      " report saved to: C:\\Users\\abhis\\outputs\\review_report.txt\n",
      "\n",
      "Generation complete. Outputs saved to: C:\\Users\\abhis\\outputs\n",
      "\n",
      "Draft generation finished successfully.\n",
      "Would you like to preview the recent draft? (y/n): y\n",
      "\n",
      "========================================================================\n",
      "PREVIEW OF GENERATED DRAFT: complete_draft_20251224_233458.txt\n",
      "========================================================================\n",
      "ACADEMIC DRAFT - RESEARCH PAPER ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Generated: 2025-12-24 23:34:58\n",
      "Papers analyzed: 4\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "ABSTRACT (100 WORDS MAX)\n",
      "------------------------\n",
      "\n",
      "This comparative analysis synthesizes findings from 4 research papers. The synthesis highlights patterns, divergences, and research gaps.\n",
      "\n",
      "\n",
      "INTRODUCTION\n",
      "------------\n",
      "\n",
      "This comparative review considers 4 papers. It synthesizes methodologies and findings to identify trends and gaps.\n",
      "\n",
      "\n",
      "METHODS COMPARISON\n",
      "------------------\n",
      "\n",
      "Across studies, methodological approaches show both overlap and variation. Unique approaches highlight different research focuses across papers.\n",
      "\n",
      "\n",
      "RESULTS SYNTHESIS\n",
      "-----------------\n",
      "\n",
      "Synthesis across papers indicates several recurring findings: Comparative results illuminate both convergences and divergences among studies.\n",
      "\n",
      "\n",
      "CONCLUSION\n",
      "----------\n",
      "\n",
      "This comparative review identifies key trends and open research areas. Notable ...\n",
      "\n",
      "Total words in draft: 148\n",
      "\n",
      "Validation checks passed (approx): 3/4\n"
     ]
    }
   ],
   "source": [
    "# pip install tiktoken\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "    _HAS_TIKTOKEN = True\n",
    "except Exception:\n",
    "    _HAS_TIKTOKEN = False\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. GPT SECTION GENERATOR\n",
    "# -------------------------\n",
    "class GPTSectionGenerator:\n",
    "    \"\"\"\n",
    "    Simulated GPT-based section generator.\n",
    "\n",
    "    This class provides simple template-based generation for:\n",
    "      - abstract\n",
    "      - introduction\n",
    "      - methods\n",
    "      - results\n",
    "      - conclusion\n",
    "      - references\n",
    "\n",
    "    The class is intentionally conservative: it does not call any external API.\n",
    "    In production, replace the template functions with actual API calls while\n",
    "    keeping the same method signatures.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: Optional[str] = None, model: str = \"gpt-3.5-turbo\") -> None:\n",
    "        \"\"\"\n",
    "        Initialize the generator.\n",
    "\n",
    "        Args:\n",
    "            api_key: Optional API key (not used in this educational template).\n",
    "            model: Model name string (used for token estimation).\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        if _HAS_TIKTOKEN:\n",
    "            try:\n",
    "                self.encoding = tiktoken.encoding_for_model(model)\n",
    "            except Exception:\n",
    "                self.encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "        else:\n",
    "            self.encoding = None\n",
    "\n",
    "        print(f\" GPTSectionGenerator initialized (simulated, model={self.model})\")\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Count tokens in text. Uses tiktoken when available for realistic counts,\n",
    "        otherwise falls back to a conservative word-based estimate.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "        if _HAS_TIKTOKEN and self.encoding is not None:\n",
    "            try:\n",
    "                return len(self.encoding.encode(text))\n",
    "            except Exception:\n",
    "                return len(text.split())\n",
    "        return len(text.split())\n",
    "\n",
    "    # -------------------------\n",
    "    # Template generation API\n",
    "    # -------------------------\n",
    "    def create_system_prompt(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a fixed system prompt for academic writing.\n",
    "        (Kept as a function so production replacements can reuse it.)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            \"You are an academic research assistant. Generate structured academic \"\n",
    "            \"sections based on provided analysis data. Use formal academic language; \"\n",
    "            \"base content on provided analysis; use APA-style references where possible.\"\n",
    "        )\n",
    "\n",
    "    def generate_with_template(self, section_type: str, analysis_data: Dict[str, Any], paper_count: int = 1) -> str:\n",
    "        \"\"\"\n",
    "        Dispatch to the appropriate template generator.\n",
    "\n",
    "        Args:\n",
    "            section_type: One of 'abstract', 'introduction', 'methods', 'results', 'conclusion', 'references'.\n",
    "            analysis_data: Dict returned by Module 4 (single or comparison).\n",
    "            paper_count: Number of papers the analysis covered.\n",
    "\n",
    "        Returns:\n",
    "            Generated text for the section.\n",
    "        \"\"\"\n",
    "        if section_type == \"abstract\":\n",
    "            return self._generate_abstract(analysis_data, paper_count)\n",
    "        if section_type == \"introduction\":\n",
    "            return self._generate_introduction(analysis_data, paper_count)\n",
    "        if section_type == \"methods\":\n",
    "            return self._generate_methods_comparison(analysis_data, paper_count)\n",
    "        if section_type == \"results\":\n",
    "            return self._generate_results_synthesis(analysis_data, paper_count)\n",
    "        if section_type == \"conclusion\":\n",
    "            return self._generate_conclusion(analysis_data, paper_count)\n",
    "        if section_type == \"references\":\n",
    "            return self._generate_references(analysis_data)\n",
    "        return \"Section type not recognized\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Internal template methods\n",
    "    # -------------------------\n",
    "    def _generate_abstract(self, analysis_data: Dict[str, Any], paper_count: int) -> str:\n",
    "        \"\"\"Generate a short abstract. Keep <= 100 words where possible.\"\"\"\n",
    "\n",
    "        if paper_count == 1 and \"analysis\" in analysis_data:\n",
    "            paper = analysis_data.get(\"analysis\", {})\n",
    "            title = paper.get(\"title\", \"This paper\")\n",
    "            methods = paper.get(\"methods_used\", [])\n",
    "            findings = paper.get(\"key_findings\", [])\n",
    "            abstract = f\"This review examines '{title}'. \"\n",
    "            if methods:\n",
    "                abstract += f\"The approach uses {methods[0]}. \"\n",
    "            if findings:\n",
    "                abstract += f\"Key finding: {findings[0][:140]}. \"\n",
    "            abstract += \"This analysis summarizes methodological choices and implications.\"\n",
    "        else:\n",
    "            # Multi-paper abstract\n",
    "            comp = analysis_data.get(\"data\", {}).get(\"comparison\", {})\n",
    "            common_methods = comp.get(\"common_methods\", []) if comp else []\n",
    "            abstract = f\"This comparative analysis synthesizes findings from {paper_count} research papers. \"\n",
    "            if common_methods:\n",
    "                abstract += f\"Common approaches include {', '.join(common_methods[:2])}. \"\n",
    "            abstract += \"The synthesis highlights patterns, divergences, and research gaps.\"\n",
    "        # Enforce rough 100-word limit\n",
    "        words = abstract.split()\n",
    "        if len(words) > 100:\n",
    "            abstract = \" \".join(words[:100]) + \"...\"\n",
    "        return abstract\n",
    "\n",
    "    def _generate_introduction(self, analysis_data: Dict[str, Any], paper_count: int) -> str:\n",
    "        \"\"\"Generate an introduction tailored to single- or multi-paper analyses.\"\"\"\n",
    "        if paper_count == 1:\n",
    "            paper = analysis_data.get(\"analysis\", {})\n",
    "            title = paper.get(\"title\", \"this research\")\n",
    "            year = paper.get(\"year\", \"\")\n",
    "            intro = f\"This analysis examines {title}\"\n",
    "            if year and year != \"Unknown\":\n",
    "                intro += f\" ({year})\"\n",
    "            intro += \". The paper addresses important questions and applies appropriate methods. \"\n",
    "            intro += \"This review evaluates research design, methodological choices, and implications.\"\n",
    "        else:\n",
    "            papers_info = analysis_data.get(\"papers_info\", [])\n",
    "            years = [p.get(\"year\") for p in papers_info if p.get(\"year\") and p.get(\"year\") != \"Unknown\"]\n",
    "            intro = f\"This comparative review considers {paper_count} papers\"\n",
    "            if years:\n",
    "                intro += f\" spanning {min(years)}–{max(years)}\"\n",
    "            intro += \". It synthesizes methodologies and findings to identify trends and gaps.\"\n",
    "        return intro\n",
    "\n",
    "    def _generate_methods_comparison(self, analysis_data: Dict[str, Any], paper_count: int) -> str:\n",
    "        \"\"\"Generate a methods section that summarizes methodological commonalities and differences.\"\"\"\n",
    "        if paper_count == 1:\n",
    "            paper = analysis_data.get(\"analysis\", {})\n",
    "            methods = paper.get(\"methods_used\", [])\n",
    "            datasets = paper.get(\"datasets_mentioned\", [])\n",
    "            text = \"The study's methodology is characterized by \"\n",
    "            if methods:\n",
    "                text += f\"{methods[0]}\"\n",
    "                if len(methods) > 1:\n",
    "                    text += f\" and {methods[1]}\"\n",
    "                text += \". \"\n",
    "            else:\n",
    "                text += \"standard and appropriate approaches for the research problem. \"\n",
    "            if datasets:\n",
    "                text += f\"The dataset used includes {datasets[0]}. \"\n",
    "            text += \"Methodological choices appear aligned with the objectives.\"\n",
    "        else:\n",
    "            comparison = analysis_data.get(\"data\", {}).get(\"comparison\", {})\n",
    "            common = comparison.get(\"common_methods\", []) if comparison else []\n",
    "            text = \"Across studies, methodological approaches show both overlap and variation. \"\n",
    "            if common:\n",
    "                text += f\"Common methods observed include {', '.join(common[:3])}. \"\n",
    "            text += \"Unique approaches highlight different research focuses across papers.\"\n",
    "        return text\n",
    "\n",
    "    def _generate_results_synthesis(self, analysis_data: Dict[str, Any], paper_count: int) -> str:\n",
    "        \"\"\"Synthesize results/findings from the analysis data.\"\"\"\n",
    "        if paper_count == 1:\n",
    "            paper = analysis_data.get(\"analysis\", {})\n",
    "            findings = paper.get(\"key_findings\", [])\n",
    "            metrics = paper.get(\"metrics_reported\", [])\n",
    "            text = \"The analysis reveals the following key findings: \"\n",
    "            if findings:\n",
    "                for i, f in enumerate(findings[:3], 1):\n",
    "                    text += f\"{i}. {f[:140]}. \"\n",
    "            if metrics:\n",
    "                text += f\"Reported metrics include {', '.join(metrics[:3])}. \"\n",
    "            text += \"These results inform the paper's contributions and limitations.\"\n",
    "        else:\n",
    "            papers_info = analysis_data.get(\"papers_info\", [])\n",
    "            all_findings = []\n",
    "            for p in papers_info:\n",
    "                all_findings.extend(p.get(\"key_findings\", []))\n",
    "            text = \"Synthesis across papers indicates several recurring findings: \"\n",
    "            if all_findings:\n",
    "                for i, f in enumerate(all_findings[:4], 1):\n",
    "                    text += f\"{i}. {f[:100]}. \"\n",
    "            text += \"Comparative results illuminate both convergences and divergences among studies.\"\n",
    "        return text\n",
    "\n",
    "    def _generate_conclusion(self, analysis_data: Dict[str, Any], paper_count: int) -> str:\n",
    "        \"\"\"Produce a brief conclusion summarizing contributions, limitations, and future directions.\"\"\"\n",
    "        if paper_count == 1:\n",
    "            paper = analysis_data.get(\"analysis\", {})\n",
    "            limitations = paper.get(\"limitations\", [])\n",
    "            recs = paper.get(\"recommendations_for_future_research\", [])\n",
    "            text = \"In conclusion, the analysis highlights the work's methodological strengths and contributions. \"\n",
    "            if limitations:\n",
    "                text += f\"Identified limitations include {limitations[0][:140]}. \"\n",
    "            if recs:\n",
    "                text += f\"Future work should consider {recs[0][:140]}. \"\n",
    "            text += \"Overall, the paper provides a useful foundation for further research.\"\n",
    "        else:\n",
    "            comp = analysis_data.get(\"data\", {}).get(\"comparison\", {})\n",
    "            gaps = comp.get(\"research_gaps\", []) if comp else []\n",
    "            text = \"This comparative review identifies key trends and open research areas. \"\n",
    "            if gaps:\n",
    "                text += f\"Notable research gaps include {gaps[0]}. \"\n",
    "            text += \"These directions suggest fruitful opportunities for future studies.\"\n",
    "        return text\n",
    "\n",
    "    def _generate_references(self, analysis_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate a small APA-style references block for demo purposes.\"\"\"\n",
    "\n",
    "        if \"analysis\" in analysis_data:\n",
    "            paper = analysis_data.get(\"analysis\", {})\n",
    "            pid = paper.get(\"paper_id\", \"paper\")\n",
    "            title = paper.get(\"title\", \"Untitled\")\n",
    "            year = paper.get(\"year\", \"n.d.\")\n",
    "            refs = f\"{pid}. ({year}). {title}. [Analyzed research paper].\\n\\n\"\n",
    "            refs += \"American Psychological Association. (2020). Publication manual of the American Psychological Association (7th ed.).\\n\"\n",
    "            refs += \"Smith, J., & Johnson, A. (2019). Research methods in academic writing. Academic Press.\\n\"\n",
    "            return refs\n",
    "        papers_info = analysis_data.get(\"papers_info\", []) or []\n",
    "        lines = []\n",
    "        for p in papers_info:\n",
    "            pid = p.get(\"paper_id\", \"paper\")\n",
    "            title = p.get(\"title\", \"Untitled\")\n",
    "            year = p.get(\"year\", \"n.d.\")\n",
    "            lines.append(f\"{pid}. ({year}). {title}.\")\n",
    "        lines.append(\"\\nAmerican Psychological Association. (2020). Publication manual (7th ed.).\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. LOAD ANALYSIS DATA\n",
    "# -------------------------\n",
    "def load_analysis_data() -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load analysis data produced by Module 4 (single or comparison).\n",
    "\n",
    "    Returns:\n",
    "        A dict describing the analysis context, or None if no data found.\n",
    "    \"\"\"\n",
    "    analysis_path = Path(\"data/analysis\")\n",
    "    comparison_file = analysis_path / \"comparison.json\"\n",
    "    single_file = analysis_path / \"single_paper_analysis.json\"\n",
    "\n",
    "    if comparison_file.exists():\n",
    "        try:\n",
    "            with comparison_file.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                comparison_data = json.load(fh)\n",
    "        except Exception as exc:\n",
    "            print(f\" Error reading comparison.json: {exc}\")\n",
    "            return None\n",
    "\n",
    "        papers_info = []\n",
    "        for summary in comparison_data.get(\"papers\", []):\n",
    "            pid = summary.get(\"paper_id\")\n",
    "            candidate = Path(\"data/extracted\") / f\"{pid}_extracted.json\"\n",
    "            if candidate.exists():\n",
    "                try:\n",
    "                    with candidate.open(\"r\", encoding=\"utf-8\") as pf:\n",
    "                        papers_info.append(json.load(pf))\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        return {\n",
    "            \"type\": \"comparison\",\n",
    "            \"data\": {\"comparison\": comparison_data},\n",
    "            \"papers_info\": papers_info,\n",
    "            \"paper_count\": len(papers_info) if papers_info else len(comparison_data.get(\"papers\", []))\n",
    "        }\n",
    "\n",
    "    if single_file.exists():\n",
    "        try:\n",
    "            with single_file.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                analysis_data = json.load(fh)\n",
    "        except Exception as exc:\n",
    "            print(f\" Error reading single_paper_analysis.json: {exc}\")\n",
    "            return None\n",
    "        return {\"type\": \"single\", \"analysis\": analysis_data, \"paper_count\": 1}\n",
    "\n",
    "    print(\" No analysis data found in data/analysis. Run Module 4 first.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. DRAFT GENERATION\n",
    "# -------------------------\n",
    "def generate_all_sections(analysis_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate all standard draft sections using GPTSectionGenerator templates.\n",
    "\n",
    "    Returns:\n",
    "        sections: Dict mapping section_key -> {name, content, word_count, token_count}\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\" GENERATING ACADEMIC DRAFT SECTIONS\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    if not analysis_data:\n",
    "        raise ValueError(\"analysis_data required for generation\")\n",
    "\n",
    "    paper_count = analysis_data.get(\"paper_count\", 1)\n",
    "    generator = GPTSectionGenerator()\n",
    "\n",
    "    section_specs = [\n",
    "        (\"abstract\", \"Abstract (100 words max)\"),\n",
    "        (\"introduction\", \"Introduction\"),\n",
    "        (\"methods\", \"Methods Comparison\"),\n",
    "        (\"results\", \"Results Synthesis\"),\n",
    "        (\"conclusion\", \"Conclusion\"),\n",
    "        (\"references\", \"APA References\"),\n",
    "    ]\n",
    "\n",
    "    sections: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    print(f\"\\n Generating sections for {paper_count} paper(s)...\")\n",
    "    for key, display_name in section_specs:\n",
    "        print(f\"  - Generating: {display_name}...\")\n",
    "        content = generator.generate_with_template(key, analysis_data, paper_count)\n",
    "        word_count = len(content.split())\n",
    "        token_count = generator.count_tokens(content)\n",
    "        sections[key] = {\n",
    "            \"name\": display_name,\n",
    "            \"content\": content,\n",
    "            \"word_count\": word_count,\n",
    "            \"token_count\": token_count\n",
    "        }\n",
    "        print(f\"    ✓ {key}: {word_count} words, {token_count} tokens\")\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. VALIDATION CHECKS\n",
    "# -------------------------\n",
    "def validate_sections(sections: Dict[str, Dict[str, Any]], analysis_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run simple validation checks on generated sections.\n",
    "\n",
    "    Checks performed:\n",
    "      - Abstract word limit <= 100 words\n",
    "      - References contain basic APA-like patterns\n",
    "      - Sections are factually tied to analysis (simple keyword check)\n",
    "      - All required sections present\n",
    "\n",
    "    Returns:\n",
    "        validation_results: dict with boolean flags and issues list.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\" VALIDATING GENERATED SECTIONS\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    results = {\n",
    "        \"abstract_word_limit\": False,\n",
    "        \"references_apa_format\": False,\n",
    "        \"sections_factual\": False,\n",
    "        \"all_sections_present\": False,\n",
    "        \"issues\": []\n",
    "    }\n",
    "\n",
    "    # Abstract check\n",
    "    abstract_text = sections.get(\"abstract\", {}).get(\"content\", \"\")\n",
    "    abstract_words = len(abstract_text.split())\n",
    "    results[\"abstract_word_limit\"] = abstract_words <= 100\n",
    "    if not results[\"abstract_word_limit\"]:\n",
    "        results[\"issues\"].append(f\"Abstract exceeds 100 words ({abstract_words})\")\n",
    "    else:\n",
    "        print(f\" Abstract word count OK: {abstract_words}/100\")\n",
    "\n",
    "    references_text = sections.get(\"references\", {}).get(\"content\", \"\")\n",
    "    has_parenthetical_dates = bool(re.search(r\"\\(\\d{4}\\)\", references_text))\n",
    "    has_author_initials = bool(re.search(r\"[A-Z][a-z]+,?\\s+[A-Z]\\.\", references_text))\n",
    "    results[\"references_apa_format\"] = has_parenthetical_dates and has_author_initials\n",
    "    if not results[\"references_apa_format\"]:\n",
    "        results[\"issues\"].append(\"References may not follow basic APA structure\")\n",
    "\n",
    "    if analysis_data.get(\"type\") == \"single\":\n",
    "        analysis = analysis_data.get(\"analysis\", {})\n",
    "        key_terms: List[str] = []\n",
    "        if analysis.get(\"title\"):\n",
    "            key_terms.append(analysis[\"title\"].split()[:3] and \" \".join(analysis[\"title\"].split()[:3]))\n",
    "        if analysis.get(\"methods_used\"):\n",
    "            key_terms.extend([m.split()[:3] and \" \".join(m.split()[:3]) for m in analysis[\"methods_used\"][:2]])\n",
    "        all_text = \" \".join([s[\"content\"] for s in sections.values()]) if sections else \"\"\n",
    "        matches = sum(1 for term in key_terms if term and term.lower() in all_text.lower())\n",
    "        results[\"sections_factual\"] = matches >= 1\n",
    "        if not results[\"sections_factual\"]:\n",
    "            results[\"issues\"].append(\"Generated sections do not reference analysis key terms\")\n",
    "        else:\n",
    "            print(f\" Sections reference {matches} key terms from analysis\")\n",
    "    else:\n",
    "        combined_text = \" \".join([s[\"content\"] for s in sections.values()])\n",
    "        results[\"sections_factual\"] = any(word in combined_text.lower() for word in [\"method\", \"result\", \"finding\", \"study\"])\n",
    "        if not results[\"sections_factual\"]:\n",
    "            results[\"issues\"].append(\"Generated multi-paper sections may lack method/result mentions\")\n",
    "\n",
    "    required = {\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"}\n",
    "    missing = required - set(sections.keys())\n",
    "    results[\"all_sections_present\"] = len(missing) == 0\n",
    "    if missing:\n",
    "        results[\"issues\"].append(f\"Missing sections: {', '.join(sorted(missing))}\")\n",
    "\n",
    "    # Print summary\n",
    "    passed = sum(1 for key in [\"abstract_word_limit\", \"references_apa_format\", \"sections_factual\", \"all_sections_present\"] if results.get(key))\n",
    "    print(\"\\nValidation summary:\")\n",
    "    print(f\" Checks passed: {passed}/4\")\n",
    "    if results[\"issues\"]:\n",
    "        print(\" Issues found:\")\n",
    "        for issue in results[\"issues\"]:\n",
    "            print(\"  -\", issue)\n",
    "    else:\n",
    "        print(\" No validation issues detected.\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. SAVE OUTPUTS\n",
    "# -------------------------\n",
    "def save_draft_outputs(sections: Dict[str, Dict[str, Any]], analysis_data: Dict[str, Any], validation_results: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Save generated sections and metadata to /outputs/.\n",
    "\n",
    "    Returns:\n",
    "        Path to outputs directory (string).\n",
    "    \"\"\"\n",
    "    outputs_dir = Path(\"outputs\")\n",
    "    outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"\\n Saving outputs to: {outputs_dir.resolve()}\")\n",
    "\n",
    "    # Save each section as a separate file\n",
    "    for key, data in sections.items():\n",
    "        filename = outputs_dir / f\"{key}_{timestamp}.txt\"\n",
    "        with filename.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            fh.write(f\"{data['name']}\\n\")\n",
    "            fh.write(\"=\" * len(data[\"name\"]) + \"\\n\\n\")\n",
    "            fh.write(data[\"content\"])\n",
    "            fh.write(f\"\\n\\n[Word count: {data['word_count']}]\\n\")\n",
    "            fh.write(f\"[Token count: {data['token_count']}]\\n\")\n",
    "        print(f\"  Saved: {filename.name}\")\n",
    "\n",
    "    # Save complete draft\n",
    "    complete = outputs_dir / f\"complete_draft_{timestamp}.txt\"\n",
    "    with complete.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(\"ACADEMIC DRAFT - RESEARCH PAPER ANALYSIS\\n\")\n",
    "        fh.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        fh.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        fh.write(f\"Papers analyzed: {analysis_data.get('paper_count', 1)}\\n\")\n",
    "        fh.write(\"-\" * 50 + \"\\n\\n\")\n",
    "        for sec in [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]:\n",
    "            if sec in sections:\n",
    "                s = sections[sec]\n",
    "                fh.write(f\"\\n{s['name'].upper()}\\n\")\n",
    "                fh.write(\"-\" * len(s['name']) + \"\\n\\n\")\n",
    "                fh.write(s[\"content\"] + \"\\n\\n\")\n",
    "    print(f\"  Saved complete draft: {complete.name}\")\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"generation_date\": timestamp,\n",
    "        \"paper_count\": analysis_data.get(\"paper_count\", 1),\n",
    "        \"analysis_type\": analysis_data.get(\"type\", \"unknown\"),\n",
    "        \"sections_generated\": len(sections),\n",
    "        \"validation_results\": validation_results,\n",
    "        \"section_stats\": {k: {\"word_count\": v[\"word_count\"], \"token_count\": v[\"token_count\"]} for k, v in sections.items()}\n",
    "    }\n",
    "    meta_file = outputs_dir / f\"draft_metadata_{timestamp}.json\"\n",
    "    with meta_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(metadata, fh, indent=2, ensure_ascii=False)\n",
    "    print(f\"  Saved metadata: {meta_file.name}\")\n",
    "\n",
    "    return str(outputs_dir.resolve())\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. GENERATE REPORT\n",
    "# -------------------------\n",
    "def generate_report(sections: Dict[str, Dict[str, Any]], validation_results: Dict[str, Any], output_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a short review report summarizing generated content and validation.\n",
    "\n",
    "    Returns:\n",
    "        Path to the report file as string.\n",
    "    \"\"\"\n",
    "    outdir = Path(output_path)\n",
    "    report_file = outdir / \"review_report.txt\"\n",
    "    lines: List[str] = []\n",
    "    lines.append(\"=\" * 80)\n",
    "    lines.append(\"REVIEW REPORT - GENERATED DRAFT SECTIONS\")\n",
    "    lines.append(\"=\" * 80)\n",
    "    lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    lines.append(\"\\nOBJECTIVE CHECKLIST:\")\n",
    "    lines.append(\"-\" * 40)\n",
    "\n",
    "    objectives = [\n",
    "        (\"Abstract (<=100 words)\", validation_results.get(\"abstract_word_limit\", False),\n",
    "         f\"Abstract words: {sections.get('abstract', {}).get('word_count', 0)}\"),\n",
    "        (\"References basic APA\", validation_results.get(\"references_apa_format\", False),\n",
    "         \"Basic APA elements detected\" if validation_results.get(\"references_apa_format\") else \"May need formatting\"),\n",
    "        (\"Sections factually tied\", validation_results.get(\"sections_factual\", False),\n",
    "         \"Evidence of analysis terms in sections\" if validation_results.get(\"sections_factual\") else \"Review factual alignment\"),\n",
    "        (\"All sections present\", validation_results.get(\"all_sections_present\", False),\n",
    "         \"6/6 sections\" if validation_results.get(\"all_sections_present\") else \"Missing sections\")\n",
    "    ]\n",
    "\n",
    "    for title, passed, details in objectives:\n",
    "        status = \"PASSED\" if passed else \"NEEDS REVIEW\"\n",
    "        lines.append(f\"\\n{title}:\\n  Status: {status}\\n  Details: {details}\\n\")\n",
    "\n",
    "    lines.append(\"\\nSECTION STATISTICS:\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    for key, s in sections.items():\n",
    "        lines.append(f\"\\n{s['name']}: Words={s['word_count']}, Tokens={s['token_count']}\")\n",
    "\n",
    "    lines.append(\"\\nVALIDATION ISSUES:\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    if validation_results.get(\"issues\"):\n",
    "        for issue in validation_results[\"issues\"]:\n",
    "            lines.append(f\"• {issue}\")\n",
    "    else:\n",
    "        lines.append(\"No significant issues found\")\n",
    "\n",
    "    lines.append(\"\\nNEXT STEPS:\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    lines.append(\"1. Manually verify factual accuracy against original papers.\")\n",
    "    lines.append(\"2. Edit references to full APA format as needed.\")\n",
    "    lines.append(\"3. Expand sections if reviewer requests more detail.\")\n",
    "\n",
    "    with report_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(\"\\n\".join(lines))\n",
    "    print(f\"\\n report saved to: {report_file}\")\n",
    "\n",
    "    return str(report_file)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 7. MAIN GENERATION PIPELINE\n",
    "# -------------------------\n",
    "def run_draft_generation() -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    End-to-end pipeline to generate draft sections from analysis data.\n",
    "\n",
    "    Steps:\n",
    "      1. Load analysis data from Module 4 outputs\n",
    "      2. Generate sections using GPTSectionGenerator templates\n",
    "      3. Validate generated content\n",
    "      4. Save outputs and generate a review report\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"GENERATE DRAFT SECTIONS WITH GPT (Pipeline)\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    analysis_data = load_analysis_data()\n",
    "    if not analysis_data:\n",
    "        print(\"Cannot proceed without analysis data.\")\n",
    "        return None\n",
    "\n",
    "    paper_count = analysis_data.get(\"paper_count\", 1)\n",
    "    print(f\" Loaded analysis data for {paper_count} paper(s)\")\n",
    "\n",
    "    # Step 2: generate\n",
    "    sections = generate_all_sections(analysis_data)\n",
    "\n",
    "    # Step 3: validate\n",
    "    validation_results = validate_sections(sections, analysis_data)\n",
    "\n",
    "    # Step 4: save\n",
    "    output_path = save_draft_outputs(sections, analysis_data, validation_results)\n",
    "\n",
    "    # Step 5: create review report\n",
    "    mentor_report = generate_report(sections, validation_results, output_path)\n",
    "\n",
    "    print(\"\\nGeneration complete. Outputs saved to:\", output_path)\n",
    "    return {\"sections\": sections, \"validation\": validation_results, \"output_path\": output_path}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 8. PREVIEW FUNCTION\n",
    "# -------------------------\n",
    "def preview_generated_draft() -> None:\n",
    "    \"\"\"\n",
    "    Show a short preview of the most recent complete draft file and metadata.\n",
    "    \"\"\"\n",
    "    outputs_dir = Path(\"outputs\")\n",
    "    if not outputs_dir.exists():\n",
    "        print(\"No outputs found. Run run_draft_generation() first.\")\n",
    "        return\n",
    "\n",
    "    drafts = sorted(outputs_dir.glob(\"complete_draft_*.txt\"), key=lambda p: p.stat().st_mtime)\n",
    "    if not drafts:\n",
    "        print(\"No complete draft file found in outputs/\")\n",
    "        return\n",
    "\n",
    "    latest = drafts[-1]\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"PREVIEW OF GENERATED DRAFT:\", latest.name)\n",
    "    print(\"=\" * 72)\n",
    "    try:\n",
    "        with latest.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "            content = fh.read()\n",
    "            preview = content[:1000] + (\"...\" if len(content) > 1000 else \"\")\n",
    "            print(preview)\n",
    "            print(f\"\\nTotal words in draft: {len(content.split())}\")\n",
    "    except Exception as exc:\n",
    "        print(\"Error reading draft:\", exc)\n",
    "        return\n",
    "\n",
    "    metadata_files = sorted(outputs_dir.glob(\"draft_metadata_*.json\"), key=lambda p: p.stat().st_mtime)\n",
    "    if metadata_files:\n",
    "        try:\n",
    "            with metadata_files[-1].open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                metadata = json.load(fh)\n",
    "            val = metadata.get(\"validation_results\", {})\n",
    "            passed = sum(1 for k, v in val.items() if isinstance(v, bool) and v)\n",
    "            print(f\"\\nValidation checks passed (approx): {passed}/4\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 9. ENTRYPOINT\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    result = run_draft_generation()\n",
    "    if result:\n",
    "        print(\"\\nDraft generation finished successfully.\")\n",
    "        try:\n",
    "            preview = input(\"Would you like to preview the recent draft? (y/n): \")\n",
    "        except Exception:\n",
    "            preview = \"n\"\n",
    "        if preview.strip().lower().startswith(\"y\"):\n",
    "            preview_generated_draft()\n",
    "    else:\n",
    "        print(\"Draft generation did not complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f0fd7c",
   "metadata": {},
   "source": [
    "# Module 6 : Validation of Correctness and Completeness of the Extracted Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aabe74e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================\n",
      "DRAFT AGGREGATION & CRITIQUE MODULE\n",
      "========================================================================\n",
      "STEP 1: Loading generated draft...\n",
      "Loaded draft: complete_draft_20251224_233458.txt\n",
      "Loading individual sections...\n",
      "  Loaded 6 sections\n",
      "STEP 2: Creating full markdown draft...\n",
      "Full draft saved: full_draft_initial.md\n",
      "STEP 3: Running draft critique...\n",
      "Analyzing draft quality...\n",
      "  • Checking clarity... ✅\n",
      "  • Checking flow... ✅\n",
      "  • Checking missing_references... ❌\n",
      "  • Checking repetition... ❌\n",
      "  • Checking style... ❌\n",
      "  • Checking structure... ❌\n",
      "Critique Score: 2/6\n",
      "Critique feedback saved: critique_feedback_iteration_1.json\n",
      "\n",
      "Iteration 1/2\n",
      "Running revision cycle 1...\n",
      "Revised draft saved: revised_draft_iteration_1.md\n",
      "Analyzing draft quality...\n",
      "  • Checking clarity... ✅\n",
      "  • Checking flow... ✅\n",
      "  • Checking missing_references... ❌\n",
      "  • Checking repetition... ❌\n",
      "  • Checking style... ❌\n",
      "  • Checking structure... ❌\n",
      "Critique feedback saved: critique_feedback_iteration_2.json\n",
      "  Score after revision 1: 2/6\n",
      "\n",
      "Iteration 2/2\n",
      "Running revision cycle 2...\n",
      "Revised draft saved: revised_draft_iteration_2.md\n",
      "Analyzing draft quality...\n",
      "  • Checking clarity... ✅\n",
      "  • Checking flow... ✅\n",
      "  • Checking missing_references... ❌\n",
      "  • Checking repetition... ❌\n",
      "  • Checking style... ❌\n",
      "  • Checking structure... ❌\n",
      "Critique feedback saved: critique_feedback_iteration_3.json\n",
      "  Score after revision 2: 2/6\n",
      "\n",
      "STEP 5: Creating revision summary...\n",
      "Revision summary saved: revision_summary.json\n",
      "\n",
      "========================================================================\n",
      "COMPLETE!\n",
      "========================================================================\n",
      "\n",
      "OUTPUTS GENERATED:\n",
      " • full_draft_initial.md - Initial full draft\n",
      " • revised_draft_iteration_1.md - Revised draft v1\n",
      " • revised_draft_iteration_2.md - Revised draft v2\n",
      " • revision_summary.json - Revision summary\n",
      "\n",
      "DRAFT AGGREGATION & CRITIQUE SUCCESSFUL!\n",
      "Would you like to preview critique results? (y/n): y\n",
      "\n",
      "========================================================================\n",
      "CRITIQUE RESULTS PREVIEW\n",
      "========================================================================\n",
      "File: critique_feedback_iteration_3.json\n",
      "Score: 2/6\n",
      "\n",
      "Top Suggestions:\n",
      "1. Add 2-3 more relevant citations and ensure APA format.\n",
      "2. Replace frequently repeated words with synonyms.\n",
      "3. Eliminate informal language; reduce first-person pronouns.\n",
      "4. Ensure all sections are present and appropriately balanced by length.\n",
      "5. Read the draft aloud to catch awkward phrasing.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. LOAD GENERATED DRAFT\n",
    "# -------------------------\n",
    "def load_latest_draft(outputs_dir: str = \"outputs\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Load the most recent aggregated complete draft text.\n",
    "\n",
    "    Args:\n",
    "        outputs_dir: Path to outputs folder where complete drafts are saved.\n",
    "\n",
    "    Returns:\n",
    "        The draft text (string) or None if not found.\n",
    "    \"\"\"\n",
    "    out_path = Path(outputs_dir)\n",
    "    if not out_path.exists():\n",
    "        print(\"No outputs found. Run Module 5 first.\")\n",
    "        return None\n",
    "\n",
    "    draft_files = sorted(out_path.glob(\"complete_draft_*.txt\"), key=lambda p: p.stat().st_mtime)\n",
    "    if not draft_files:\n",
    "        print(\"No complete draft found in outputs/\")\n",
    "        return None\n",
    "\n",
    "    latest = draft_files[-1]\n",
    "    try:\n",
    "        with latest.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "            content = fh.read()\n",
    "        print(f\"Loaded draft: {latest.name}\")\n",
    "        return content\n",
    "    except Exception as exc:\n",
    "        print(f\"Error reading draft {latest.name}: {exc}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_individual_sections(outputs_dir: str = \"outputs\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load the latest individual section files (abstract, introduction, etc.).\n",
    "\n",
    "    Args:\n",
    "        outputs_dir: Path containing individual section files.\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping section keys to section content.\n",
    "    \"\"\"\n",
    "    out_path = Path(outputs_dir)\n",
    "    if not out_path.exists():\n",
    "        return {}\n",
    "\n",
    "    section_patterns = {\n",
    "        \"abstract\": \"abstract_*.txt\",\n",
    "        \"introduction\": \"introduction_*.txt\",\n",
    "        \"methods\": \"methods_*.txt\",\n",
    "        \"results\": \"results_*.txt\",\n",
    "        \"conclusion\": \"conclusion_*.txt\",\n",
    "        \"references\": \"references_*.txt\",\n",
    "    }\n",
    "\n",
    "    sections: Dict[str, str] = {}\n",
    "    for key, pattern in section_patterns.items():\n",
    "        files = list(out_path.glob(pattern))\n",
    "        if not files:\n",
    "            continue\n",
    "        latest = max(files, key=lambda p: p.stat().st_mtime)\n",
    "        try:\n",
    "            with latest.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                content = fh.read()\n",
    "            lines = content.splitlines()\n",
    "            section_content = \"\\n\".join(lines[2:]) if len(lines) > 2 else content\n",
    "            sections[key] = section_content.strip()\n",
    "        except Exception as exc:\n",
    "            print(f\"Error reading section file {latest.name}: {exc}\")\n",
    "            continue\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. AGGREGATE FULL DRAFT\n",
    "# -------------------------\n",
    "def create_full_draft_markdown(\n",
    "    sections: Dict[str, str],\n",
    "    critique_feedback: Optional[Dict[str, Any]] = None,\n",
    "    title: str = \"Research Paper Analysis Review\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Combine individual sections into a polished markdown draft.\n",
    "\n",
    "    Args:\n",
    "        sections: Dict mapping section_key -> content\n",
    "        critique_feedback: Optional critique results to append as revision notes\n",
    "        title: Title for the markdown draft\n",
    "\n",
    "    Returns:\n",
    "        The combined markdown string.\n",
    "    \"\"\"\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    lines: List[str] = []\n",
    "    lines.append(f\"# {title}\\n\")\n",
    "    lines.append(f\"*Generated: {now}*\")\n",
    "    lines.append(f\"*Status: {'Revised' if critique_feedback else 'Initial'} Draft*\")\n",
    "    lines.append(\"\\n---\\n\")\n",
    "\n",
    "    section_order = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]\n",
    "    for key in section_order:\n",
    "        content = sections.get(key)\n",
    "        if content:\n",
    "            lines.append(f\"\\n## {key.upper()}\\n\")\n",
    "            lines.append(content)\n",
    "            lines.append(\"\\n---\\n\")\n",
    "\n",
    "    if critique_feedback:\n",
    "        lines.append(\"\\n## Critique & Revision Notes\\n\")\n",
    "        lines.append(\"### Issues Identified:\\n\")\n",
    "        issues_found = False\n",
    "        checks = critique_feedback.get(\"checks\", {})\n",
    "        for check_type, check_data in checks.items():\n",
    "            passed = bool(check_data.get(\"passed\", False))\n",
    "            suggestion = check_data.get(\"suggestion\", \"\")\n",
    "            if not passed:\n",
    "                issues_found = True\n",
    "                lines.append(f\"- **{check_type.replace('_', ' ').title()}**: {suggestion}\")\n",
    "\n",
    "        if not issues_found:\n",
    "            lines.append(\"No major issues identified. Draft is well-structured.\")\n",
    "\n",
    "        lines.append(\"\\n### Suggested Revisions:\\n\")\n",
    "        for suggestion in critique_feedback.get(\"suggestions\", [])[:5]:\n",
    "            lines.append(f\"- {suggestion}\")\n",
    "\n",
    "    # Add basic word count\n",
    "    full_text = \"\\n\".join(lines)\n",
    "    word_count = len(re.findall(r\"\\b\\w+\\b\", full_text))\n",
    "    lines.append(f\"\\n\\n*Word count: {word_count}*\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. CRITIQUE SYSTEM\n",
    "# -------------------------\n",
    "class DraftCritique:\n",
    "    \"\"\"\n",
    "    Critique system that analyzes the draft for clarity, flow, references, repetition,\n",
    "    academic style, and structure.\n",
    "\n",
    "    The critique methods return (passed: bool, feedback: List[str]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.criteria = {\n",
    "            \"clarity\": self.check_clarity,\n",
    "            \"flow\": self.check_flow,\n",
    "            \"missing_references\": self.check_missing_references,\n",
    "            \"repetition\": self.check_repetition,\n",
    "            \"style\": self.check_academic_style,\n",
    "            \"structure\": self.check_structure,\n",
    "        }\n",
    "\n",
    "    def critique_draft(self, draft_text: str, sections: Dict[str, str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the full critique over the draft.\n",
    "\n",
    "        Args:\n",
    "            draft_text: Full draft text (markdown)\n",
    "            sections: Individual sections dictionary\n",
    "\n",
    "        Returns:\n",
    "            critique_results: structured dict containing checks, suggestions, score metrics\n",
    "        \"\"\"\n",
    "        print(\"Analyzing draft quality...\")\n",
    "\n",
    "        total_checks = len(self.criteria)\n",
    "        critique_results: Dict[str, Any] = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"checks\": {},\n",
    "            \"suggestions\": [],\n",
    "            \"score\": 0,\n",
    "            \"total_checks\": total_checks,\n",
    "        }\n",
    "\n",
    "        passed_checks = 0\n",
    "        for name, func in self.criteria.items():\n",
    "            print(f\"  • Checking {name}...\", end=\" \")\n",
    "            try:\n",
    "                passed, feedback = func(draft_text, sections)\n",
    "            except Exception as exc:\n",
    "                passed = False\n",
    "                feedback = [f\"Error running check: {exc}\"]\n",
    "\n",
    "            critique_results[\"checks\"][name] = {\n",
    "                \"passed\": passed,\n",
    "                \"feedback\": feedback,\n",
    "                \"suggestion\": self.generate_suggestion(name, passed, feedback),\n",
    "            }\n",
    "\n",
    "            if passed:\n",
    "                passed_checks += 1\n",
    "                print(\"✅\")\n",
    "            else:\n",
    "                print(\"❌\")\n",
    "\n",
    "        critique_results[\"score\"] = passed_checks\n",
    "        critique_results[\"passed_checks\"] = passed_checks\n",
    "        critique_results[\"suggestions\"] = self.generate_overall_suggestions(critique_results)\n",
    "\n",
    "        return critique_results\n",
    "\n",
    "    # ---------- individual checks ----------\n",
    "    def check_clarity(self, draft_text: str, sections: Dict[str, str]) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Check for clarity issues: long sentences and overuse of passive voice.\n",
    "        \"\"\"\n",
    "        issues: List[str] = []\n",
    "        # Simple sentence split\n",
    "        sentences = [s.strip() for s in re.split(r\"[.!?]+\", draft_text) if s.strip()]\n",
    "        long_sentences = [s for s in sentences if len(s.split()) > 40]\n",
    "        if long_sentences:\n",
    "            issues.append(f\"{len(long_sentences)} sentences are very long (>40 words)\")\n",
    "\n",
    "        passive_patterns = [r\"\\b(is|are|was|were)\\s+\\w+ed\\b\", r\"\\bbe\\s+\\w+ed\\b\"]\n",
    "        passive_count = sum(len(re.findall(pat, draft_text.lower())) for pat in passive_patterns)\n",
    "        if passive_count > 10:\n",
    "            issues.append(f\"High use of passive voice ({passive_count} instances)\")\n",
    "\n",
    "        passed = len(issues) == 0\n",
    "        return passed, issues\n",
    "\n",
    "    def check_flow(self, draft_text: str, sections: Dict[str, str]) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Check logical flow between sections: presence of required sections and referencing.\n",
    "        \"\"\"\n",
    "        issues: List[str] = []\n",
    "        required_order = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]\n",
    "        missing = [s for s in required_order if s not in sections]\n",
    "        if missing:\n",
    "            issues.append(f\"Missing sections: {', '.join(missing)}\")\n",
    "\n",
    "        if \"conclusion\" in sections and \"introduction\" in sections:\n",
    "            intro_keywords = [\"paper\", \"study\", \"research\", \"analysis\"]\n",
    "            conclusion_text = sections.get(\"conclusion\", \"\").lower()\n",
    "            if not any(k in conclusion_text for k in intro_keywords):\n",
    "                issues.append(\"Conclusion does not clearly reference introduction/key aims\")\n",
    "\n",
    "        passed = len(issues) == 0\n",
    "        return passed, issues\n",
    "\n",
    "    def check_missing_references(self, draft_text: str, sections: Dict[str, str]) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Check for basic reference completeness (years, authors, count).\n",
    "        \"\"\"\n",
    "        issues: List[str] = []\n",
    "        if \"references\" in sections:\n",
    "            ref_text = sections[\"references\"]\n",
    "            has_years = bool(re.search(r\"\\(\\d{4}\\)\", ref_text))\n",
    "            has_authors = bool(re.search(r\"[A-Z][a-z]+,\\s+[A-Z]\\.\", ref_text))\n",
    "            if not has_years:\n",
    "                issues.append(\"References missing publication years\")\n",
    "            if not has_authors:\n",
    "                issues.append(\"References may be missing author names or initials\")\n",
    "\n",
    "            ref_lines = [line for line in ref_text.splitlines() if line.strip()]\n",
    "            if len(ref_lines) < 3:\n",
    "                issues.append(f\"Only {len(ref_lines)} references found (suggest 5+ for review)\")\n",
    "        else:\n",
    "            issues.append(\"No references section found\")\n",
    "\n",
    "        passed = len(issues) == 0\n",
    "        return passed, issues\n",
    "\n",
    "    def check_repetition(self, draft_text: str, sections: Dict[str, str]) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Check for repetitive words/phrases and similar section openings.\n",
    "        \"\"\"\n",
    "        issues: List[str] = []\n",
    "        words = re.findall(r\"\\b\\w+\\b\", draft_text.lower())\n",
    "        freq: Dict[str, int] = defaultdict(int)\n",
    "        for w in words:\n",
    "            if len(w) > 4:\n",
    "                freq[w] += 1\n",
    "\n",
    "        common_exclude = {\"paper\", \"study\", \"research\", \"analysis\", \"method\", \"result\", \"finding\"}\n",
    "        overused = sorted([(w, c) for w, c in freq.items() if c > 5 and w not in common_exclude], key=lambda x: -x[1])\n",
    "        if overused:\n",
    "            top = overused[:3]\n",
    "            issues.append(\"Overused words: \" + \", \".join([f\"{w}({c})\" for w, c in top]))\n",
    "\n",
    "        section_texts = list(sections.values())\n",
    "        for i in range(len(section_texts)):\n",
    "            for j in range(i + 1, len(section_texts)):\n",
    "                a_start = section_texts[i][:50].strip()\n",
    "                b_start = section_texts[j][:50].strip()\n",
    "                if a_start and a_start == b_start:\n",
    "                    issues.append(\"Two sections share the same opening text — possible duplication\")\n",
    "                    break\n",
    "\n",
    "        passed = len(issues) == 0\n",
    "        return passed, issues\n",
    "\n",
    "    def check_academic_style(self, draft_text: str, sections: Dict[str, str]) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Check for informal language, first-person usage, and paragraph length.\n",
    "        \"\"\"\n",
    "        issues: List[str] = []\n",
    "        informal_words = [\"really\", \"very\", \"a lot\", \"got\", \"stuff\", \"thing\"]\n",
    "        informal_count = sum(draft_text.lower().count(w) for w in informal_words)\n",
    "        if informal_count > 3:\n",
    "            issues.append(f\"Informal language used ({informal_count} instances)\")\n",
    "\n",
    "        first_person_matches = re.findall(r\"\\b(I|we|our|us|my|mine)\\b\", draft_text, flags=re.I)\n",
    "        if len(first_person_matches) > 5:\n",
    "            issues.append(f\"High use of first-person pronouns ({len(first_person_matches)})\")\n",
    "\n",
    "        paragraphs = [p for p in draft_text.split(\"\\n\\n\") if p.strip()]\n",
    "        short_paragraphs = [p for p in paragraphs if len(p.split()) < 50]\n",
    "        if len(short_paragraphs) > 3:\n",
    "            issues.append(f\"{len(short_paragraphs)} very short paragraphs (<50 words)\")\n",
    "\n",
    "        passed = len(issues) == 0\n",
    "        return passed, issues\n",
    "\n",
    "    def check_structure(self, draft_text: str, sections: Dict[str, str]) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Check that required sections exist and have reasonable lengths.\n",
    "        \"\"\"\n",
    "        issues: List[str] = []\n",
    "        required = {\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"}\n",
    "        present = set(sections.keys())\n",
    "        missing = required - present\n",
    "        if missing:\n",
    "            issues.append(f\"Missing required sections: {', '.join(sorted(missing))}\")\n",
    "\n",
    "        for sec, content in sections.items():\n",
    "            words = len(re.findall(r\"\\b\\w+\\b\", content))\n",
    "\n",
    "            if sec == \"abstract\" and words > 150:\n",
    "                issues.append(f\"Abstract too long ({words} words; aim for <150)\")\n",
    "\n",
    "            if sec == \"introduction\" and words < 100:\n",
    "                issues.append(f\"Introduction too short ({words} words; aim for 100+)\")\n",
    "\n",
    "        if \"methods\" in sections and \"results\" in sections and \"conclusion\" in sections:\n",
    "            m_len = len(re.findall(r\"\\b\\w+\\b\", sections.get(\"methods\", \"\")))\n",
    "            r_len = len(re.findall(r\"\\b\\w+\\b\", sections.get(\"results\", \"\")))\n",
    "            c_len = len(re.findall(r\"\\b\\w+\\b\", sections.get(\"conclusion\", \"\")))\n",
    "            if c_len > (m_len + r_len):\n",
    "                issues.append(\"Conclusion unusually long compared to Methods and Results\")\n",
    "\n",
    "        passed = len(issues) == 0\n",
    "        return passed, issues\n",
    "\n",
    "    # ---------- suggestion helpers ----------\n",
    "    def generate_suggestion(self, criterion: str, passed: bool, feedback: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Create a short actionable suggestion for a failed criterion.\n",
    "        \"\"\"\n",
    "        base_recs = {\n",
    "            \"clarity\": \"Use shorter sentences and prefer active voice.\",\n",
    "            \"flow\": \"Add clear transitions and ensure the conclusion ties back to the introduction.\",\n",
    "            \"missing_references\": \"Expand and format the references list (use APA).\",\n",
    "            \"repetition\": \"Vary vocabulary and rephrase repeated phrases.\",\n",
    "            \"style\": \"Avoid informal words and excessive first-person pronouns.\",\n",
    "            \"structure\": \"Ensure all required sections are present and balanced in length.\",\n",
    "        }\n",
    "        if passed:\n",
    "            return f\"{criterion.title()} is fine.\"\n",
    "        suggestion = base_recs.get(criterion, \"Review and improve this area.\")\n",
    "        if feedback:\n",
    "            return f\"{suggestion} Issues: {'; '.join(feedback[:3])}\"\n",
    "        return suggestion\n",
    "\n",
    "    def generate_overall_suggestions(self, critique_results: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Create a concise list of next-step suggestions based on failed checks.\n",
    "        \"\"\"\n",
    "        suggestions: List[str] = []\n",
    "        failed = [name for name, c in critique_results.get(\"checks\", {}).items() if not c.get(\"passed\")]\n",
    "        if not failed:\n",
    "            suggestions.append(\"Draft is well-structured. Minor polishing only needed.\")\n",
    "            suggestions.append(\"Check references formatting before submission.\")\n",
    "            return suggestions\n",
    "\n",
    "        if \"clarity\" in failed:\n",
    "            suggestions.append(\"Revise long sentences; convert passive voice to active where appropriate.\")\n",
    "        if \"flow\" in failed:\n",
    "            suggestions.append(\"Add transition sentences between sections; make conclusion reference introduction.\")\n",
    "        if \"missing_references\" in failed:\n",
    "            suggestions.append(\"Add 2-3 more relevant citations and ensure APA format.\")\n",
    "        if \"repetition\" in failed:\n",
    "            suggestions.append(\"Replace frequently repeated words with synonyms.\")\n",
    "        if \"style\" in failed:\n",
    "            suggestions.append(\"Eliminate informal language; reduce first-person pronouns.\")\n",
    "        if \"structure\" in failed:\n",
    "            suggestions.append(\"Ensure all sections are present and appropriately balanced by length.\")\n",
    "\n",
    "        # General final tips\n",
    "        suggestions.append(\"Read the draft aloud to catch awkward phrasing.\")\n",
    "        suggestions.append(\"Have a peer review for factual accuracy.\")\n",
    "        return suggestions[:7]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. REVISION CYCLE\n",
    "# -------------------------\n",
    "def run_revision_cycle(\n",
    "    draft_text: str,\n",
    "    sections: Dict[str, str],\n",
    "    critique_results: Dict[str, Any],\n",
    "    iteration: int = 1,\n",
    ") -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Run a single revision cycle applying simple automated fixes based on critique.\n",
    "\n",
    "    Args:\n",
    "        draft_text: Full draft text (not strictly required for some operations)\n",
    "        sections: Individual section contents\n",
    "        critique_results: Output of DraftCritique.critique_draft\n",
    "        iteration: Current iteration number (for logging)\n",
    "\n",
    "    Returns:\n",
    "        revised_draft: New markdown draft text\n",
    "        revised_sections: Updated sections dict\n",
    "    \"\"\"\n",
    "    print(f\"Running revision cycle {iteration}...\")\n",
    "    revised = dict(sections)\n",
    "\n",
    "    checks = critique_results.get(\"checks\", {})\n",
    "    for criterion, info in checks.items():\n",
    "        if not info.get(\"passed\"):\n",
    "            feedback = info.get(\"feedback\", [])\n",
    "            revised = apply_revisions(revised, criterion, feedback)\n",
    "\n",
    "    revised_draft = create_full_draft_markdown(revised, critique_results)\n",
    "    return revised_draft, revised\n",
    "\n",
    "\n",
    "def apply_revisions(sections: Dict[str, str], criterion: str, feedback: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Apply rule-based revisions to sections for a specific failing criterion.\n",
    "\n",
    "    This function uses conservative edits — it avoids altering factual content,\n",
    "    instead focusing on surface-level improvements (sentence splitting, synonyms, small expansions).\n",
    "\n",
    "    Args:\n",
    "        sections: dict of section_name -> content\n",
    "        criterion: the name of the failed criterion\n",
    "        feedback: list of feedback strings from the critique\n",
    "\n",
    "    Returns:\n",
    "        revised_sections: updated sections dict\n",
    "    \"\"\"\n",
    "    revised = dict(sections) \n",
    "\n",
    "    if criterion == \"clarity\":\n",
    "        for name, content in list(revised.items()):\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', content)\n",
    "            new_sentences: List[str] = []\n",
    "            for s in sentences:\n",
    "                words = s.split()\n",
    "                if len(words) > 60:\n",
    "                    mid = len(words) // 2\n",
    "                    new_sentences.append(\" \".join(words[:mid]) + \".\")\n",
    "                    new_sentences.append(\" \".join(words[mid:]) + \".\")\n",
    "                else:\n",
    "                    new_sentences.append(s)\n",
    "            revised[name] = \" \".join(new_sentences).strip()\n",
    "\n",
    "    elif criterion == \"repetition\":\n",
    "        replacements = {\n",
    "            \"paper\": \"study\",\n",
    "            \"research\": \"investigation\",\n",
    "            \"analysis\": \"examination\",\n",
    "            \"method\": \"approach\",\n",
    "            \"result\": \"finding\"\n",
    "        }\n",
    "        for name in (\"abstract\", \"conclusion\"):\n",
    "            if name in revised:\n",
    "                content = revised[name]\n",
    "                for old, new in replacements.items():\n",
    "                    if content.lower().count(old) > 2:\n",
    "                        content = re.sub(rf'\\b{old}\\b', new, content, count=2, flags=re.I)\n",
    "                revised[name] = content\n",
    "\n",
    "    elif criterion == \"structure\":\n",
    "        intro = revised.get(\"introduction\", \"\")\n",
    "        intro_words = len(re.findall(r\"\\b\\w+\\b\", intro))\n",
    "        if intro and intro_words < 100:\n",
    "            addition = (\n",
    "                \" This analysis provides a more detailed examination of the methodological \"\n",
    "                \"approaches and findings. The review situates the work within the broader \"\n",
    "                \"research context and evaluates implications and potential improvements.\"\n",
    "            )\n",
    "            revised[\"introduction\"] = (intro + \" \" + addition).strip()\n",
    "\n",
    "    elif criterion == \"missing_references\":\n",
    "        if \"references\" in revised:\n",
    "            if \"PLEASE_ADD_FULL_REFERENCES\" not in revised[\"references\"]:\n",
    "                revised[\"references\"] = revised[\"references\"].strip() + \"\\n\\n[PLEASE_ADD_FULL_REFERENCES]\"\n",
    "        else:\n",
    "            revised[\"references\"] = \"[PLEASE_ADD_FULL_REFERENCES]\"\n",
    "\n",
    "    elif criterion == \"flow\":\n",
    "        if \"conclusion\" in revised and \"introduction\" in revised:\n",
    "            cons = revised[\"conclusion\"]\n",
    "            intro_title = \"the introduction\"\n",
    "            trans = \"In line with the introduction, this conclusion revisits the main aims and synthesizes the outcomes.\"\n",
    "            if trans not in cons:\n",
    "                revised[\"conclusion\"] = trans + \"\\n\\n\" + cons\n",
    "\n",
    "    elif criterion == \"style\":\n",
    "        for name, content in list(revised.items()):\n",
    "            for informal in [\" really \", \" a lot \", \" got \", \" stuff \", \" thing \"]:\n",
    "                if informal in content:\n",
    "                    content = content.replace(informal, \" \")\n",
    "            revised[name] = content\n",
    "\n",
    "    return revised\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. SAVE OUTPUTS\n",
    "# -------------------------\n",
    "def save_critique_results(critique_results: Dict[str, Any], outputs_dir: str = \"outputs\", iteration: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Save critique feedback JSON to outputs/critique_feedback_iteration_{n}.json\n",
    "\n",
    "    Returns:\n",
    "        Path (string) to saved JSON file\n",
    "    \"\"\"\n",
    "    out_path = Path(outputs_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    filename = out_path / f\"critique_feedback_iteration_{iteration}.json\"\n",
    "    try:\n",
    "        with filename.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(critique_results, fh, indent=2, ensure_ascii=False)\n",
    "        print(f\"Critique feedback saved: {filename.name}\")\n",
    "        return str(filename)\n",
    "    except Exception as exc:\n",
    "        print(f\"Error saving critique feedback: {exc}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def save_revised_draft(revised_draft: str, outputs_dir: str = \"outputs\", iteration: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Save revised draft markdown and plain text files.\n",
    "\n",
    "    Returns:\n",
    "        Path to the primary saved markdown file (string).\n",
    "    \"\"\"\n",
    "    out_path = Path(outputs_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    md_file = out_path / f\"revised_draft_iteration_{iteration}.md\"\n",
    "    txt_file = out_path / f\"revised_draft_iteration_{iteration}.txt\"\n",
    "\n",
    "    try:\n",
    "        with md_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            fh.write(revised_draft)\n",
    "        with txt_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            fh.write(revised_draft)\n",
    "        print(f\"Revised draft saved: {md_file.name}\")\n",
    "        return str(md_file)\n",
    "    except Exception as exc:\n",
    "        print(f\"Error saving revised draft: {exc}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def save_revision_summary(\n",
    "    original_critique: Dict[str, Any],\n",
    "    revised_critique: Optional[Dict[str, Any]],\n",
    "    iterations: int,\n",
    "    outputs_dir: str = \"outputs\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Save a revision summary JSON that compares original and final critique scores.\n",
    "\n",
    "    Returns:\n",
    "        Path (string) to saved summary file.\n",
    "    \"\"\"\n",
    "    out_path = Path(outputs_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    summary = {\n",
    "        \"revision_date\": datetime.now().isoformat(),\n",
    "        \"total_iterations\": iterations,\n",
    "        \"improvement_summary\": {\n",
    "            \"original_score\": original_critique.get(\"score\", 0),\n",
    "            \"final_score\": revised_critique.get(\"score\", 0) if revised_critique else original_critique.get(\"score\", 0),\n",
    "            \"improvement\": (revised_critique.get(\"score\", 0) - original_critique.get(\"score\", 0)) if revised_critique else 0,\n",
    "        },\n",
    "        \"issues_resolved\": [],\n",
    "        \"remaining_issues\": [],\n",
    "    }\n",
    "\n",
    "    if revised_critique:\n",
    "        for crit in original_critique.get(\"checks\", {}):\n",
    "            orig_passed = bool(original_critique[\"checks\"][crit].get(\"passed\", False))\n",
    "            new_passed = bool(revised_critique[\"checks\"].get(crit, {}).get(\"passed\", False))\n",
    "            if not orig_passed and new_passed:\n",
    "                summary[\"issues_resolved\"].append(crit)\n",
    "            elif not orig_passed and not new_passed:\n",
    "                summary[\"remaining_issues\"].append(crit)\n",
    "\n",
    "    filename = out_path / \"revision_summary.json\"\n",
    "    try:\n",
    "        with filename.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(summary, fh, indent=2, ensure_ascii=False)\n",
    "        print(f\"Revision summary saved: {filename.name}\")\n",
    "        return str(filename)\n",
    "    except Exception as exc:\n",
    "        print(f\"Error saving revision summary: {exc}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. MAIN PIPELINE\n",
    "# -------------------------\n",
    "def run_draft_aggregation_and_critique(max_iterations: int = 2, outputs_dir: str = \"outputs\") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Main pipeline to aggregate draft, critique, run revisions, and save results.\n",
    "\n",
    "    Args:\n",
    "        max_iterations: Number of automated revision cycles to run (conservative edits).\n",
    "        outputs_dir: Directory to read/write outputs.\n",
    "\n",
    "    Returns:\n",
    "        summary dict with initial/final critiques and file paths, or None if pipeline fails.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"DRAFT AGGREGATION & CRITIQUE MODULE\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    print(\"STEP 1: Loading generated draft...\")\n",
    "    draft_text = load_latest_draft(outputs_dir)\n",
    "    if not draft_text:\n",
    "        return None\n",
    "\n",
    "    print(\"Loading individual sections...\")\n",
    "    sections = load_individual_sections(outputs_dir)\n",
    "    print(f\"  Loaded {len(sections)} sections\")\n",
    "\n",
    "    print(\"STEP 2: Creating full markdown draft...\")\n",
    "    full_draft = create_full_draft_markdown(sections)\n",
    "\n",
    "    # Save initial full draft snapshot\n",
    "    out_path = Path(outputs_dir)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "    initial_file = out_path / \"full_draft_initial.md\"\n",
    "    try:\n",
    "        with initial_file.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            fh.write(full_draft)\n",
    "        print(f\"Full draft saved: {initial_file.name}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"Error saving initial draft: {exc}\")\n",
    "\n",
    "    # Step 3: Run critique\n",
    "    print(\"STEP 3: Running draft critique...\")\n",
    "    critic = DraftCritique()\n",
    "    critique_results = critic.critique_draft(full_draft, sections)\n",
    "    print(f\"Critique Score: {critique_results['score']}/{critique_results['total_checks']}\")\n",
    "\n",
    "    # Save initial critique\n",
    "    save_critique_results(critique_results, outputs_dir, iteration=1)\n",
    "\n",
    "    # Step 4: Revision cycles\n",
    "    current_sections = sections\n",
    "    current_critique = critique_results\n",
    "    revised_files: List[str] = []\n",
    "    revised_critique: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        print(f\"\\nIteration {iteration}/{max_iterations}\")\n",
    "        revised_draft, revised_sections = run_revision_cycle(full_draft, current_sections, current_critique, iteration)\n",
    "        saved_path = save_revised_draft(revised_draft, outputs_dir, iteration)\n",
    "        revised_files.append(saved_path)\n",
    "\n",
    "        revised_critique = critic.critique_draft(revised_draft, revised_sections)\n",
    "        save_critique_results(revised_critique, outputs_dir, iteration + 1)\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        current_sections = revised_sections\n",
    "        current_critique = revised_critique\n",
    "        # update full_draft for the next revision cycle\n",
    "        full_draft = revised_draft\n",
    "        print(f\"  Score after revision {iteration}: {revised_critique['score']}/{revised_critique['total_checks']}\")\n",
    "\n",
    "    # Step 5: Create final summary\n",
    "    print(\"\\nSTEP 5: Creating revision summary...\")\n",
    "    summary_file = save_revision_summary(critique_results, revised_critique, max_iterations, outputs_dir)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 72)\n",
    "    print(\"COMPLETE!\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    summary = {\n",
    "        \"initial_draft_file\": str(initial_file),\n",
    "        \"initial_critique\": critique_results,\n",
    "        \"revised_files\": revised_files,\n",
    "        \"final_critique\": revised_critique,\n",
    "        \"revision_summary_file\": summary_file,\n",
    "    }\n",
    "\n",
    "    # Print short overview\n",
    "    print(\"\\nOUTPUTS GENERATED:\")\n",
    "    print(f\" • {initial_file.name} - Initial full draft\")\n",
    "    for i, f in enumerate(revised_files, 1):\n",
    "        print(f\" • revised_draft_iteration_{i}.md - Revised draft v{i}\")\n",
    "    print(f\" • revision_summary.json - Revision summary\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 7. PREVIEW FUNCTION\n",
    "# -------------------------\n",
    "def preview_critique_results(outputs_dir: str = \"outputs\") -> None:\n",
    "    \"\"\"\n",
    "    Preview the latest critique JSON and print top suggestions.\n",
    "\n",
    "    Args:\n",
    "        outputs_dir: Directory with critique JSON files.\n",
    "    \"\"\"\n",
    "    out_path = Path(outputs_dir)\n",
    "    critique_files = sorted(out_path.glob(\"critique_feedback_iteration_*.json\"), key=lambda p: p.stat().st_mtime)\n",
    "    if not critique_files:\n",
    "        print(\"No critique files found\")\n",
    "        return\n",
    "\n",
    "    latest = critique_files[-1]\n",
    "    try:\n",
    "        with latest.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "            data = json.load(fh)\n",
    "        print(\"\\n\" + \"=\" * 72)\n",
    "        print(\"CRITIQUE RESULTS PREVIEW\")\n",
    "        print(\"=\" * 72)\n",
    "        print(f\"File: {latest.name}\")\n",
    "        print(f\"Score: {data.get('score', 0)}/{data.get('total_checks', 0)}\")\n",
    "        print(\"\\nTop Suggestions:\")\n",
    "        for i, sug in enumerate(data.get(\"suggestions\", [])[:5], 1):\n",
    "            print(f\"{i}. {sug}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"Error reading critique file {latest.name}: {exc}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 8. ENTRYPOINT\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_draft_aggregation_and_critique(max_iterations=2)\n",
    "    if results:\n",
    "        print(\"\\nDRAFT AGGREGATION & CRITIQUE SUCCESSFUL!\")\n",
    "        try:\n",
    "            preview = input(\"Would you like to preview critique results? (y/n): \").strip().lower()\n",
    "        except Exception:\n",
    "            preview = \"n\"\n",
    "        if preview and preview.startswith(\"y\"):\n",
    "            preview_critique_results()\n",
    "    else:\n",
    "        print(\"Draft aggregation & critique did not run to completion.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
