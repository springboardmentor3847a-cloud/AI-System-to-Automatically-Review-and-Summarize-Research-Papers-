# -*- coding: utf-8 -*-
"""milestone 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QOrUCpm4OjPMYNyMlc6b7YEqa6NPjyEW

my api key: wFKolR3bfa5XUZaFntmdo5AXd7kL506y1klYRd3y
"""

# ============================================================
# MODULE 0: SEMANTIC SCHOLAR API CONFIGURATION
# ============================================================
"""
This module handles:
‚úî Secure API key loading
‚úî Semantic Scholar client initialization
‚úî Safe fallback to limited-access mode

All other modules import and reuse this client.
"""

import os
from dotenv import load_dotenv
from semanticscholar import SemanticScholar


def get_semantic_scholar_client():
    """
    Initializes Semantic Scholar API client.

    Priority:
    1. Environment variable (recommended)
    2. Manual API key (fallback)
    3. Limited-access mode

    Returns:
        SemanticScholar object
    """

    # Load .env variables (if available)
    load_dotenv()

    # OPTION 1: Environment variable
    api_key = os.getenv("wFKolR3bfa5XUZaFntmdo5AXd7kL506y1klYRd3y")

    # OPTION 2: MANUAL KEY (PUT YOUR KEY HERE IF NEEDED)
    # ‚ö† REMOVE BEFORE SUBMISSION
    MANUAL_API_KEY = ""  # <-- Paste your API key here if needed

    if api_key:
        print("üîë API key loaded from environment.")
        return SemanticScholar(api_key=api_key)

    elif MANUAL_API_KEY:
        print("üîë API key loaded manually.")
        return SemanticScholar(api_key=MANUAL_API_KEY)

    else:
        print("‚ö† No API key found.")
        print("‚ö† Running in LIMITED API mode.")
        return SemanticScholar()

# MODULE 1: Topic Input & Paper Search (LIMITED TO 50 PAPERS)

!pip install semanticscholar python-dotenv requests tabulate pandas -q

import json
import os
import pandas as pd
from tabulate import tabulate
from semanticscholar import SemanticScholar
from dotenv import load_dotenv


# ------------------------------------------------------------
# 0. CREATE PROJECT FOLDER STRUCTURE
# ------------------------------------------------------------
def create_project_folders():
    folders = [
        "data",
        "data/search_results",
        "data/raw_pdfs",
        "data/metadata"
    ]
    for folder in folders:
        os.makedirs(folder, exist_ok=True)
    print("üìÅ Project folders verified/created.")


# ------------------------------------------------------------
# 1. SETUP API KEY
# ------------------------------------------------------------
def setup_api_key():
    load_dotenv()
    API_KEY = os.getenv("wFKolR3bfa5XUZaFntmdo5AXd7kL506y1klYRd3y")

    if not API_KEY:
        print("‚ö† No API key found. Running with LIMITED API rate.")
        return SemanticScholar()
    else:
        print("üîë API Key loaded successfully.")
        return SemanticScholar(api_key=API_KEY)


# ------------------------------------------------------------
# 2. SEARCH PAPERS (STRICT LIMIT = 50)
# ------------------------------------------------------------
def search_papers(topic, year_filter=None, open_access_only=False):
    sch = setup_api_key()

    MAX_PAPERS = 50
    print(f"\nüîé Searching for papers on: '{topic}'")
    print(f"üì° Fetching up to {MAX_PAPERS} papers from Semantic Scholar...")

    try:
        results = sch.search_paper(
            query=topic,
            limit=MAX_PAPERS,
            fields=[
                "paperId", "title", "abstract", "year", "authors",
                "citationCount", "openAccessPdf", "url", "venue"
            ]
        )

        papers = []

        for paper in results:
            if len(papers) >= MAX_PAPERS:
                break  # HARD STOP

            # Apply optional filters
            if year_filter and paper.year != year_filter:
                continue
            if open_access_only and not paper.openAccessPdf:
                continue

            papers.append({
                "paperId": paper.paperId,
                "title": paper.title,
                "authors": [a["name"] for a in paper.authors] if paper.authors else [],
                "year": paper.year,
                "abstract": (paper.abstract[:300] + "...") if paper.abstract else "No abstract available",
                "citationCount": paper.citationCount,
                "venue": paper.venue,
                "url": paper.url,
                "pdf_url": paper.openAccessPdf["url"] if paper.openAccessPdf else None,
                "has_pdf": bool(paper.openAccessPdf)
            })

        print("\nüìä Search Completed!")
        print(f"   ‚û§ Total papers collected: {len(papers)}")
        print(f"   ‚û§ Papers with PDF: {sum(p['has_pdf'] for p in papers)}")

        return {
            "topic": topic,
            "total_results": len(papers),
            "papers": papers
        }

    except Exception as e:
        print(f"‚ùå Error during search: {e}")
        return None


# ------------------------------------------------------------
# 3. SAVE RESULTS TO JSON + CSV
# ------------------------------------------------------------
def save_search_results(data):
    topic_clean = data["topic"].replace(" ", "_")
    json_path = f"data/search_results/{topic_clean}.json"
    csv_path = f"data/search_results/{topic_clean}.csv"

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)
    print(f"üíæ JSON saved to: {json_path}")

    df = pd.DataFrame(data["papers"])
    df.to_csv(csv_path, index=False)
    print(f"üìä CSV saved to: {csv_path}")

    return json_path, csv_path


# ------------------------------------------------------------
# 4. DISPLAY RESULTS
# ------------------------------------------------------------
def display_search_results(data):
    papers = data["papers"]

    print("\n" + "="*90)
    print(f"SEARCH SUMMARY: {data['topic']}")
    print("="*90)

    print(f"\nüìå Total Papers: {len(papers)}")
    print(f"üìå Papers with PDF: {sum(p['has_pdf'] for p in papers)}")
    print(f"üìå Papers without PDF: {sum(not p['has_pdf'] for p in papers)}")

    table_data = []
    for p in papers[:10]:
        table_data.append([
            p["title"][:40] + ("..." if len(p["title"]) > 40 else ""),
            p["year"],
            p["citationCount"],
            "Yes" if p["has_pdf"] else "No"
        ])

    print("\nüìù TOP PAPERS TABLE:")
    print(tabulate(table_data, headers=["Title", "Year", "Citations", "PDF"], tablefmt="pretty"))


# ------------------------------------------------------------
# 5. MAIN EXECUTION
# ------------------------------------------------------------
def main_search():
    create_project_folders()

    print("\n" + "="*80)
    print(" MODULE 1: TOPIC INPUT & PAPER SEARCH")
    print("="*80)

    topic = input("\nEnter research topic: ").strip() or "machine learning"

    year_input = input("Filter by year? (Press Enter to skip): ").strip()
    year_filter = int(year_input) if year_input.isdigit() else None

    oa_input = input("Open access only? (y/n): ").lower().strip()
    open_access_only = oa_input == "y"

    results = search_papers(
        topic,
        year_filter=year_filter,
        open_access_only=open_access_only
    )

    if not results or results["total_results"] == 0:
        print("\n‚ùå No papers found.")
        return None

    json_path, csv_path = save_search_results(results)
    display_search_results(results)

    print("\n‚úÖ Module 1 Completed Successfully!")
    print(f"‚û° JSON saved at: {json_path}")
    print(f"‚û° CSV saved at: {csv_path}")
    print("‚û° Proceed to Module 2 for downloading PDFs.")

    return results


# RUN MODULE 1
main_search()

# ============================================
# MODULE 2: Paper Selection & PDF Download
# (FIXED & ROBUST VERSION)
# ============================================

import json
import os
import requests

SEARCH_DIR = "data/search_results"
DOWNLOAD_DIR = "downloads"
os.makedirs(DOWNLOAD_DIR, exist_ok=True)

# ------------------------------------------------------------
# 1. LOAD SEARCH RESULTS (SAFE VERSION)
# ------------------------------------------------------------
def load_search_results():
    if not os.path.exists(SEARCH_DIR):
        raise FileNotFoundError(
            "Search results folder not found. Run Module 1 first."
        )

    files = [f for f in os.listdir(SEARCH_DIR) if f.endswith(".json")]

    if not files:
        raise FileNotFoundError(
            "No search result JSON found. Run Module 1 first."
        )

    # Pick latest file automatically
    files.sort(key=lambda f: os.path.getmtime(os.path.join(SEARCH_DIR, f)), reverse=True)
    filepath = os.path.join(SEARCH_DIR, files[0])

    print(f"üìÇ Loading search results from: {filepath}")

    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)

    return data

# ------------------------------------------------------------
# 2. FILTER PAPERS WITH PDFs
# ------------------------------------------------------------
def filter_papers_with_pdf(papers):
    pdf_papers = []
    for p in papers:
        if p.get("pdf_url"):
            pdf_papers.append(p)
    print(f"üìÑ Papers with PDF: {len(pdf_papers)}")
    return pdf_papers

# ------------------------------------------------------------
# 3. DOWNLOAD PDFS
# ------------------------------------------------------------
def download_pdfs(limit=3):
    data = load_search_results()
    papers = filter_papers_with_pdf(data["papers"])

    downloaded = 0

    for i, paper in enumerate(papers[:limit]):
        pdf_url = paper["pdf_url"]
        title = paper["title"][:50].replace(" ", "_")

        filename = f"{DOWNLOAD_DIR}/{i+1}_{title}.pdf"

        try:
            print(f"\n‚¨á Downloading: {paper['title'][:60]}...")
            r = requests.get(pdf_url, timeout=30)

            if r.status_code == 200 and r.content[:4] == b"%PDF":
                with open(filename, "wb") as f:
                    f.write(r.content)
                downloaded += 1
                print(f"   ‚úÖ Saved as {filename}")
            else:
                print("   ‚ùå Not a valid PDF")

        except Exception as e:
            print("   ‚ùå Download failed:", e)

    print(f"\nüìä Total PDFs downloaded: {downloaded}")

# ------------------------------------------------------------
# RUN MODULE 2
# ------------------------------------------------------------
download_pdfs(limit=3)

# ============================================================
# MILESTONE 2 (WEEK 3‚Äì4)
# MODULE 3 + MODULE 4
# PDF TEXT EXTRACTION, ANALYSIS & CROSS-PAPER COMPARISON
# ============================================================

"""
WHAT THIS CODE DOES (HIGH-LEVEL):

1. Loads research paper PDFs downloaded in Module 2
2. Extracts text from each PDF using robust strategies
3. Cleans noisy PDF text
4. Extracts academic sections:
   - Title
   - Abstract
   - Introduction
   - Methods
   - Results
   - Conclusion
5. Stores extracted content in structured JSON format
6. Extracts key findings from Results & Conclusion
7. Validates correctness and completeness
8. Compares findings across all papers
9. Generates summary + comparison reports

THIS DIRECTLY SATISFIES:
‚úî Text extraction module
‚úî Section-wise parsing
‚úî Structured storage
‚úî Key finding extraction
‚úî Cross-paper comparison
‚úî Validation of correctness
"""

# ------------------------------------------------------------
# INSTALL REQUIRED LIBRARIES
# ------------------------------------------------------------
!pip install pymupdf pymupdf4llm tqdm -q

import os
import re
import json
from pathlib import Path
from collections import Counter
from datetime import datetime
from tqdm import tqdm
import fitz                     # PyMuPDF
import pymupdf4llm              # Layout-aware extraction

# ------------------------------------------------------------
# CONFIGURATION
# ------------------------------------------------------------
DOWNLOAD_DIR = "downloads"          # PDFs from Module 2
OUTPUT_DIR = "data/extracted"       # Structured output
os.makedirs(OUTPUT_DIR, exist_ok=True)

print("üìÅ Using PDF directory:", DOWNLOAD_DIR)
print("üìÅ Extracted data stored in:", OUTPUT_DIR)

# ------------------------------------------------------------
# 1. BASIC TEXT CLEANING (ENHANCEMENT)
# ------------------------------------------------------------
def clean_text(text):
    """
    Cleans raw PDF text:
    - Removes excessive spaces
    - Fixes broken hyphenated words
    - Removes unreadable characters
    """
    if not text:
        return ""

    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'-\s+', '', text)   # Fix line-break hyphens
    text = ''.join(c for c in text if ord(c) >= 32)
    return text.strip()

# ------------------------------------------------------------
# 2. PDF TEXT EXTRACTION
# ------------------------------------------------------------
def extract_text_from_pdf(pdf_path):
    """
    Extract text using TWO strategies and select best output:
    1) pymupdf4llm (layout-aware, better for research papers)
    2) Standard PyMuPDF extraction (fallback)
    """

    print(f"\nüìÑ Extracting text from: {pdf_path.name}")

    try:
        doc = fitz.open(pdf_path)
        extracted_versions = []

        # Strategy 1: Layout-aware extraction
        try:
            md_text = pymupdf4llm.to_markdown(str(pdf_path))
            if md_text and len(md_text) > 1000:
                extracted_versions.append(md_text)
                print("   ‚úî Used layout-aware extraction")
        except:
            pass

        # Strategy 2: Regular extraction (fallback)
        raw_text = ""
        for page in doc[:min(40, len(doc))]:
            raw_text += page.get_text()

        if len(raw_text) > 1000:
            extracted_versions.append(raw_text)
            print("   ‚úî Used standard text extraction")

        doc.close()

        if not extracted_versions:
            print("   ‚ùå No usable text found")
            return None

        # Select longest version (most content)
        best_text = max(extracted_versions, key=len)
        return clean_text(best_text)

    except Exception as e:
        print(f"   ‚ùå Extraction failed: {e}")
        return None

# ------------------------------------------------------------
# 3. SECTION-WISE TEXT EXTRACTION
# ------------------------------------------------------------
def extract_sections(text):
    """
    Extracts standard research paper sections using regex patterns
    """

    sections = {
        "title": "",
        "abstract": "",
        "introduction": "",
        "methods": "",
        "results": "",
        "conclusion": ""
    }

    lines = text.split("\n")

    patterns = {
        "abstract": r"\babstract\b",
        "introduction": r"\bintroduction\b",
        "methods": r"\b(methods?|methodology|experiment)\b",
        "results": r"\b(results?|findings)\b",
        "conclusion": r"\b(conclusion|discussion)\b"
    }

    boundaries = {}

    for i, line in enumerate(lines):
        for section, pattern in patterns.items():
            if re.search(pattern, line.lower()):
                boundaries.setdefault(section, i)

    sorted_sections = sorted(boundaries.items(), key=lambda x: x[1])

    for idx, (section, start) in enumerate(sorted_sections):
        end = sorted_sections[idx + 1][1] if idx + 1 < len(sorted_sections) else len(lines)
        content = " ".join(lines[start:end]).strip()
        if len(content) > 200:
            sections[section] = content[:5000]

    # Title extraction (first meaningful line)
    for line in lines[:10]:
        if 20 < len(line) < 200:
            sections["title"] = line
            break

    return sections

# ------------------------------------------------------------
# 4. KEY FINDINGS EXTRACTION (MODULE 4)
# ------------------------------------------------------------
def extract_key_findings(sections, max_findings=5):
    """
    Extracts important conclusions from Results + Conclusion
    """

    keywords = [
        "we found", "results show", "our results",
        "significant", "improves", "outperforms",
        "demonstrates", "indicates", "we conclude"
    ]

    findings = []
    combined = sections.get("results", "") + " " + sections.get("conclusion", "")
    sentences = re.split(r'(?<=[.!?])\s+', combined)

    for sentence in sentences:
        if any(k in sentence.lower() for k in keywords):
            findings.append(sentence.strip())
        if len(findings) >= max_findings:
            break

    return findings

# ------------------------------------------------------------
# 5. VALIDATION OF CORRECTNESS
# ------------------------------------------------------------
def validate_extraction(paper):
    """
    Ensures extracted data is complete and meaningful
    """

    return {
        "has_abstract": len(paper["sections"].get("abstract", "")) > 200,
        "has_multiple_sections": sum(len(v) > 200 for v in paper["sections"].values()) >= 2,
        "has_key_findings": len(paper["key_findings"]) > 0,
        "sufficient_text": paper["stats"]["total_chars"] > 1000
    }

# ------------------------------------------------------------
# 6. PROCESS A SINGLE PDF (END-TO-END)
# ------------------------------------------------------------
def process_pdf(pdf_path):
    """
    Complete processing of ONE research paper
    """

    raw_text = extract_text_from_pdf(pdf_path)
    if not raw_text:
        return None

    sections = extract_sections(raw_text)
    key_findings = extract_key_findings(sections)

    paper = {
        "paper_id": pdf_path.stem,
        "filename": pdf_path.name,
        "sections": sections,
        "key_findings": key_findings,
        "stats": {
            "total_chars": len(raw_text),
            "sections_found": sum(len(v) > 200 for v in sections.values())
        }
    }

    paper["validation"] = validate_extraction(paper)
    return paper

# ------------------------------------------------------------
# 7. CROSS-PAPER COMPARISON
# ------------------------------------------------------------
def compare_papers(papers):
    """
    Finds common themes across all papers
    """

    all_findings = []
    for paper in papers:
        all_findings.extend(paper["key_findings"])

    words = []
    for finding in all_findings:
        words.extend(re.findall(r'\b[a-zA-Z]{4,}\b', finding.lower()))

    common_terms = Counter(words).most_common(10)

    return {
        "total_papers": len(papers),
        "total_findings": len(all_findings),
        "common_terms": common_terms
    }

# ------------------------------------------------------------
# 8. RUN COMPLETE PIPELINE
# ------------------------------------------------------------
def run_milestone_2():
    """
    Runs complete Milestone-2 pipeline
    """

    pdf_files = list(Path(DOWNLOAD_DIR).glob("*.pdf"))

    print("\n==============================")
    print("MILESTONE 2 PIPELINE STARTED")
    print("==============================")
    print(f"üìÑ PDFs found: {len(pdf_files)}")

    if not pdf_files:
        print("‚ùå No PDFs found. Run Module 2 first.")
        return

    results = []

    for pdf in tqdm(pdf_files):
        paper = process_pdf(pdf)
        if paper:
            results.append(paper)
            with open(f"{OUTPUT_DIR}/{paper['paper_id']}.json", "w", encoding="utf-8") as f:
                json.dump(paper, f, indent=2)

    comparison = compare_papers(results)

    with open(f"{OUTPUT_DIR}/comparison_report.json", "w", encoding="utf-8") as f:
        json.dump(comparison, f, indent=2)

    print("\n‚úÖ MILESTONE 2 COMPLETED SUCCESSFULLY")
    print(f"üìä Papers processed: {len(results)}")
    print("üìÅ Output stored in:", OUTPUT_DIR)

    return results, comparison

# ------------------------------------------------------------
# RUN
# ------------------------------------------------------------
results, comparison = run_milestone_2()

# ============================================================
# MILESTONE 2 (WEEK 3‚Äì4)
# PDF TEXT EXTRACTION, SECTION ANALYSIS & CROSS-PAPER COMPARISON
# ============================================================



# ------------------------------------------------------------
# 0. INSTALL REQUIRED LIBRARIES
# ------------------------------------------------------------
# PyMuPDF: robust PDF reading
# pymupdf4llm: layout-aware extraction (works well for structured research PDFs)
# tqdm: progress bar for loops
!pip install pymupdf pymupdf4llm tqdm -q

# ------------------------------------------------------------
# 1. IMPORT MODULES
# ------------------------------------------------------------
import os
import re
import json
from pathlib import Path
from collections import Counter
from tqdm import tqdm
import fitz                # PyMuPDF
import pymupdf4llm         # Layout-aware text extraction

# ------------------------------------------------------------
# 2. CONFIGURATION
# ------------------------------------------------------------
# Directory where PDFs are stored (downloaded in Module 2)
DOWNLOAD_DIR = "downloads"

# Directory to save structured JSON outputs
OUTPUT_DIR = "data/extracted"
os.makedirs(OUTPUT_DIR, exist_ok=True)

print("üìÅ Using PDF directory:", DOWNLOAD_DIR)
print("üìÅ Extracted data stored in:", OUTPUT_DIR)

# ------------------------------------------------------------
# 3. TEXT CLEANING FUNCTION
# ------------------------------------------------------------
def clean_text(text):
    """
    Cleans the raw PDF text.

    Steps:
    1. Remove multiple consecutive spaces.
    2. Fix line-break hyphenation (e.g., "ex-\nample" -> "example").
    3. Remove non-printable/unreadable characters.

    Returns:
        Cleaned string.
    """
    if not text:
        return ""

    # Replace multiple spaces with single space
    text = re.sub(r'\s+', ' ', text)

    # Remove hyphenation at line breaks
    text = re.sub(r'-\s+', '', text)

    # Remove unreadable characters
    text = ''.join(c for c in text if ord(c) >= 32)

    return text.strip()

# ------------------------------------------------------------
# 4. PDF TEXT EXTRACTION FUNCTION
# ------------------------------------------------------------
def extract_text_from_pdf(pdf_path):
    """
    Extract text from PDF using TWO strategies:

    Strategy 1: Layout-aware extraction (pymupdf4llm) ‚Üí better for academic PDFs.
    Strategy 2: Standard PyMuPDF extraction ‚Üí fallback if layout-aware fails.

    Returns:
        Best cleaned text (string) or None if extraction fails.
    """
    print(f"\nüìÑ Extracting text from: {pdf_path.name}")

    try:
        # Open PDF with PyMuPDF
        doc = fitz.open(pdf_path)
        extracted_versions = []

        # --- Strategy 1: Layout-aware extraction ---
        try:
            layout_text = pymupdf4llm.to_markdown(str(pdf_path))
            if layout_text and len(layout_text) > 1000:  # Only accept meaningful content
                extracted_versions.append(layout_text)
                print("   ‚úî Used layout-aware extraction")
        except Exception as e:
            print(f"   ‚ö† Layout-aware extraction failed: {e}")

        # --- Strategy 2: Standard PyMuPDF extraction (fallback) ---
        raw_text = ""
        for page in doc[:min(40, len(doc))]:  # Limit to first 40 pages for speed
            raw_text += page.get_text()
        if len(raw_text) > 1000:
            extracted_versions.append(raw_text)
            print("   ‚úî Used standard text extraction")

        doc.close()

        if not extracted_versions:
            print("   ‚ùå No usable text found")
            return None

        # Select the longest text (most complete)
        best_text = max(extracted_versions, key=len)
        return clean_text(best_text)

    except Exception as e:
        print(f"   ‚ùå Extraction failed: {e}")
        return None

# ------------------------------------------------------------
# 5. SECTION-WISE EXTRACTION FUNCTION
# ------------------------------------------------------------
def extract_sections(text):
    """
    Extract standard research paper sections using regex.

    Sections:
        - Title: first meaningful line
        - Abstract
        - Introduction
        - Methods/Methodology/Experiment
        - Results/Findings
        - Conclusion/Discussion

    Returns:
        Dictionary of sections with content.
    """
    sections = {
        "title": "",
        "abstract": "",
        "introduction": "",
        "methods": "",
        "results": "",
        "conclusion": ""
    }

    lines = text.split("\n")

    # Regex patterns for section headings
    patterns = {
        "abstract": r"\babstract\b",
        "introduction": r"\bintroduction\b",
        "methods": r"\b(methods?|methodology|experiment)\b",
        "results": r"\b(results?|findings)\b",
        "conclusion": r"\b(conclusion|discussion)\b"
    }

    boundaries = {}  # Stores line indices of section starts

    # Detect section start lines
    for i, line in enumerate(lines):
        for section, pattern in patterns.items():
            if re.search(pattern, line.lower()):
                boundaries.setdefault(section, i)

    # Sort sections by start line
    sorted_sections = sorted(boundaries.items(), key=lambda x: x[1])

    # Extract section content based on line boundaries
    for idx, (section, start) in enumerate(sorted_sections):
        end = sorted_sections[idx + 1][1] if idx + 1 < len(sorted_sections) else len(lines)
        content = " ".join(lines[start:end]).strip()
        if len(content) > 200:  # Only keep meaningful content
            sections[section] = content[:5000]  # Limit size for JSON

    # Title: first meaningful line in first 10 lines
    for line in lines[:10]:
        if 20 < len(line) < 200:
            sections["title"] = line.strip()
            break

    return sections

# ------------------------------------------------------------
# 6. KEY FINDINGS EXTRACTION FUNCTION
# ------------------------------------------------------------
def extract_key_findings(sections, max_findings=5):
    """
    Extracts important results/conclusions from Results + Conclusion sections.

    Looks for sentences containing keywords like:
        - we found, results show, significant, improves, outperforms, indicates

    Returns:
        List of key sentences (max `max_findings`)
    """
    keywords = [
        "we found", "results show", "our results",
        "significant", "improves", "outperforms",
        "demonstrates", "indicates", "we conclude"
    ]

    findings = []
    combined_text = sections.get("results", "") + " " + sections.get("conclusion", "")
    sentences = re.split(r'(?<=[.!?])\s+', combined_text)

    for sentence in sentences:
        if any(k in sentence.lower() for k in keywords):
            findings.append(sentence.strip())
        if len(findings) >= max_findings:
            break

    return findings

# ------------------------------------------------------------
# 7. VALIDATION FUNCTION
# ------------------------------------------------------------
def validate_extraction(paper):
    """
    Checks whether extracted data is complete and meaningful.

    Criteria:
        - Abstract exists (>200 chars)
        - At least 2 sections exist
        - Key findings exist
        - Total text is sufficiently long (>1000 chars)

    Returns:
        Dictionary of validation flags
    """
    return {
        "has_abstract": len(paper["sections"].get("abstract", "")) > 200,
        "has_multiple_sections": sum(len(v) > 200 for v in paper["sections"].values()) >= 2,
        "has_key_findings": len(paper["key_findings"]) > 0,
        "sufficient_text": paper["stats"]["total_chars"] > 1000
    }

# ------------------------------------------------------------
# 8. PROCESS SINGLE PDF
# ------------------------------------------------------------
def process_pdf(pdf_path):
    """
    End-to-end processing of one research paper:
        1. Extract text
        2. Extract sections
        3. Extract key findings
        4. Collect stats
        5. Validate
    """
    raw_text = extract_text_from_pdf(pdf_path)
    if not raw_text:
        return None

    sections = extract_sections(raw_text)
    key_findings = extract_key_findings(sections)

    paper = {
        "paper_id": pdf_path.stem,
        "filename": pdf_path.name,
        "sections": sections,
        "key_findings": key_findings,
        "stats": {
            "total_chars": len(raw_text),
            "sections_found": sum(len(v) > 200 for v in sections.values())
        }
    }

    paper["validation"] = validate_extraction(paper)
    return paper

# ------------------------------------------------------------
# 9. CROSS-PAPER COMPARISON
# ------------------------------------------------------------
def compare_papers(papers):
    """
    Finds common themes across multiple papers:
        - Combines all key findings
        - Extracts most frequent words (>3 letters)

    Returns:
        Summary dictionary with total papers, total findings, and top 10 common terms
    """
    all_findings = []
    for paper in papers:
        all_findings.extend(paper["key_findings"])

    words = []
    for finding in all_findings:
        words.extend(re.findall(r'\b[a-zA-Z]{4,}\b', finding.lower()))

    common_terms = Counter(words).most_common(10)

    return {
        "total_papers": len(papers),
        "total_findings": len(all_findings),
        "common_terms": common_terms
    }

# ------------------------------------------------------------
# 10. RUN COMPLETE PIPELINE
# ------------------------------------------------------------
def run_milestone_2():
    """
    Runs the complete Milestone 2 pipeline:
        1. Process all PDFs
        2. Save JSONs for each paper
        3. Generate comparison report
    """
    pdf_files = list(Path(DOWNLOAD_DIR).glob("*.pdf"))

    print("\n==============================")
    print("MILESTONE 2 PIPELINE STARTED")
    print("==============================")
    print(f"üìÑ PDFs found: {len(pdf_files)}")

    if not pdf_files:
        print("‚ùå No PDFs found. Run Module 2 first.")
        return

    results = []

    for pdf in tqdm(pdf_files):
        paper = process_pdf(pdf)
        if paper:
            results.append(paper)
            # Save structured JSON for each paper
            with open(f"{OUTPUT_DIR}/{paper['paper_id']}.json", "w", encoding="utf-8") as f:
                json.dump(paper, f, indent=2)

    # Cross-paper comparison
    comparison = compare_papers(results)
    with open(f"{OUTPUT_DIR}/comparison_report.json", "w", encoding="utf-8") as f:
        json.dump(comparison, f, indent=2)

    print("\n‚úÖ MILESTONE 2 COMPLETED SUCCESSFULLY")
    print(f"üìä Papers processed: {len(results)}")
    print("üìÅ Output stored in:", OUTPUT_DIR)

    return results, comparison

# ------------------------------------------------------------
# 11. EXECUTE PIPELINE
# ------------------------------------------------------------
results, comparison = run_milestone_2()