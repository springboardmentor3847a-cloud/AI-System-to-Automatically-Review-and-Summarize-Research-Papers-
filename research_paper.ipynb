{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/springboardmentor3847a-cloud/AI-System-to-Automatically-Review-and-Summarize-Research-Papers-/blob/abhinav-dumbre-branch/research_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a9f5e8",
      "metadata": {
        "id": "f9a9f5e8"
      },
      "source": [
        "# Cleaned AI System Script\n",
        "Organized into separate cells for better readability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d2b953",
      "metadata": {
        "id": "03d2b953"
      },
      "source": [
        "## Setup / Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "90d68995",
      "metadata": {
        "id": "90d68995"
      },
      "outputs": [],
      "source": [
        "!pip install semanticscholar python-dotenv requests -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "from semanticscholar import SemanticScholar\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6074dd5c",
      "metadata": {
        "id": "6074dd5c"
      },
      "source": [
        "## Function: `setup_api_key`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8964daac",
      "metadata": {
        "id": "8964daac"
      },
      "outputs": [],
      "source": [
        "def setup_api_key():\n",
        "\n",
        "    load_dotenv()\n",
        "    API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
        "\n",
        "    if not API_KEY:\n",
        "\n",
        "        with open(\".env\", \"w\") as f:\n",
        "            f.write(\"SEMANTIC_SCHOLAR_API_KEY=83rBkeaXb14D8vGpXJezU6nrCFFmyn5L8RCvT9MM\\n\")\n",
        "        load_dotenv()\n",
        "        API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
        "\n",
        "    if API_KEY:\n",
        "        sch = SemanticScholar(api_key=API_KEY)\n",
        "        print(\"Semantic Scholar initialized with API key\")\n",
        "    else:\n",
        "        sch = SemanticScholar()\n",
        "        print(\" Using Semantic Scholar without API key (limited rate)\")\n",
        "\n",
        "    return sch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c9d19f",
      "metadata": {
        "id": "e0c9d19f"
      },
      "source": [
        "## Function: `search_papers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "21567523",
      "metadata": {
        "id": "21567523"
      },
      "outputs": [],
      "source": [
        "def search_papers(topic, limit=20):\n",
        "\n",
        "    print(f\"\\n Searching for papers on: '{topic}'\")\n",
        "    print(f\"   Requesting {limit} papers from Semantic Scholar...\")\n",
        "\n",
        "    sch = setup_api_key()\n",
        "\n",
        "    try:\n",
        "\n",
        "        results = sch.search_paper(\n",
        "            query=topic,\n",
        "            limit=limit,\n",
        "            fields=[\"paperId\", \"title\", \"abstract\", \"year\", \"authors\",\n",
        "                   \"citationCount\", \"openAccessPdf\", \"url\", \"venue\"]\n",
        "        )\n",
        "\n",
        "        papers = []\n",
        "        for paper in results:\n",
        "            paper_data = {\n",
        "                \"title\": paper.title,\n",
        "                \"authors\": [author['name'] for author in paper.authors] if paper.authors else [],\n",
        "                \"year\": paper.year,\n",
        "                \"paperId\": paper.paperId,\n",
        "                \"abstract\": paper.abstract[:300] + \"...\" if paper.abstract else \"No abstract available\",\n",
        "                \"citationCount\": paper.citationCount,\n",
        "                \"venue\": paper.venue if hasattr(paper, 'venue') else None,\n",
        "                \"url\": paper.url,\n",
        "                \"pdf_url\": paper.openAccessPdf['url'] if paper.openAccessPdf else None,\n",
        "                \"has_pdf\": bool(paper.openAccessPdf)\n",
        "            }\n",
        "            papers.append(paper_data)\n",
        "\n",
        "        papers_with_pdf = sum(1 for p in papers if p[\"has_pdf\"])\n",
        "\n",
        "        print(f\"Search complete!\")\n",
        "        print(f\"   Total papers found: {len(papers)}\")\n",
        "        print(f\"   Papers with PDF available: {papers_with_pdf}\")\n",
        "\n",
        "        return {\n",
        "            \"topic\": topic,\n",
        "            \"search_timestamp\": \"timestamp_placeholder\",\n",
        "            \"total_results\": len(papers),\n",
        "            \"papers_with_pdf\": papers_with_pdf,\n",
        "            \"papers\": papers\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error searching papers: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6360a00",
      "metadata": {
        "id": "e6360a00"
      },
      "source": [
        "## Function: `save_search_results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e33f9dac",
      "metadata": {
        "id": "e33f9dac"
      },
      "outputs": [],
      "source": [
        "def save_search_results(data, filename=None):\n",
        "\n",
        "    if not filename:\n",
        "\n",
        "        safe_topic = \"\".join(c for c in data[\"topic\"] if c.isalnum() or c == \" \").replace(\" \", \"_\")\n",
        "        filename = f\"paper_search_results_{safe_topic}.json\"\n",
        "\n",
        "    os.makedirs(\"data/search_results\", exist_ok=True)\n",
        "    filepath = os.path.join(\"data/search_results\", filename)\n",
        "\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\" Search results saved to: {filepath}\")\n",
        "    return filepath"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96d206f0",
      "metadata": {
        "id": "96d206f0"
      },
      "source": [
        "## Function: `display_search_results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fe948062",
      "metadata": {
        "id": "fe948062"
      },
      "outputs": [],
      "source": [
        "def display_search_results(data, max_display=10):\n",
        "\n",
        "    if not data or \"papers\" not in data:\n",
        "        print(\"No data to display\")\n",
        "        return\n",
        "\n",
        "    papers = data[\"papers\"]\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"SEARCH RESULTS: {data['topic']}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nStatistics:\")\n",
        "    print(f\"  • Total papers: {len(papers)}\")\n",
        "    print(f\"  • Papers with PDF: {sum(1 for p in papers if p['has_pdf'])}\")\n",
        "    print(f\"  • Papers without PDF: {sum(1 for p in papers if not p['has_pdf'])}\")\n",
        "\n",
        "    print(f\"\\n Top {min(max_display, len(papers))} Papers:\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for i, paper in enumerate(papers[:max_display]):\n",
        "        print(f\"\\n{i+1}. {paper['title'][:80]}{'...' if len(paper['title']) > 80 else ''}\")\n",
        "        print(f\"   Authors: {', '.join(paper['authors'][:3])}\" +\n",
        "              (\"...\" if len(paper['authors']) > 3 else \"\"))\n",
        "        print(f\"   Year: {paper['year']} | Citations: {paper['citationCount']}\")\n",
        "        print(f\"   PDF Available: {'✅' if paper['has_pdf'] else '❌'}\")\n",
        "        print(f\"   Abstract: {paper['abstract'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7594f9b",
      "metadata": {
        "id": "b7594f9b"
      },
      "source": [
        "## Function: `main_search`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "79e9eea4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79e9eea4",
        "outputId": "9d3cf4a0-5121-4d7d-ea91-d4690db2a658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MODULE 1: TOPIC INPUT & PAPER SEARCH\n",
            "================================================================================\n",
            "\n",
            "Enter research topic: React\n",
            "\n",
            " Searching for papers on: 'React'\n",
            "   Requesting 20 papers from Semantic Scholar...\n",
            "Semantic Scholar initialized with API key\n",
            "Search complete!\n",
            "   Total papers found: 1000\n",
            "   Papers with PDF available: 1000\n",
            " Search results saved to: data/search_results/paper_search_results_React.json\n",
            "\n",
            "================================================================================\n",
            "SEARCH RESULTS: React\n",
            "================================================================================\n",
            "\n",
            "Statistics:\n",
            "  • Total papers: 1000\n",
            "  • Papers with PDF: 1000\n",
            "  • Papers without PDF: 0\n",
            "\n",
            " Top 10 Papers:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. ReAct: Synergizing Reasoning and Acting in Language Models\n",
            "   Authors: Shunyu Yao, Jeffrey Zhao, Dian Yu...\n",
            "   Year: 2022 | Citations: 4807\n",
            "   PDF Available: ✅\n",
            "   Abstract: While large language models (LLMs) have demonstrated impressive capabilities across tasks in languag...\n",
            "\n",
            "2. MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action\n",
            "   Authors: Zhengyuan Yang, Linjie Li, Jianfeng Wang...\n",
            "   Year: 2023 | Citations: 484\n",
            "   PDF Available: ✅\n",
            "   Abstract: We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achi...\n",
            "\n",
            "3. REACT 2024: the Second Multiple Appropriate Facial Reaction Generation Challenge\n",
            "   Authors: Siyang Song, Micol Spitale, Cheng Luo...\n",
            "   Year: 2024 | Citations: 20\n",
            "   PDF Available: ✅\n",
            "   Abstract: In dyadic interactions, humans communicate their intentions and state of mind using verbal and non-v...\n",
            "\n",
            "4. ReAct: Out-of-distribution Detection With Rectified Activations\n",
            "   Authors: Yiyou Sun, Chuan Guo, Yixuan Li\n",
            "   Year: 2021 | Citations: 569\n",
            "   PDF Available: ✅\n",
            "   Abstract: Out-of-distribution (OOD) detection has received much attention lately due to its practical importan...\n",
            "\n",
            "5. ReAcTable: Enhancing ReAct for Table Question Answering\n",
            "   Authors: Yunjia Zhang, Jordan Henkel, Avrilia Floratou...\n",
            "   Year: 2023 | Citations: 90\n",
            "   PDF Available: ✅\n",
            "   Abstract: Table Question Answering (TQA) presents a substantial challenge at the intersection of natural langu...\n",
            "\n",
            "6. ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\n",
            "   Authors: Renat Aksitov, Sobhan Miryoosefi, Zong-xiao Li...\n",
            "   Year: 2023 | Citations: 71\n",
            "   PDF Available: ✅\n",
            "   Abstract: Answering complex natural language questions often necessitates multi-step reasoning and integrating...\n",
            "\n",
            "7. Exoskeletons need to react faster than physiological responses to improve standi...\n",
            "   Authors: Owen N. Beck, Max K. Shepherd, Rish Rastogi...\n",
            "   Year: 2023 | Citations: 66\n",
            "   PDF Available: ✅\n",
            "   Abstract: Maintaining balance throughout daily activities is challenging because of the unstable nature of the...\n",
            "\n",
            "8. REACT\n",
            "   Authors: Li Xiong, Cyrus Shahabi, Yanan Da...\n",
            "   Year: 2020 | Citations: 893\n",
            "   PDF Available: ✅\n",
            "   Abstract: Contact tracing is an essential public health tool for controlling epidemic disease outbreaks such a...\n",
            "\n",
            "9. ReAct: Temporal Action Detection with Relational Queries\n",
            "   Authors: Ding Shi, Yujie Zhong, Qiong Cao...\n",
            "   Year: 2022 | Citations: 84\n",
            "   PDF Available: ✅\n",
            "   Abstract: This work aims at advancing temporal action detection (TAD) using an encoder-decoder framework with ...\n",
            "\n",
            "10. How Regions React to Recessions: Resilience and the Role of Economic Structure\n",
            "   Authors: R. Martin, P. Sunley, B. Gardiner...\n",
            "   Year: 2016 | Citations: 652\n",
            "   PDF Available: ✅\n",
            "   Abstract: No abstract available...\n",
            "\n",
            " Module 1 complete! Results saved to: data/search_results/paper_search_results_React.json\n",
            "   Proceed to Module 2 for paper selection and PDF download.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "def main_search():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODULE 1: TOPIC INPUT & PAPER SEARCH\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    topic = input(\"\\nEnter research topic: \").strip()\n",
        "    if not topic:\n",
        "        topic = \"machine learning\"\n",
        "\n",
        "    results = search_papers(topic, limit=20)\n",
        "\n",
        "    if results:\n",
        "\n",
        "        save_path = save_search_results(results)\n",
        "\n",
        "        display_search_results(results)\n",
        "\n",
        "        print(f\"\\n Module 1 complete! Results saved to: {save_path}\")\n",
        "        print(\"   Proceed to Module 2 for paper selection and PDF download.\")\n",
        "\n",
        "        return results, save_path\n",
        "    else:\n",
        "        print(\" No results found. Please try a different topic.\")\n",
        "        return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_search()\n",
        "\n",
        "!pip install PyMuPDF requests -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "import fitz\n",
        "import hashlib\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fd173da",
      "metadata": {
        "id": "9fd173da"
      },
      "source": [
        "## Function: `load_search_results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2e0a660b",
      "metadata": {
        "id": "2e0a660b"
      },
      "outputs": [],
      "source": [
        "def load_search_results(filepath=None):\n",
        "\n",
        "    if not filepath:\n",
        "\n",
        "        results_dir = \"data/search_results\"\n",
        "        if os.path.exists(results_dir):\n",
        "            json_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]\n",
        "            if json_files:\n",
        "\n",
        "                json_files.sort(key=lambda x: os.path.getmtime(os.path.join(results_dir, x)), reverse=True)\n",
        "                filepath = os.path.join(results_dir, json_files[0])\n",
        "                print(f\" Loading most recent search results: {json_files[0]}\")\n",
        "            else:\n",
        "                print(\" No search results found. Run Module 1 first.\")\n",
        "                return None\n",
        "        else:\n",
        "            print(\" Search results directory not found. Run Module 1 first.\")\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        print(f\" Loaded {len(data['papers'])} papers on '{data['topic']}'\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\" Error loading file: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a069c4ec",
      "metadata": {
        "id": "a069c4ec"
      },
      "source": [
        "## Function: `filter_papers_with_pdfs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2521b73e",
      "metadata": {
        "id": "2521b73e"
      },
      "outputs": [],
      "source": [
        "def filter_papers_with_pdfs(papers):\n",
        "\n",
        "    papers_with_pdf = []\n",
        "    for paper in papers:\n",
        "        if paper.get(\"pdf_url\") and paper[\"pdf_url\"].strip():\n",
        "\n",
        "            url = paper[\"pdf_url\"].lower()\n",
        "            if url.endswith('.pdf') or '.pdf?' in url or 'pdf' in url:\n",
        "                papers_with_pdf.append(paper)\n",
        "\n",
        "    print(f\"\\n PDF Availability:\")\n",
        "    print(f\"  • Total papers: {len(papers)}\")\n",
        "    print(f\"  • Papers with PDF URLs: {len(papers_with_pdf)}\")\n",
        "\n",
        "    return papers_with_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70c733b9",
      "metadata": {
        "id": "70c733b9"
      },
      "source": [
        "## Function: `rank_papers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "890e4a92",
      "metadata": {
        "id": "890e4a92"
      },
      "outputs": [],
      "source": [
        "def rank_papers(papers):\n",
        "\n",
        "    valid_papers = []\n",
        "    for paper in papers:\n",
        "        if paper.get(\"year\") and paper.get(\"citationCount\") is not None:\n",
        "            valid_papers.append(paper)\n",
        "\n",
        "    ranked = sorted(valid_papers,\n",
        "                   key=lambda x: (x[\"citationCount\"], x[\"year\"]),\n",
        "                   reverse=True)\n",
        "\n",
        "    return ranked"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4caba19",
      "metadata": {
        "id": "e4caba19"
      },
      "source": [
        "## Function: `select_top_papers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c1a4adfb",
      "metadata": {
        "id": "c1a4adfb"
      },
      "outputs": [],
      "source": [
        "def select_top_papers(papers, count=3):\n",
        "\n",
        "    papers_with_pdf = filter_papers_with_pdfs(papers)\n",
        "\n",
        "    ranked_papers = rank_papers(papers_with_pdf)\n",
        "\n",
        "    selected = ranked_papers[:count]\n",
        "\n",
        "    print(f\"\\n Selected top {len(selected)} papers for download:\")\n",
        "    for i, paper in enumerate(selected):\n",
        "        print(f\"\\n{i+1}. {paper['title'][:70]}...\")\n",
        "        print(f\"   Citations: {paper['citationCount']}\")\n",
        "        print(f\"   Year: {paper['year']}\")\n",
        "        print(f\"   Authors: {', '.join(paper['authors'][:2])}\")\n",
        "\n",
        "    return selected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31e051eb",
      "metadata": {
        "id": "31e051eb"
      },
      "source": [
        "## Function: `download_pdf_with_verification`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "446290c5",
      "metadata": {
        "id": "446290c5"
      },
      "outputs": [],
      "source": [
        "def download_pdf_with_verification(url, filename, max_retries=2):\n",
        "\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                print(f\"  Attempt {attempt + 1}/{max_retries}...\")\n",
        "                response = requests.get(url, headers=headers, timeout=30)\n",
        "\n",
        "                if response.status_code != 200:\n",
        "                    print(f\"    HTTP Error: {response.status_code}\")\n",
        "                    continue\n",
        "\n",
        "                if not (response.content[:4] == b'%PDF' or\n",
        "                       'pdf' in response.headers.get('content-type', '').lower()):\n",
        "                    print(f\"    Not a PDF file\")\n",
        "                    continue\n",
        "\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "                if verify_pdf(filename):\n",
        "                    size = os.path.getsize(filename)\n",
        "                    print(f\"    Downloaded: {size:,} bytes\")\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"     Invalid PDF\")\n",
        "                    os.remove(filename)\n",
        "                    continue\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f\"    Timeout\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Error: {str(e)[:50]}\")\n",
        "\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Download failed: {str(e)[:50]}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22d1011e",
      "metadata": {
        "id": "22d1011e"
      },
      "source": [
        "## Function: `verify_pdf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1265cc6a",
      "metadata": {
        "id": "1265cc6a"
      },
      "outputs": [],
      "source": [
        "def verify_pdf(filepath):\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(filepath):\n",
        "            return False\n",
        "\n",
        "        if os.path.getsize(filepath) < 1024:\n",
        "            return False\n",
        "\n",
        "        with fitz.open(filepath) as doc:\n",
        "\n",
        "            if len(doc) > 0:\n",
        "                return True\n",
        "        return False\n",
        "    except:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5653a72e",
      "metadata": {
        "id": "5653a72e"
      },
      "source": [
        "## Function: `get_pdf_info`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "dcf5836e",
      "metadata": {
        "id": "dcf5836e"
      },
      "outputs": [],
      "source": [
        "def get_pdf_info(filepath):\n",
        "\n",
        "    try:\n",
        "        with fitz.open(filepath) as doc:\n",
        "            return {\n",
        "                'pages': len(doc),\n",
        "                'size_bytes': os.path.getsize(filepath),\n",
        "                'size_mb': round(os.path.getsize(filepath) / (1024 * 1024), 2),\n",
        "                'is_valid': True\n",
        "            }\n",
        "    except:\n",
        "        return {'is_valid': False}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f90409aa",
      "metadata": {
        "id": "f90409aa"
      },
      "source": [
        "## Function: `download_selected_papers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "db4f6d33",
      "metadata": {
        "id": "db4f6d33"
      },
      "outputs": [],
      "source": [
        "def download_selected_papers(selected_papers, output_dir=\"downloads\"):\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n Starting PDF downloads to: {output_dir}/\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    downloaded_papers = []\n",
        "\n",
        "    for i, paper in enumerate(selected_papers):\n",
        "        print(f\"\\n[{i+1}/{len(selected_papers)}] Downloading: {paper['title'][:60]}...\")\n",
        "\n",
        "        safe_title = \"\".join(c for c in paper['title'] if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
        "        if len(safe_title) > 50:\n",
        "            safe_title = safe_title[:50]\n",
        "\n",
        "        filename = f\"{output_dir}/paper_{i+1}_{hashlib.md5(safe_title.encode()).hexdigest()[:8]}.pdf\"\n",
        "\n",
        "        success = download_pdf_with_verification(paper['pdf_url'], filename)\n",
        "\n",
        "        if success:\n",
        "\n",
        "            pdf_info = get_pdf_info(filename)\n",
        "\n",
        "            paper['downloaded'] = True\n",
        "            paper['local_path'] = filename\n",
        "            paper['download_time'] = datetime.now().isoformat()\n",
        "            paper['pdf_info'] = pdf_info\n",
        "\n",
        "            downloaded_papers.append(paper)\n",
        "            print(f\"    Success! {pdf_info['pages']} pages, {pdf_info['size_mb']} MB\")\n",
        "        else:\n",
        "            paper['downloaded'] = False\n",
        "            print(f\"   Failed to download\")\n",
        "\n",
        "    return downloaded_papers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f20675",
      "metadata": {
        "id": "d3f20675"
      },
      "source": [
        "## Function: `save_download_report`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5fa37d45",
      "metadata": {
        "id": "5fa37d45"
      },
      "outputs": [],
      "source": [
        "def save_download_report(downloaded_papers, topic, output_dir=\"downloads\"):\n",
        "\n",
        "    report = {\n",
        "        'topic': topic,\n",
        "        'download_timestamp': datetime.now().isoformat(),\n",
        "        'total_selected': len(downloaded_papers),\n",
        "        'successful_downloads': sum(1 for p in downloaded_papers if p.get('downloaded', False)),\n",
        "        'failed_downloads': sum(1 for p in downloaded_papers if not p.get('downloaded', False)),\n",
        "        'papers': downloaded_papers\n",
        "    }\n",
        "\n",
        "    os.makedirs(\"data/reports\", exist_ok=True)\n",
        "    report_file = f\"data/reports/download_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(report, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n Download report saved to: {report_file}\")\n",
        "\n",
        "    download_list = []\n",
        "    for paper in downloaded_papers:\n",
        "        if paper.get('downloaded'):\n",
        "            download_list.append({\n",
        "                'title': paper['title'],\n",
        "                'local_file': paper['local_path'],\n",
        "                'size_mb': paper['pdf_info']['size_mb'],\n",
        "                'pages': paper['pdf_info']['pages']\n",
        "            })\n",
        "\n",
        "    list_file = f\"{output_dir}/downloaded_papers_list.json\"\n",
        "    with open(list_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(download_list, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    return report_file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c8d8abd",
      "metadata": {
        "id": "8c8d8abd"
      },
      "source": [
        "## Function: `verify_downloads`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1685ded7",
      "metadata": {
        "id": "1685ded7"
      },
      "outputs": [],
      "source": [
        "def verify_downloads(output_dir=\"downloads\"):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" VERIFICATION OF DOWNLOADS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        print(f\" Directory '{output_dir}' does not exist!\")\n",
        "        return 0\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(output_dir) if f.endswith('.pdf')]\n",
        "\n",
        "    print(f\"\\n Directory: {os.path.abspath(output_dir)}\")\n",
        "    print(f\" PDF files found: {len(pdf_files)}\")\n",
        "\n",
        "    if pdf_files:\n",
        "        print(\"\\nFile Details:\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        total_size = 0\n",
        "        valid_files = 0\n",
        "\n",
        "        for pdf in pdf_files:\n",
        "            filepath = os.path.join(output_dir, pdf)\n",
        "            size = os.path.getsize(filepath)\n",
        "            total_size += size\n",
        "\n",
        "            if verify_pdf(filepath):\n",
        "                valid_files += 1\n",
        "                with fitz.open(filepath) as doc:\n",
        "                    pages = len(doc)\n",
        "                print(f\" {pdf}\")\n",
        "                print(f\"   Size: {size:,} bytes ({size/1024/1024:.2f} MB)\")\n",
        "                print(f\"   Pages: {pages}\")\n",
        "            else:\n",
        "                print(f\" {pdf} - INVALID PDF\")\n",
        "                print(f\"   Size: {size:,} bytes\")\n",
        "\n",
        "    print(f\"\\n Summary:\")\n",
        "    print(f\"  • Total PDF files: {len(pdf_files)}\")\n",
        "    print(f\"  • Valid PDFs: {valid_files}\")\n",
        "    print(f\"  • Total size: {total_size/1024/1024:.2f} MB\")\n",
        "\n",
        "    return valid_files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae26635",
      "metadata": {
        "id": "eae26635"
      },
      "source": [
        "## Function: `main_download`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "68ac30ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ac30ac",
        "outputId": "efeb54eb-cb1b-4a8c-81c1-40d6e8bd1efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MODULE 2: PAPER SELECTION & PDF DOWNLOAD\n",
            "================================================================================\n",
            " Loading most recent search results: paper_search_results_React.json\n",
            " Loaded 1000 papers on 'React'\n",
            "\n",
            " PDF Availability:\n",
            "  • Total papers: 1000\n",
            "  • Papers with PDF URLs: 310\n",
            "\n",
            " Selected top 3 papers for download:\n",
            "\n",
            "1. THE PREPARATION OF I-131-LABELLED HUMAN GROWTH HORMONE OF HIGH SPECIFI...\n",
            "   Citations: 9556\n",
            "   Year: 1963\n",
            "   Authors: F. Greenwood, W. Hunter\n",
            "\n",
            "2. Apparent hydroxyl radical production by peroxynitrite: implications fo...\n",
            "   Citations: 7036\n",
            "   Year: 1990\n",
            "   Authors: J. Beckman, Tanya W. Beckman\n",
            "\n",
            "3. Human Computer Interaction...\n",
            "   Citations: 4505\n",
            "   Year: 2015\n",
            "   Authors: Hau-San Wong, H. Ip\n",
            "\n",
            " Starting PDF downloads to: downloads/\n",
            "------------------------------------------------------------\n",
            "\n",
            "[1/3] Downloading: THE PREPARATION OF I-131-LABELLED HUMAN GROWTH HORMONE OF HI...\n",
            "  Attempt 1/2...\n",
            "    Downloaded: 1,673,115 bytes\n",
            "    Success! 10 pages, 1.6 MB\n",
            "\n",
            "[2/3] Downloading: Apparent hydroxyl radical production by peroxynitrite: impli...\n",
            "  Attempt 1/2...\n",
            "    Downloaded: 1,077,446 bytes\n",
            "    Success! 5 pages, 1.03 MB\n",
            "\n",
            "[3/3] Downloading: Human Computer Interaction...\n",
            "  Attempt 1/2...\n",
            "    Downloaded: 152,844 bytes\n",
            "    Success! 13 pages, 0.15 MB\n",
            "\n",
            " Download report saved to: data/reports/download_report_20251212_032004.json\n",
            "\n",
            "============================================================\n",
            " VERIFICATION OF DOWNLOADS\n",
            "============================================================\n",
            "\n",
            " Directory: /content/downloads\n",
            " PDF files found: 3\n",
            "\n",
            "File Details:\n",
            "------------------------------------------------------------\n",
            " paper_3_20c4d88e.pdf\n",
            "   Size: 152,844 bytes (0.15 MB)\n",
            "   Pages: 13\n",
            " paper_2_ab041d35.pdf\n",
            "   Size: 1,077,446 bytes (1.03 MB)\n",
            "   Pages: 5\n",
            " paper_1_9485645b.pdf\n",
            "   Size: 1,673,115 bytes (1.60 MB)\n",
            "   Pages: 10\n",
            "\n",
            " Summary:\n",
            "  • Total PDF files: 3\n",
            "  • Valid PDFs: 3\n",
            "  • Total size: 2.77 MB\n",
            "\n",
            " Module 2 complete!\n",
            "   Downloaded papers are in: downloads/\n",
            "   Report saved to: data/reports/download_report_20251212_032004.json\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hConsider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
          ]
        }
      ],
      "source": [
        "def main_download(filepath=None, download_count=3):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODULE 2: PAPER SELECTION & PDF DOWNLOAD\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    data = load_search_results(filepath)\n",
        "    if not data:\n",
        "        return None\n",
        "\n",
        "    selected_papers = select_top_papers(data[\"papers\"], count=download_count)\n",
        "\n",
        "    if not selected_papers:\n",
        "        print(\" No papers with PDFs available for download.\")\n",
        "        return None\n",
        "\n",
        "    downloaded = download_selected_papers(selected_papers)\n",
        "\n",
        "    report_file = save_download_report(downloaded, data[\"topic\"])\n",
        "\n",
        "    verify_downloads()\n",
        "\n",
        "    print(f\"\\n Module 2 complete!\")\n",
        "    print(f\"   Downloaded papers are in: downloads/\")\n",
        "    print(f\"   Report saved to: {report_file}\")\n",
        "\n",
        "    return downloaded\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main_download(download_count=3)\n",
        "\n",
        "!pip install PyMuPDF4LLM tqdm -q\n",
        "\n",
        "!pip install pymupdf4llm pymupdf -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pymupdf4llm\n",
        "import pymupdf\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df898b25",
      "metadata": {
        "id": "df898b25"
      },
      "source": [
        "## Function: `extract_text_improved`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f7275054",
      "metadata": {
        "id": "f7275054"
      },
      "outputs": [],
      "source": [
        "def extract_text_improved(pdf_path):\n",
        "\n",
        "    try:\n",
        "\n",
        "        doc = pymupdf.open(pdf_path)\n",
        "\n",
        "        if doc.is_encrypted:\n",
        "            print(f\" PDF is encrypted, trying to extract anyway...\")\n",
        "\n",
        "        first_page_text = doc[0].get_text().strip() if len(doc) > 0 else \"\"\n",
        "\n",
        "        copyright_keywords = [\"copyright\", \"removed\", \"deleted\", \"takedown\", \"not available\"]\n",
        "        if any(keyword in first_page_text.lower() for keyword in copyright_keywords):\n",
        "            print(f\"  PDF appears to have copyright restrictions\")\n",
        "            doc.close()\n",
        "            return None\n",
        "\n",
        "        texts = []\n",
        "\n",
        "        try:\n",
        "            markdown_text = pymupdf4llm.to_markdown(str(pdf_path))\n",
        "            if markdown_text and len(markdown_text) > 500:\n",
        "                texts.append((\"markdown\", markdown_text))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        full_text = \"\"\n",
        "        for page_num in range(min(50, len(doc))):\n",
        "            page = doc[page_num]\n",
        "            page_text = page.get_text()\n",
        "            if page_text:\n",
        "                full_text += page_text + \"\\n\"\n",
        "\n",
        "        if full_text and len(full_text) > 500:\n",
        "            texts.append((\"regular\", full_text))\n",
        "\n",
        "        doc.close()\n",
        "\n",
        "        if not texts:\n",
        "            return None\n",
        "\n",
        "        for method, text in texts:\n",
        "            if method == \"markdown\" and len(text) > 1000:\n",
        "                return text\n",
        "\n",
        "        best_text = max(texts, key=lambda x: len(x[1]))[1]\n",
        "        return best_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Extraction error: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d63b6183",
      "metadata": {
        "id": "d63b6183"
      },
      "source": [
        "## Function: `extract_sections_improved`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "231ea009",
      "metadata": {
        "id": "231ea009"
      },
      "outputs": [],
      "source": [
        "def extract_sections_improved(text):\n",
        "\n",
        "    sections = {\n",
        "        \"title\": \"\",\n",
        "        \"abstract\": \"\",\n",
        "        \"introduction\": \"\",\n",
        "        \"methods\": \"\",\n",
        "        \"results\": \"\",\n",
        "        \"conclusion\": \"\",\n",
        "        \"references\": \"\",\n",
        "        \"extracted_text\": text[:20000]\n",
        "    }\n",
        "\n",
        "    if not text or len(text) < 500:\n",
        "        return sections\n",
        "\n",
        "    text = clean_text_basic(text)\n",
        "\n",
        "    section_headers = {\n",
        "        \"abstract\": [r'abstract', r'summary'],\n",
        "        \"introduction\": [r'1\\.\\s*introduction', r'introduction', r'background'],\n",
        "        \"methods\": [r'2\\.\\s*methods?', r'methods?', r'methodology', r'experiment'],\n",
        "        \"results\": [r'3\\.\\s*results?', r'results?', r'findings?'],\n",
        "        \"conclusion\": [r'4\\.\\s*conclusions?', r'conclusions?', r'discussion'],\n",
        "        \"references\": [r'references?', r'bibliography']\n",
        "    }\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    section_boundaries = {}\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        line_clean = line.strip().lower()\n",
        "        for section_name, patterns in section_headers.items():\n",
        "            for pattern in patterns:\n",
        "                if re.match(rf'^{pattern}[.:]?\\s*$', line_clean) or \\\n",
        "                   re.search(rf'\\b{pattern}\\b', line_clean) and len(line_clean) < 100:\n",
        "                    section_boundaries[section_name] = i\n",
        "                    break\n",
        "\n",
        "    if section_boundaries:\n",
        "        sorted_sections = sorted(section_boundaries.items(), key=lambda x: x[1])\n",
        "\n",
        "        for idx, (section_name, line_idx) in enumerate(sorted_sections):\n",
        "\n",
        "            start_idx = line_idx + 1\n",
        "            if idx + 1 < len(sorted_sections):\n",
        "                end_idx = sorted_sections[idx + 1][1]\n",
        "            else:\n",
        "                end_idx = len(lines)\n",
        "\n",
        "            section_text = '\\n'.join(lines[start_idx:end_idx])\n",
        "            if len(section_text.strip()) > 100:\n",
        "\n",
        "                sections[section_name] = section_text.strip()[:5000]\n",
        "\n",
        "    for line in lines[:10]:\n",
        "        line = line.strip()\n",
        "        if 20 < len(line) < 200 and not line.startswith('http'):\n",
        "            sections[\"title\"] = line\n",
        "            break\n",
        "\n",
        "    if not any(len(sections[sec]) > 200 for sec in [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]):\n",
        "        sections = extract_by_keywords_fallback(text, sections)\n",
        "\n",
        "    return sections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6de10d",
      "metadata": {
        "id": "1d6de10d"
      },
      "source": [
        "## Function: `extract_by_keywords_fallback`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "355670c6",
      "metadata": {
        "id": "355670c6"
      },
      "outputs": [],
      "source": [
        "def extract_by_keywords_fallback(text, existing_sections):\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    section_keywords = {\n",
        "        \"abstract\": [\"abstract\", \"summary\", \"we present\", \"this paper\"],\n",
        "        \"introduction\": [\"introduction\", \"background\", \"motivation\", \"related work\"],\n",
        "        \"methods\": [\"method\", \"experiment\", \"procedure\", \"dataset\", \"implementation\"],\n",
        "        \"results\": [\"result\", \"finding\", \"table\", \"figure\", \"experiment shows\"],\n",
        "        \"conclusion\": [\"conclusion\", \"discussion\", \"future work\", \"limitations\", \"summary\"]\n",
        "    }\n",
        "\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "\n",
        "    for section, keywords in section_keywords.items():\n",
        "        if existing_sections[section]:\n",
        "            continue\n",
        "\n",
        "        section_sentences = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sentence_lower = sentence.lower()\n",
        "            if any(keyword in sentence_lower for keyword in keywords):\n",
        "\n",
        "                start = max(0, i - 2)\n",
        "                end = min(len(sentences), i + 6)\n",
        "                context = ' '.join(sentences[start:end])\n",
        "                section_sentences.append(context)\n",
        "\n",
        "        if section_sentences:\n",
        "            existing_sections[section] = ' '.join(section_sentences)[:5000]\n",
        "\n",
        "    return existing_sections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00fb5898",
      "metadata": {
        "id": "00fb5898"
      },
      "source": [
        "## Function: `clean_text_basic`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "42b9302c",
      "metadata": {
        "id": "42b9302c"
      },
      "outputs": [],
      "source": [
        "def clean_text_basic(text):\n",
        "\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    text = re.sub(r'-\\s+', '', text)\n",
        "    text = re.sub(r'\\s*-\\s*', '-', text)\n",
        "\n",
        "    text = ''.join(char for char in text if ord(char) >= 32 or char == '\\n')\n",
        "\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cbfe931",
      "metadata": {
        "id": "2cbfe931"
      },
      "source": [
        "## Function: `process_paper_smart`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "05cca63f",
      "metadata": {
        "id": "05cca63f"
      },
      "outputs": [],
      "source": [
        "def process_paper_smart(pdf_path):\n",
        "\n",
        "    print(f\"\\nProcessing: {pdf_path.name}\")\n",
        "\n",
        "    file_size = pdf_path.stat().st_size\n",
        "    if file_size < 10240:\n",
        "        print(f\" File too small ({file_size:,} bytes), may be empty\")\n",
        "        return None\n",
        "\n",
        "    raw_text = extract_text_improved(pdf_path)\n",
        "\n",
        "    if raw_text is None:\n",
        "        print(f\"  Skipping - copyright restrictions or empty\")\n",
        "        return None\n",
        "\n",
        "    if len(raw_text) < 1000:\n",
        "        print(f\"  Text very short ({len(raw_text):,} chars), may be incomplete\")\n",
        "\n",
        "    print(f\"  Extracted {len(raw_text):,} characters\")\n",
        "\n",
        "    sections = extract_sections_improved(raw_text)\n",
        "\n",
        "    meaningful_sections = []\n",
        "    for section_name, content in sections.items():\n",
        "        if content and section_name != \"extracted_text\" and len(content) > 200:\n",
        "            meaningful_sections.append(section_name)\n",
        "\n",
        "    print(f\"   Found {len(meaningful_sections)} meaningful sections\")\n",
        "    for section in meaningful_sections[:3]:\n",
        "        content = sections[section]\n",
        "        print(f\"    • {section}: {len(content):,} chars\")\n",
        "\n",
        "    result = {\n",
        "        \"paper_id\": pdf_path.stem,\n",
        "        \"filename\": pdf_path.name,\n",
        "        \"file_size_bytes\": file_size,\n",
        "        \"total_characters\": len(raw_text),\n",
        "        \"meaningful_sections\": meaningful_sections,\n",
        "        \"sections\": sections,\n",
        "        \"status\": \"success\"\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20389ab6",
      "metadata": {
        "id": "20389ab6"
      },
      "source": [
        "## Function: `extract_all_papers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c6175dc5",
      "metadata": {
        "id": "c6175dc5"
      },
      "outputs": [],
      "source": [
        "def extract_all_papers(download_dir=\"downloads\", max_papers=None):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODULE 3: PDF TEXT EXTRACTION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    pdf_files = get_downloaded_papers(download_dir)\n",
        "    if not pdf_files:\n",
        "        print(\" No PDFs found. Run Module 2 first.\")\n",
        "        return []\n",
        "\n",
        "    if max_papers:\n",
        "        pdf_files = pdf_files[:max_papers]\n",
        "\n",
        "    print(f\"\\nProcessing {len(pdf_files)} PDF files...\")\n",
        "\n",
        "    results = []\n",
        "    skipped = 0\n",
        "\n",
        "    for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
        "        result = process_paper_smart(pdf_file)\n",
        "        if result:\n",
        "            results.append(result)\n",
        "        else:\n",
        "            skipped += 1\n",
        "\n",
        "    if results:\n",
        "        save_results_final(results)\n",
        "\n",
        "    print(f\"\\n Extraction complete!\")\n",
        "    print(f\"   Successfully processed: {len(results)} papers\")\n",
        "    print(f\"   Skipped: {skipped} papers\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e7f60f",
      "metadata": {
        "id": "65e7f60f"
      },
      "source": [
        "## Function: `get_downloaded_papers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6172a9c3",
      "metadata": {
        "id": "6172a9c3"
      },
      "outputs": [],
      "source": [
        "def get_downloaded_papers(download_dir=\"downloads\"):\n",
        "\n",
        "    download_path = Path(download_dir)\n",
        "    if not download_path.exists():\n",
        "        return []\n",
        "\n",
        "    pdf_files = list(download_path.glob(\"*.pdf\"))\n",
        "    return pdf_files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5024c79",
      "metadata": {
        "id": "a5024c79"
      },
      "source": [
        "## Function: `save_results_final`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b07a3b2c",
      "metadata": {
        "id": "b07a3b2c"
      },
      "outputs": [],
      "source": [
        "def save_results_final(results, output_dir=\"data/extracted\"):\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for result in results:\n",
        "        paper_id = result[\"paper_id\"]\n",
        "        output_file = output_path / f\"{paper_id}_extracted.json\"\n",
        "\n",
        "        if \"extracted_text\" in result[\"sections\"] and len(result[\"sections\"][\"extracted_text\"]) > 10000:\n",
        "            result[\"sections\"][\"extracted_text\"] = result[\"sections\"][\"extracted_text\"][:10000] + \"...[truncated]\"\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"   Saved: {output_file.name}\")\n",
        "\n",
        "    summary = {\n",
        "        \"extraction_date\": datetime.now().isoformat(),\n",
        "        \"total_papers\": len(results),\n",
        "        \"papers\": [\n",
        "            {\n",
        "                \"paper_id\": r[\"paper_id\"],\n",
        "                \"filename\": r[\"filename\"],\n",
        "                \"file_size\": r[\"file_size_bytes\"],\n",
        "                \"total_chars\": r[\"total_characters\"],\n",
        "                \"sections_found\": r[\"meaningful_sections\"]\n",
        "            }\n",
        "            for r in results\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    summary_file = output_path / \"extraction_summary.json\"\n",
        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n Summary saved to: {summary_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24eae14d",
      "metadata": {
        "id": "24eae14d"
      },
      "source": [
        "## Function: `analyze_extraction_results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "29fba463",
      "metadata": {
        "id": "29fba463"
      },
      "outputs": [],
      "source": [
        "def analyze_extraction_results():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXTRACTION ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    data_path = Path(\"data/extracted\")\n",
        "    if not data_path.exists():\n",
        "        print(\" No extraction directory found\")\n",
        "        return\n",
        "\n",
        "    json_files = list(data_path.glob(\"*_extracted.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(\" No extracted paper files found\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFound {len(json_files)} extracted papers:\\n\")\n",
        "\n",
        "    total_chars = 0\n",
        "    papers_with_abstract = 0\n",
        "    papers_with_multiple_sections = 0\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            paper_id = data.get(\"paper_id\", \"Unknown\")\n",
        "            total_chars += data.get(\"total_characters\", 0)\n",
        "\n",
        "            sections = data.get(\"sections\", {})\n",
        "            meaningful_sections = data.get(\"meaningful_sections\", [])\n",
        "\n",
        "            if sections.get(\"abstract\") and len(sections[\"abstract\"]) > 200:\n",
        "                papers_with_abstract += 1\n",
        "\n",
        "            if len(meaningful_sections) >= 2:\n",
        "                papers_with_multiple_sections += 1\n",
        "\n",
        "            print(f\" {paper_id}\")\n",
        "            print(f\"   Size: {data.get('file_size_bytes', 0):,} bytes\")\n",
        "            print(f\"   Text: {data.get('total_characters', 0):,} chars\")\n",
        "            print(f\"   Sections found: {len(meaningful_sections)}\")\n",
        "\n",
        "            if sections.get(\"title\"):\n",
        "                title = sections[\"title\"][:80]\n",
        "                print(f\"   Title: {title}\")\n",
        "\n",
        "            if sections.get(\"abstract\"):\n",
        "                abstract_preview = sections[\"abstract\"][:150]\n",
        "                print(f\"   Abstract: {abstract_preview}...\")\n",
        "\n",
        "            print()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Error reading {json_file.name}: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXTRACTION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total papers processed: {len(json_files)}\")\n",
        "    print(f\"Total characters extracted: {total_chars:,}\")\n",
        "    print(f\"Papers with abstract: {papers_with_abstract}/{len(json_files)}\")\n",
        "    print(f\"Papers with multiple sections: {papers_with_multiple_sections}/{len(json_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c4f0b3e",
      "metadata": {
        "id": "7c4f0b3e"
      },
      "source": [
        "## Function: `generate_report`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3b253338",
      "metadata": {
        "id": "3b253338"
      },
      "outputs": [],
      "source": [
        "def generate_report():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"  REVIEW REPORT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    data_path = Path(\"data/extracted\")\n",
        "    if not data_path.exists():\n",
        "        print(\" No extraction directory found\")\n",
        "        return\n",
        "\n",
        "    json_files = list(data_path.glob(\"*_extracted.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(\" No extracted papers found\")\n",
        "        return\n",
        "\n",
        "    report = {\n",
        "        \"generated_date\": datetime.now().isoformat(),\n",
        "        \"total_papers\": len(json_files),\n",
        "        \"quality_checks\": [],\n",
        "        \"papers\": []\n",
        "    }\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            paper_report = {\n",
        "                \"paper_id\": data[\"paper_id\"],\n",
        "                \"filename\": data[\"filename\"],\n",
        "                \"checks\": {\n",
        "                    \"text_clean\": False,\n",
        "                    \"sections_correct\": False,\n",
        "                    \"no_hallucinations\": False,\n",
        "                    \"no_missing_chunks\": False\n",
        "                },\n",
        "                \"section_lengths\": {},\n",
        "                \"issues\": []\n",
        "            }\n",
        "\n",
        "            sections = data.get(\"sections\", {})\n",
        "\n",
        "            sample_text = sections.get(\"abstract\", sections.get(\"extracted_text\", \"\"))\n",
        "            artifacts = ['�', '\\x00', '[?]', '[ ]']\n",
        "            has_artifacts = any(art in sample_text for art in artifacts)\n",
        "            paper_report[\"checks\"][\"text_clean\"] = not has_artifacts\n",
        "\n",
        "            if has_artifacts:\n",
        "                paper_report[\"issues\"].append(\"Text contains extraction artifacts\")\n",
        "\n",
        "            major_sections = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]\n",
        "            found_sections = [s for s in major_sections if sections.get(s) and len(sections[s]) > 200]\n",
        "            paper_report[\"checks\"][\"sections_correct\"] = len(found_sections) >= 2\n",
        "\n",
        "            if len(found_sections) < 2:\n",
        "                paper_report[\"issues\"].append(f\"Only found {len(found_sections)} major sections\")\n",
        "\n",
        "            total_chars = data.get(\"total_characters\", 0)\n",
        "            paper_report[\"checks\"][\"no_hallucinations\"] = 1000 <= total_chars <= 500000\n",
        "\n",
        "            if total_chars < 1000:\n",
        "                paper_report[\"issues\"].append(f\"Text too short: {total_chars} chars\")\n",
        "            elif total_chars > 500000:\n",
        "                paper_report[\"issues\"].append(f\"Text suspiciously long: {total_chars} chars\")\n",
        "\n",
        "            section_lengths = sum(len(str(content)) for content in sections.values() if content)\n",
        "            coverage = section_lengths / total_chars if total_chars > 0 else 0\n",
        "            paper_report[\"checks\"][\"no_missing_chunks\"] = coverage >= 0.3\n",
        "\n",
        "            if coverage < 0.3:\n",
        "                paper_report[\"issues\"].append(f\"Low coverage: {coverage:.1%}\")\n",
        "\n",
        "            for section, content in sections.items():\n",
        "                if content and len(str(content)) > 50:\n",
        "                    paper_report[\"section_lengths\"][section] = len(str(content))\n",
        "\n",
        "            report[\"papers\"].append(paper_report)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {json_file}: {e}\")\n",
        "\n",
        "    total_checks = 0\n",
        "    passed_checks = 0\n",
        "\n",
        "    for paper in report[\"papers\"]:\n",
        "        for check_name, passed in paper[\"checks\"].items():\n",
        "            total_checks += 1\n",
        "            if passed:\n",
        "                passed_checks += 1\n",
        "\n",
        "    report[\"overall_score\"] = f\"{passed_checks}/{total_checks}\" if total_checks > 0 else \"N/A\"\n",
        "    report[\"success_rate\"] = passed_checks / total_checks if total_checks > 0 else 0\n",
        "\n",
        "    report_file = data_path / \"_review_report.json\"\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\n report generated!\")\n",
        "    print(f\"   Overall score: {report['overall_score']}\")\n",
        "    print(f\"   Success rate: {report['success_rate']:.1%}\")\n",
        "    print(f\"   Report saved to: {report_file}\")\n",
        "\n",
        "    print(\"\\n QUALITY CHECK SUMMARY:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    check_names = [\"text_clean\", \"sections_correct\", \"no_hallucinations\", \"no_missing_chunks\"]\n",
        "    for check_name in check_names:\n",
        "        passed = sum(1 for paper in report[\"papers\"] if paper[\"checks\"].get(check_name, False))\n",
        "        total = len(report[\"papers\"])\n",
        "        percentage = (passed / total * 100) if total > 0 else 0\n",
        "        status = \"✅\" if percentage >= 70 else \"⚠️ \" if percentage >= 50 else \"❌\"\n",
        "        print(f\"{status} {check_name}: {passed}/{total} ({percentage:.0f}%)\")\n",
        "\n",
        "    return report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be57481",
      "metadata": {
        "id": "7be57481"
      },
      "source": [
        "## Function: `run_complete_extraction`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b4c19972",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4c19972",
        "outputId": "0f2c6a1c-27d8-4465-bc83-ae00829b7dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PDF TEXT EXTRACTION MODULE\n",
            "================================================================================\n",
            "\n",
            "STEP 1: Extracting text from PDFs...\n",
            "\n",
            "================================================================================\n",
            "MODULE 3: PDF TEXT EXTRACTION\n",
            "================================================================================\n",
            "\n",
            "Processing 3 PDF files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing PDFs:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: paper_3_20c4d88e.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing PDFs:  33%|███▎      | 1/3 [00:01<00:03,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Extracted 21,209 characters\n",
            "   Found 3 meaningful sections\n",
            "    • methods: 310 chars\n",
            "    • results: 3,998 chars\n",
            "    • conclusion: 1,882 chars\n",
            "\n",
            "Processing: paper_2_ab041d35.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing PDFs:  67%|██████▋   | 2/3 [00:10<00:05,  5.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Extracted 32,375 characters\n",
            "   Found 4 meaningful sections\n",
            "    • abstract: 784 chars\n",
            "    • methods: 4,246 chars\n",
            "    • results: 5,000 chars\n",
            "\n",
            "Processing: paper_1_9485645b.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PDFs: 100%|██████████| 3/3 [00:28<00:00,  9.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Extracted 47,295 characters\n",
            "   Found 5 meaningful sections\n",
            "    • abstract: 1,378 chars\n",
            "    • introduction: 528 chars\n",
            "    • methods: 5,000 chars\n",
            "   Saved: paper_3_20c4d88e_extracted.json\n",
            "   Saved: paper_2_ab041d35_extracted.json\n",
            "   Saved: paper_1_9485645b_extracted.json\n",
            "\n",
            " Summary saved to: data/extracted/extraction_summary.json\n",
            "\n",
            " Extraction complete!\n",
            "   Successfully processed: 3 papers\n",
            "   Skipped: 0 papers\n",
            "\n",
            " STEP 2: Analyzing extraction quality...\n",
            "\n",
            "================================================================================\n",
            "EXTRACTION ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Found 3 extracted papers:\n",
            "\n",
            " paper_1_9485645b\n",
            "   Size: 1,673,115 bytes\n",
            "   Text: 47,295 chars\n",
            "   Sections found: 5\n",
            "   Abstract:  Effect of [iodination][ on] growth [hormone] Modification of the immunological properties of growth hormone by labelling with \"8\"I could be caused by...\n",
            "\n",
            " paper_3_20c4d88e\n",
            "   Size: 152,844 bytes\n",
            "   Text: 21,209 chars\n",
            "   Sections found: 3\n",
            "\n",
            " paper_2_ab041d35\n",
            "   Size: 1,077,446 bytes\n",
            "   Text: 32,375 chars\n",
            "   Sections found: 4\n",
            "   Abstract:  BECKMAN*, JUN CHEN*, PATRICIA A  MARSHALL*, AND BRUCE A  FREEMAN*t ``` ``` Departments of *Anesthesiology and *Biochemistry, University of Alabama at...\n",
            "\n",
            "\n",
            "============================================================\n",
            "EXTRACTION SUMMARY\n",
            "============================================================\n",
            "Total papers processed: 3\n",
            "Total characters extracted: 100,879\n",
            "Papers with abstract: 2/3\n",
            "Papers with multiple sections: 3/3\n",
            "\n",
            " STEP 3: Generating eview report...\n",
            "\n",
            "================================================================================\n",
            "  REVIEW REPORT\n",
            "================================================================================\n",
            "\n",
            " report generated!\n",
            "   Overall score: 12/12\n",
            "   Success rate: 100.0%\n",
            "   Report saved to: data/extracted/_review_report.json\n",
            "\n",
            " QUALITY CHECK SUMMARY:\n",
            "----------------------------------------\n",
            "✅ text_clean: 3/3 (100%)\n",
            "✅ sections_correct: 3/3 (100%)\n",
            "✅ no_hallucinations: 3/3 (100%)\n",
            "✅ no_missing_chunks: 3/3 (100%)\n",
            "\n",
            "================================================================================\n",
            " COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "What has been accomplished:\n",
            "\n",
            "================================================================================\n",
            "EXAMPLE OF EXTRACTED CONTENT\n",
            "================================================================================\n",
            "\n",
            "Paper: paper_3_20c4d88e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def run_complete_extraction():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PDF TEXT EXTRACTION MODULE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nSTEP 1: Extracting text from PDFs...\")\n",
        "    results = extract_all_papers(max_papers=5)\n",
        "\n",
        "    if not results:\n",
        "        print(\" No papers extracted successfully\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n STEP 2: Analyzing extraction quality...\")\n",
        "    analyze_extraction_results()\n",
        "\n",
        "    print(\"\\n STEP 3: Generating eview report...\")\n",
        "    report = generate_report()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nWhat has been accomplished:\")\n",
        "\n",
        "    return results, report\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, report = run_complete_extraction()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXAMPLE OF EXTRACTED CONTENT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        first_paper = results[0]\n",
        "        sections = first_paper[\"sections\"]\n",
        "\n",
        "        print(f\"\\nPaper: {first_paper['paper_id']}\")\n",
        "\n",
        "        for section_name in [\"title\", \"abstract\", \"introduction\"]:\n",
        "            if sections.get(section_name) and len(sections[section_name]) > 50:\n",
        "                content = sections[section_name]\n",
        "                print(f\"\\n{section_name.upper()}:\")\n",
        "                print(\"-\" * 40)\n",
        "\n",
        "                preview = content[:500] + \"...\" if len(content) > 500 else content\n",
        "                print(preview)\n",
        "                print(f\"[Total length: {len(content):,} characters]\")\n",
        "\n",
        "!pip install scikit-learn numpy -q\n",
        "\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "794d89c2",
      "metadata": {
        "id": "794d89c2"
      },
      "source": [
        "## Function: `load_extracted_papers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "00341caf",
      "metadata": {
        "id": "00341caf"
      },
      "outputs": [],
      "source": [
        "def load_extracted_papers(data_dir=\"data/extracted\"):\n",
        "\n",
        "    data_path = Path(data_dir)\n",
        "    papers = []\n",
        "\n",
        "    json_files = list(data_path.glob(\"*_extracted.json\"))\n",
        "\n",
        "    if not json_files:\n",
        "        print(\"No extracted papers found. Run Module 3 first.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Loading {len(json_files)} extracted papers...\")\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                papers.append(data)\n",
        "                print(f\"  ✓ {data['paper_id']}: {data['total_characters']:,} chars\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {json_file}: {e}\")\n",
        "\n",
        "    return papers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae156714",
      "metadata": {
        "id": "ae156714"
      },
      "source": [
        "## Function: `analyze_single_paper`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "aaa62053",
      "metadata": {
        "id": "aaa62053"
      },
      "outputs": [],
      "source": [
        "def analyze_single_paper(paper):\n",
        "\n",
        "    print(\"\\n Performing deep analysis of single paper...\")\n",
        "\n",
        "    info = extract_key_information(paper)\n",
        "\n",
        "    analysis = {\n",
        "        \"paper_id\": info[\"paper_id\"],\n",
        "        \"title\": info[\"title\"],\n",
        "        \"year\": info[\"year\"],\n",
        "        \"methods_used\": info[\"methods\"],\n",
        "        \"datasets_mentioned\": info[\"datasets\"],\n",
        "        \"key_findings\": info[\"key_findings\"],\n",
        "        \"limitations\": info[\"limitations\"],\n",
        "        \"contributions\": info[\"contributions\"],\n",
        "        \"metrics_reported\": info[\"metrics\"],\n",
        "        \"paper_structure\": analyze_paper_structure(paper),\n",
        "        \"research_quality_indicators\": assess_research_quality(info),\n",
        "        \"recommendations_for_future_research\": generate_recommendations(info)\n",
        "    }\n",
        "\n",
        "    return analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b512b81e",
      "metadata": {
        "id": "b512b81e"
      },
      "source": [
        "## Function: `analyze_paper_structure`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "bf63a224",
      "metadata": {
        "id": "bf63a224"
      },
      "outputs": [],
      "source": [
        "def analyze_paper_structure(paper):\n",
        "\n",
        "    sections = paper[\"sections\"]\n",
        "    structure = {\n",
        "        \"sections_present\": [],\n",
        "        \"sections_missing\": [],\n",
        "        \"section_lengths\": {}\n",
        "    }\n",
        "\n",
        "    expected_sections = [\"title\", \"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]\n",
        "\n",
        "    for section in expected_sections:\n",
        "        content = sections.get(section, \"\")\n",
        "        if content and len(content) > 50:\n",
        "            structure[\"sections_present\"].append(section)\n",
        "            structure[\"section_lengths\"][section] = len(content)\n",
        "        else:\n",
        "            structure[\"sections_missing\"].append(section)\n",
        "\n",
        "    return structure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37283735",
      "metadata": {
        "id": "37283735"
      },
      "source": [
        "## Function: `assess_research_quality`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f3095a1d",
      "metadata": {
        "id": "f3095a1d"
      },
      "outputs": [],
      "source": [
        "def assess_research_quality(info):\n",
        "\n",
        "    quality_indicators = {\n",
        "        \"has_methods\": len(info[\"methods\"]) > 0,\n",
        "        \"has_datasets\": len(info[\"datasets\"]) > 0,\n",
        "        \"has_findings\": len(info[\"key_findings\"]) > 0,\n",
        "        \"has_limitations\": len(info[\"limitations\"]) > 0,\n",
        "        \"has_metrics\": len(info[\"metrics\"]) > 0,\n",
        "        \"method_diversity\": len(info[\"methods\"]),\n",
        "        \"finding_clarity\": len(info[\"key_findings\"])\n",
        "    }\n",
        "\n",
        "    score = 0\n",
        "    max_score = 7\n",
        "\n",
        "    if quality_indicators[\"has_methods\"]: score += 1\n",
        "    if quality_indicators[\"has_datasets\"]: score += 1\n",
        "    if quality_indicators[\"has_findings\"]: score += 1\n",
        "    if quality_indicators[\"has_limitations\"]: score += 1\n",
        "    if quality_indicators[\"has_metrics\"]: score += 1\n",
        "    if quality_indicators[\"method_diversity\"] >= 2: score += 1\n",
        "    if quality_indicators[\"finding_clarity\"] >= 2: score += 1\n",
        "\n",
        "    quality_indicators[\"overall_score\"] = f\"{score}/{max_score}\"\n",
        "    quality_indicators[\"percentage\"] = (score / max_score) * 100\n",
        "\n",
        "    return quality_indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c693c49a",
      "metadata": {
        "id": "c693c49a"
      },
      "source": [
        "## Function: `generate_recommendations`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "357b2eac",
      "metadata": {
        "id": "357b2eac"
      },
      "outputs": [],
      "source": [
        "def generate_recommendations(info):\n",
        "\n",
        "    recommendations = []\n",
        "\n",
        "    methods = info.get(\"methods\", [])\n",
        "    if methods:\n",
        "        recommendations.append(f\"Consider comparing with other papers using: {methods[0]}\")\n",
        "\n",
        "    limitations = info.get(\"limitations\", [])\n",
        "    if limitations:\n",
        "        recommendations.append(f\"Address limitations mentioned: {limitations[0][:100]}...\")\n",
        "\n",
        "    datasets = info.get(\"datasets\", [])\n",
        "    if datasets:\n",
        "        recommendations.append(f\"Explore other datasets in addition to those mentioned\")\n",
        "\n",
        "    recommendations.append(\"Compare with recent papers in the same field\")\n",
        "    recommendations.append(\"Explore alternative methodologies mentioned in related work\")\n",
        "\n",
        "    return recommendations[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ab8e79",
      "metadata": {
        "id": "32ab8e79"
      },
      "source": [
        "## Function: `extract_key_information`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "50b27c2e",
      "metadata": {
        "id": "50b27c2e"
      },
      "outputs": [],
      "source": [
        "def extract_key_information(paper):\n",
        "\n",
        "    info = {\n",
        "        \"paper_id\": paper[\"paper_id\"],\n",
        "        \"title\": paper[\"sections\"].get(\"title\", \"Unknown\"),\n",
        "        \"year\": extract_year(paper),\n",
        "        \"methods\": extract_methods(paper),\n",
        "        \"datasets\": extract_datasets(paper),\n",
        "        \"key_findings\": extract_key_findings(paper),\n",
        "        \"limitations\": extract_limitations(paper),\n",
        "        \"contributions\": extract_contributions(paper),\n",
        "        \"metrics\": extract_metrics(paper)\n",
        "    }\n",
        "\n",
        "    return info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda7d921",
      "metadata": {
        "id": "cda7d921"
      },
      "source": [
        "## Function: `extract_year`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "17ee3bd5",
      "metadata": {
        "id": "17ee3bd5"
      },
      "outputs": [],
      "source": [
        "def extract_year(paper):\n",
        "\n",
        "    title = paper[\"sections\"].get(\"title\", \"\")\n",
        "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', title)\n",
        "    if year_match:\n",
        "        return year_match.group()\n",
        "\n",
        "    text = paper[\"sections\"].get(\"extracted_text\", \"\")\n",
        "    year_match = re.search(r'\\b(19|20)\\d{2}\\b', text[:5000])\n",
        "    if year_match:\n",
        "        return year_match.group()\n",
        "\n",
        "    return \"Unknown\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f42ab249",
      "metadata": {
        "id": "f42ab249"
      },
      "source": [
        "## Function: `extract_methods`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "fe84d65a",
      "metadata": {
        "id": "fe84d65a"
      },
      "outputs": [],
      "source": [
        "def extract_methods(paper):\n",
        "\n",
        "    methods_text = paper[\"sections\"].get(\"methods\", \"\")\n",
        "    if not methods_text:\n",
        "        methods_text = paper[\"sections\"].get(\"extracted_text\", \"\")[:5000]\n",
        "\n",
        "    method_keywords = [\n",
        "        \"deep learning\", \"machine learning\", \"neural network\", \"transformer\",\n",
        "        \"cnn\", \"rnn\", \"lstm\", \"bert\", \"gpt\", \"reinforcement learning\",\n",
        "        \"statistical\", \"regression\", \"classification\", \"clustering\",\n",
        "        \"svm\", \"random forest\", \"xgboost\", \"bayesian\", \"monte carlo\",\n",
        "        \"simulation\", \"experiment\", \"analysis\", \"framework\", \"model\",\n",
        "        \"algorithm\", \"approach\", \"technique\", \"methodology\"\n",
        "    ]\n",
        "\n",
        "    found_methods = []\n",
        "    sentences = re.split(r'[.!?]+', methods_text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for keyword in method_keywords:\n",
        "            if keyword in sentence and len(sentence) > 20:\n",
        "                clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "                if clean_sentence not in found_methods:\n",
        "                    found_methods.append(clean_sentence[:200])\n",
        "                    break\n",
        "\n",
        "    if not found_methods:\n",
        "        results_text = paper[\"sections\"].get(\"results\", \"\")[:1000]\n",
        "        conclusion_text = paper[\"sections\"].get(\"conclusion\", \"\")[:1000]\n",
        "        combined = results_text + \" \" + conclusion_text\n",
        "\n",
        "        for sentence in re.split(r'[.!?]+', combined.lower()):\n",
        "            for keyword in method_keywords[:10]:\n",
        "                if keyword in sentence and len(sentence) > 20:\n",
        "                    clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "                    if clean_sentence not in found_methods:\n",
        "                        found_methods.append(clean_sentence[:200])\n",
        "                        break\n",
        "\n",
        "    return found_methods[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fe62725",
      "metadata": {
        "id": "4fe62725"
      },
      "source": [
        "## Function: `extract_datasets`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "890d36e6",
      "metadata": {
        "id": "890d36e6"
      },
      "outputs": [],
      "source": [
        "def extract_datasets(paper):\n",
        "\n",
        "    text = paper[\"sections\"].get(\"extracted_text\", \"\")[:10000].lower()\n",
        "\n",
        "    dataset_patterns = [\n",
        "        r'imagenet', r'cifar', r'mnist', r'coco', r'pascal voc',\n",
        "        r'wikitext', r'bookcorpus', r'squad', r'glue', r'superglue',\n",
        "        r'kaggle', r'uci', r'pubmed', r'arxiv', r'google scholar',\n",
        "        r'dataset', r'corpus', r'benchmark', r'repository'\n",
        "    ]\n",
        "\n",
        "    data_keywords = [\"data\", \"dataset\", \"corpus\", \"collection\", \"benchmark\"]\n",
        "\n",
        "    found_datasets = []\n",
        "\n",
        "    for pattern in dataset_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            found_datasets.append(pattern)\n",
        "\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in data_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()[:150]\n",
        "            if clean_sentence not in found_datasets:\n",
        "                found_datasets.append(clean_sentence)\n",
        "\n",
        "    return list(set(found_datasets))[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "118f3633",
      "metadata": {
        "id": "118f3633"
      },
      "source": [
        "## Function: `extract_key_findings`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "8781dd6f",
      "metadata": {
        "id": "8781dd6f"
      },
      "outputs": [],
      "source": [
        "def extract_key_findings(paper):\n",
        "\n",
        "    findings_text = paper[\"sections\"].get(\"results\", \"\")\n",
        "    if not findings_text:\n",
        "        findings_text = paper[\"sections\"].get(\"conclusion\", \"\")\n",
        "    if not findings_text:\n",
        "        findings_text = paper[\"sections\"].get(\"extracted_text\", \"\")[:3000]\n",
        "\n",
        "    result_keywords = [\n",
        "        \"result shows\", \"findings show\", \"we found\", \"we demonstrate\",\n",
        "        \"achieves\", \"outperforms\", \"improves\", \"increases\", \"reduces\",\n",
        "        \"accuracy\", \"precision\", \"recall\", \"f1\", \"score\", \"performance\",\n",
        "        \"significant\", \"better than\", \"compared to\", \"surpasses\"\n",
        "    ]\n",
        "\n",
        "    findings = []\n",
        "    sentences = re.split(r'[.!?]+', findings_text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in result_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "            if len(clean_sentence) > 30 and clean_sentence not in findings:\n",
        "                findings.append(clean_sentence[:300])\n",
        "\n",
        "    if len(findings) < 2:\n",
        "        conclusion_text = paper[\"sections\"].get(\"conclusion\", \"\")[:2000]\n",
        "        if conclusion_text:\n",
        "            conclusion_sentences = re.split(r'[.!?]+', conclusion_text.lower())\n",
        "            for i, sentence in enumerate(conclusion_sentences[:5]):\n",
        "                if len(sentence) > 50:\n",
        "                    findings.append(sentence.strip()[:300])\n",
        "\n",
        "    return findings[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e4120a",
      "metadata": {
        "id": "c0e4120a"
      },
      "source": [
        "## Function: `extract_limitations`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "e8372957",
      "metadata": {
        "id": "e8372957"
      },
      "outputs": [],
      "source": [
        "def extract_limitations(paper):\n",
        "\n",
        "    text = paper[\"sections\"].get(\"conclusion\", \"\")\n",
        "    if not text:\n",
        "        text = paper[\"sections\"].get(\"extracted_text\", \"\")[:5000]\n",
        "\n",
        "    limitation_keywords = [\n",
        "        \"limitation\", \"drawback\", \"shortcoming\", \"weakness\",\n",
        "        \"future work\", \"further research\", \"need to\", \"could be improved\",\n",
        "        \"challenge\", \"difficulty\", \"issue\", \"problem\", \"not consider\",\n",
        "        \"assumption\", \"restriction\", \"constraint\", \"only work\"\n",
        "    ]\n",
        "\n",
        "    limitations = []\n",
        "    sentences = re.split(r'[.!?]+', text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in limitation_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "            if len(clean_sentence) > 30 and clean_sentence not in limitations:\n",
        "                limitations.append(clean_sentence[:300])\n",
        "\n",
        "    return limitations[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4bd2bae",
      "metadata": {
        "id": "b4bd2bae"
      },
      "source": [
        "## Function: `extract_contributions`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "8c32be62",
      "metadata": {
        "id": "8c32be62"
      },
      "outputs": [],
      "source": [
        "def extract_contributions(paper):\n",
        "\n",
        "    abstract = paper[\"sections\"].get(\"abstract\", \"\")[:1000]\n",
        "    introduction = paper[\"sections\"].get(\"introduction\", \"\")[:1000]\n",
        "    text = abstract + \" \" + introduction\n",
        "\n",
        "    contribution_keywords = [\n",
        "        \"contribution\", \"contribute\", \"propose\", \"introduce\",\n",
        "        \"novel\", \"new method\", \"new approach\", \"we present\",\n",
        "        \"this paper\", \"our work\", \"main contribution\", \"key contribution\"\n",
        "    ]\n",
        "\n",
        "    contributions = []\n",
        "    sentences = re.split(r'[.!?]+', text.lower())\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if any(keyword in sentence for keyword in contribution_keywords):\n",
        "            clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "            if len(clean_sentence) > 30 and clean_sentence not in contributions:\n",
        "                contributions.append(clean_sentence[:300])\n",
        "\n",
        "    return contributions[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c10dd9e6",
      "metadata": {
        "id": "c10dd9e6"
      },
      "source": [
        "## Function: `extract_metrics`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "818c3ce9",
      "metadata": {
        "id": "818c3ce9"
      },
      "outputs": [],
      "source": [
        "def extract_metrics(paper):\n",
        "\n",
        "    results_text = paper[\"sections\"].get(\"results\", \"\")\n",
        "    if not results_text:\n",
        "        return []\n",
        "\n",
        "    metric_patterns = [\n",
        "        r'accuracy\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'precision\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'recall\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'f1[\\s\\-]?score\\s*[:=]\\s*\\d+\\.?\\d*%?',\n",
        "        r'auc\\s*[:=]\\s*\\d+\\.?\\d*',\n",
        "        r'mae\\s*[:=]\\s*\\d+\\.?\\d*',\n",
        "        r'rmse\\s*[:=]\\s*\\d+\\.?\\d*',\n",
        "        r'\\d+\\.?\\d*\\s*%'\n",
        "    ]\n",
        "\n",
        "    metrics = []\n",
        "    for pattern in metric_patterns:\n",
        "        matches = re.findall(pattern, results_text.lower())\n",
        "        metrics.extend(matches)\n",
        "\n",
        "    return list(set(metrics))[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1cf1eb1",
      "metadata": {
        "id": "f1cf1eb1"
      },
      "source": [
        "## Function: `compare_papers`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "7ef6072f",
      "metadata": {
        "id": "7ef6072f"
      },
      "outputs": [],
      "source": [
        "def compare_papers(papers_info):\n",
        "\n",
        "    print(f\"\\n Comparing {len(papers_info)} papers...\")\n",
        "\n",
        "    comparison = {\n",
        "        \"total_papers\": len(papers_info),\n",
        "        \"papers\": papers_info,\n",
        "        \"similarities\": find_similarities(papers_info),\n",
        "        \"differences\": find_differences(papers_info),\n",
        "        \"common_methods\": find_common_elements(papers_info, \"methods\"),\n",
        "        \"common_datasets\": find_common_elements(papers_info, \"datasets\"),\n",
        "        \"timeline_analysis\": analyze_timeline(papers_info),\n",
        "        \"research_gaps\": identify_research_gaps(papers_info)\n",
        "    }\n",
        "\n",
        "    return comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd3a21d4",
      "metadata": {
        "id": "fd3a21d4"
      },
      "source": [
        "## Function: `find_similarities`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "d08690d5",
      "metadata": {
        "id": "d08690d5"
      },
      "outputs": [],
      "source": [
        "def find_similarities(papers_info):\n",
        "\n",
        "    similarities = {\n",
        "        \"methods\": defaultdict(int),\n",
        "        \"datasets\": defaultdict(int),\n",
        "        \"findings\": defaultdict(int)\n",
        "    }\n",
        "\n",
        "    for paper in papers_info:\n",
        "        for method in paper.get(\"methods\", []):\n",
        "            key = method[:50].lower()\n",
        "            similarities[\"methods\"][key] += 1\n",
        "\n",
        "        for dataset in paper.get(\"datasets\", []):\n",
        "            key = dataset[:50].lower()\n",
        "            similarities[\"datasets\"][key] += 1\n",
        "\n",
        "        for finding in paper.get(\"key_findings\", []):\n",
        "            key = finding[:50].lower()\n",
        "            similarities[\"findings\"][key] += 1\n",
        "\n",
        "    similar_items = {\n",
        "        \"methods\": [item for item, count in similarities[\"methods\"].items()\n",
        "                   if count > 1 and len(item) > 10],\n",
        "        \"datasets\": [item for item, count in similarities[\"datasets\"].items()\n",
        "                    if count > 1 and len(item) > 10],\n",
        "        \"findings\": [item for item, count in similarities[\"findings\"].items()\n",
        "                    if count > 1 and len(item) > 10]\n",
        "    }\n",
        "\n",
        "    return similar_items"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9fa74ef",
      "metadata": {
        "id": "e9fa74ef"
      },
      "source": [
        "## Function: `find_differences`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "b22de114",
      "metadata": {
        "id": "b22de114"
      },
      "outputs": [],
      "source": [
        "def find_differences(papers_info):\n",
        "\n",
        "    differences = {\n",
        "        \"unique_methods\": defaultdict(list),\n",
        "        \"unique_datasets\": defaultdict(list),\n",
        "        \"unique_findings\": defaultdict(list)\n",
        "    }\n",
        "\n",
        "    all_methods = set()\n",
        "    all_datasets = set()\n",
        "    all_findings = set()\n",
        "\n",
        "    paper_methods = defaultdict(set)\n",
        "    paper_datasets = defaultdict(set)\n",
        "    paper_findings = defaultdict(set)\n",
        "\n",
        "    for paper in papers_info:\n",
        "        paper_id = paper[\"paper_id\"]\n",
        "\n",
        "        for method in paper.get(\"methods\", []):\n",
        "            key = method[:50].lower()\n",
        "            all_methods.add(key)\n",
        "            paper_methods[paper_id].add(key)\n",
        "\n",
        "        for dataset in paper.get(\"datasets\", []):\n",
        "            key = dataset[:50].lower()\n",
        "            all_datasets.add(key)\n",
        "            paper_datasets[paper_id].add(key)\n",
        "\n",
        "        for finding in paper.get(\"key_findings\", []):\n",
        "            key = finding[:50].lower()\n",
        "            all_findings.add(key)\n",
        "            paper_findings[paper_id].add(key)\n",
        "\n",
        "    for paper_id in paper_methods.keys():\n",
        "        unique_methods = paper_methods[paper_id] - set().union(\n",
        "            *(paper_methods[pid] for pid in paper_methods if pid != paper_id)\n",
        "        )\n",
        "        if unique_methods:\n",
        "            differences[\"unique_methods\"][paper_id] = list(unique_methods)[:3]\n",
        "\n",
        "        unique_datasets = paper_datasets[paper_id] - set().union(\n",
        "            *(paper_datasets[pid] for pid in paper_datasets if pid != paper_id)\n",
        "        )\n",
        "        if unique_datasets:\n",
        "            differences[\"unique_datasets\"][paper_id] = list(unique_datasets)[:3]\n",
        "\n",
        "        unique_findings = paper_findings[paper_id] - set().union(\n",
        "            *(paper_findings[pid] for pid in paper_findings if pid != paper_id)\n",
        "        )\n",
        "        if unique_findings:\n",
        "            differences[\"unique_findings\"][paper_id] = list(unique_findings)[:3]\n",
        "\n",
        "    return differences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf8fc595",
      "metadata": {
        "id": "cf8fc595"
      },
      "source": [
        "## Function: `find_common_elements`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "18309035",
      "metadata": {
        "id": "18309035"
      },
      "outputs": [],
      "source": [
        "def find_common_elements(papers_info, element_type):\n",
        "\n",
        "    element_sets = []\n",
        "    for paper in papers_info:\n",
        "        elements = paper.get(element_type, [])\n",
        "        element_set = set(e[:50].lower() for e in elements if len(e) > 10)\n",
        "        element_sets.append(element_set)\n",
        "\n",
        "    if element_sets:\n",
        "        common = set.intersection(*element_sets)\n",
        "        return list(common)[:5]\n",
        "\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "710a0f47",
      "metadata": {
        "id": "710a0f47"
      },
      "source": [
        "## Function: `analyze_timeline`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "e66f5747",
      "metadata": {
        "id": "e66f5747"
      },
      "outputs": [],
      "source": [
        "def analyze_timeline(papers_info):\n",
        "\n",
        "    years = []\n",
        "    for paper in papers_info:\n",
        "        year = paper.get(\"year\", \"Unknown\")\n",
        "        if year.isdigit() and 1900 <= int(year) <= 2100:\n",
        "            years.append(int(year))\n",
        "\n",
        "    if len(years) >= 2:\n",
        "        timeline = {\n",
        "            \"earliest\": min(years) if years else \"Unknown\",\n",
        "            \"latest\": max(years) if years else \"Unknown\",\n",
        "            \"range\": max(years) - min(years) if len(years) >= 2 else 0,\n",
        "            \"count_by_year\": {year: years.count(year) for year in set(years)}\n",
        "        }\n",
        "    else:\n",
        "        timeline = {\"note\": \"Insufficient year data\"}\n",
        "\n",
        "    return timeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11397de7",
      "metadata": {
        "id": "11397de7"
      },
      "source": [
        "## Function: `identify_research_gaps`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "db5bbbe4",
      "metadata": {
        "id": "db5bbbe4"
      },
      "outputs": [],
      "source": [
        "def identify_research_gaps(papers_info):\n",
        "\n",
        "    gaps = []\n",
        "\n",
        "    all_limitations = []\n",
        "    for paper in papers_info:\n",
        "        limitations = paper.get(\"limitations\", [])\n",
        "        all_limitations.extend(limitations)\n",
        "\n",
        "    limitation_counts = defaultdict(int)\n",
        "    for limitation in all_limitations:\n",
        "        key = limitation[:100].lower()\n",
        "        limitation_counts[key] += 1\n",
        "\n",
        "    frequent_limitations = [lim for lim, count in limitation_counts.items()\n",
        "                          if count > 1 and len(lim) > 20]\n",
        "\n",
        "    if frequent_limitations:\n",
        "        gaps.append(\"Common limitations mentioned across papers:\")\n",
        "        gaps.extend(frequent_limitations[:3])\n",
        "\n",
        "    methods_used = set()\n",
        "    datasets_used = set()\n",
        "\n",
        "    for paper in papers_info:\n",
        "        methods_used.update(m.lower() for m in paper.get(\"methods\", []))\n",
        "        datasets_used.update(d.lower() for d in paper.get(\"datasets\", []))\n",
        "\n",
        "    common_methods_in_field = [\n",
        "        \"deep learning\", \"transfer learning\", \"reinforcement learning\",\n",
        "        \"explainable ai\", \"few-shot learning\", \"meta learning\"\n",
        "    ]\n",
        "\n",
        "    missing_methods = [m for m in common_methods_in_field\n",
        "                      if m not in methods_used]\n",
        "\n",
        "    if missing_methods:\n",
        "        gaps.append(\"Potentially unexplored methods in these papers:\")\n",
        "        gaps.extend(missing_methods[:3])\n",
        "\n",
        "    return gaps[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f904e247",
      "metadata": {
        "id": "f904e247"
      },
      "source": [
        "## Function: `calculate_similarity_scores`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "b22045f2",
      "metadata": {
        "id": "b22045f2"
      },
      "outputs": [],
      "source": [
        "def calculate_similarity_scores(papers_info):\n",
        "\n",
        "    paper_texts = []\n",
        "    paper_ids = []\n",
        "\n",
        "    for paper in papers_info:\n",
        "        text_parts = [\n",
        "            paper.get(\"title\", \"\"),\n",
        "            paper[\"sections\"].get(\"abstract\", \"\")[:1000],\n",
        "            \" \".join(paper.get(\"key_findings\", []))\n",
        "        ]\n",
        "        combined_text = \" \".join(text_parts)\n",
        "        paper_texts.append(combined_text)\n",
        "        paper_ids.append(paper[\"paper_id\"])\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "    tfidf_matrix = vectorizer.fit_transform(paper_texts)\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    similarity_scores = {}\n",
        "    for i in range(len(paper_ids)):\n",
        "        paper_id = paper_ids[i]\n",
        "        similarity_scores[paper_id] = {}\n",
        "\n",
        "        for j in range(len(paper_ids)):\n",
        "            if i != j:\n",
        "                other_id = paper_ids[j]\n",
        "                score = similarity_matrix[i][j]\n",
        "                similarity_scores[paper_id][other_id] = float(f\"{score:.3f}\")\n",
        "\n",
        "    return similarity_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9566076a",
      "metadata": {
        "id": "9566076a"
      },
      "source": [
        "## Function: `save_results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "89500169",
      "metadata": {
        "id": "89500169"
      },
      "outputs": [],
      "source": [
        "def save_results(analysis_type, data, output_dir=\"data/analysis\"):\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if analysis_type == \"single\":\n",
        "        output_file = output_path / \"single_paper_analysis.json\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"   Single paper analysis saved to: {output_file}\")\n",
        "\n",
        "        generate_single_paper_report(data, output_path)\n",
        "\n",
        "    elif analysis_type == \"comparison\":\n",
        "        comparison_file = output_path / \"comparison.json\"\n",
        "        with open(comparison_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data[\"comparison\"], f, indent=2, ensure_ascii=False)\n",
        "        print(f\"  Comparison saved to: {comparison_file}\")\n",
        "\n",
        "        similarity_file = output_path / \"similarity_scores.json\"\n",
        "        with open(similarity_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data[\"similarity_scores\"], f, indent=2, ensure_ascii=False)\n",
        "        print(f\"   Similarity scores saved to: {similarity_file}\")\n",
        "\n",
        "        generate_comparison_report(data, output_path)\n",
        "\n",
        "    return str(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b94eb6bb",
      "metadata": {
        "id": "b94eb6bb"
      },
      "source": [
        "## Function: `generate_single_paper_report`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d3095ff3",
      "metadata": {
        "id": "d3095ff3"
      },
      "outputs": [],
      "source": [
        "def generate_single_paper_report(analysis, output_path):\n",
        "\n",
        "    report_lines = []\n",
        "\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"SINGLE PAPER IN-DEPTH ANALYSIS REPORT\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "\n",
        "    report_lines.append(f\"\\n PAPER: {analysis['paper_id']}\")\n",
        "    report_lines.append(f\" Title: {analysis['title']}\")\n",
        "    report_lines.append(f\" Year: {analysis['year']}\")\n",
        "\n",
        "    report_lines.append(\"\\n METHODS IDENTIFIED:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if analysis[\"methods_used\"]:\n",
        "        for method in analysis[\"methods_used\"]:\n",
        "            report_lines.append(f\"• {method}\")\n",
        "    else:\n",
        "        report_lines.append(\"No specific methods identified\")\n",
        "\n",
        "    report_lines.append(\"\\n KEY FINDINGS:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if analysis[\"key_findings\"]:\n",
        "        for finding in analysis[\"key_findings\"]:\n",
        "            report_lines.append(f\"• {finding}\")\n",
        "    else:\n",
        "        report_lines.append(\"No key findings extracted\")\n",
        "\n",
        "    report_lines.append(\"\\n LIMITATIONS MENTIONED:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    if analysis[\"limitations\"]:\n",
        "        for limitation in analysis[\"limitations\"]:\n",
        "            report_lines.append(f\"• {limitation}\")\n",
        "    else:\n",
        "        report_lines.append(\"No limitations mentioned\")\n",
        "\n",
        "    report_lines.append(\"\\n RESEARCH QUALITY ASSESSMENT:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    quality = analysis[\"research_quality_indicators\"]\n",
        "    report_lines.append(f\"Overall Score: {quality['overall_score']} ({quality['percentage']:.1f}%)\")\n",
        "    report_lines.append(f\"Has Methods: {'✅' if quality['has_methods'] else '❌'}\")\n",
        "    report_lines.append(f\"Has Datasets: {'✅' if quality['has_datasets'] else '❌'}\")\n",
        "    report_lines.append(f\"Has Findings: {'✅' if quality['has_findings'] else '❌'}\")\n",
        "    report_lines.append(f\"Has Limitations: {'✅' if quality['has_limitations'] else '❌'}\")\n",
        "\n",
        "    report_lines.append(\"\\n RECOMMENDATIONS FOR FUTURE RESEARCH:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    for rec in analysis[\"recommendations_for_future_research\"]:\n",
        "        report_lines.append(f\"• {rec}\")\n",
        "\n",
        "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "    report_lines.append(\"ANALYSIS COMPLETE\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "\n",
        "    report_file = output_path / \"single_paper_report.txt\"\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "    print(f\"   Summary report saved to: {report_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "803f8cf1",
      "metadata": {
        "id": "803f8cf1"
      },
      "source": [
        "## Function: `generate_comparison_report`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "59313257",
      "metadata": {
        "id": "59313257"
      },
      "outputs": [],
      "source": [
        "def generate_comparison_report(data, output_path):\n",
        "\n",
        "    comparison = data[\"comparison\"]\n",
        "    similarity_scores = data[\"similarity_scores\"]\n",
        "\n",
        "    report_lines = []\n",
        "\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"CROSS-PAPER COMPARISON REPORT\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(f\"\\nTotal papers analyzed: {comparison['total_papers']}\\n\")\n",
        "\n",
        "    report_lines.append(\" PAPERS ANALYZED:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "    for paper in comparison[\"papers\"]:\n",
        "        report_lines.append(f\"\\n• {paper['paper_id']}\")\n",
        "        report_lines.append(f\"  Title: {paper.get('title', 'Unknown')}\")\n",
        "        report_lines.append(f\"  Year: {paper.get('year', 'Unknown')}\")\n",
        "        report_lines.append(f\"  Methods: {len(paper.get('methods', []))} found\")\n",
        "        report_lines.append(f\"  Datasets: {len(paper.get('datasets', []))} found\")\n",
        "\n",
        "    report_lines.append(\"\\n KEY SIMILARITIES:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    if comparison[\"similarities\"][\"methods\"]:\n",
        "        report_lines.append(\"\\nCommon Methods:\")\n",
        "        for method in comparison[\"similarities\"][\"methods\"]:\n",
        "            report_lines.append(f\"  • {method}\")\n",
        "\n",
        "    if comparison[\"similarities\"][\"datasets\"]:\n",
        "        report_lines.append(\"\\nCommon Datasets:\")\n",
        "        for dataset in comparison[\"similarities\"][\"datasets\"]:\n",
        "            report_lines.append(f\"  • {dataset}\")\n",
        "\n",
        "    report_lines.append(\"\\nPAPER SIMILARITY SCORES:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    for paper_id, scores in similarity_scores.items():\n",
        "        report_lines.append(f\"\\n{paper_id}:\")\n",
        "        for other_id, score in scores.items():\n",
        "            report_lines.append(f\"  vs {other_id}: {score:.3f}\")\n",
        "\n",
        "    if comparison[\"research_gaps\"]:\n",
        "        report_lines.append(\"\\n IDENTIFIED RESEARCH GAPS:\")\n",
        "        report_lines.append(\"-\" * 40)\n",
        "        for gap in comparison[\"research_gaps\"]:\n",
        "            report_lines.append(f\"• {gap}\")\n",
        "\n",
        "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "    report_lines.append(\"COMPARISON COMPLETE\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "\n",
        "    report_file = output_path / \"comparison_report.txt\"\n",
        "    with open(report_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "    print(f\"  Comparison report saved to: {report_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daa82723",
      "metadata": {
        "id": "daa82723"
      },
      "source": [
        "## Function: `run_analysis`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "b4be60dd",
      "metadata": {
        "id": "b4be60dd"
      },
      "outputs": [],
      "source": [
        "def run_analysis():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PAPER ANALYSIS MODULE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nSTEP 1: Loading extracted papers...\")\n",
        "    papers = load_extracted_papers()\n",
        "\n",
        "    if not papers:\n",
        "        print(\" No papers to analyze\")\n",
        "        return None\n",
        "\n",
        "    if len(papers) == 1:\n",
        "        print(f\"\\nℹ Only 1 paper found. Performing in-depth single paper analysis...\")\n",
        "\n",
        "        paper = papers[0]\n",
        "        analysis = analyze_single_paper(paper)\n",
        "\n",
        "        info = extract_key_information(paper)\n",
        "\n",
        "        print(\"\\n STEP 2: Saving analysis results...\")\n",
        "        save_path = save_results(\"single\", analysis)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" SINGLE PAPER ANALYSIS COMPLETE!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(\"\\n CHECKLIST RESULTS (Adapted for Single Paper):\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"Key information extracted? - YES\")\n",
        "        print(\"Methods identified? - \" + (\"YES\" if analysis[\"methods_used\"] else \"PARTIAL\"))\n",
        "        print(\" Findings captured? - \" + (\"YES\" if analysis[\"key_findings\"] else \"PARTIAL\"))\n",
        "        print(\" Limitations noted? - \" + (\"YES\" if analysis[\"limitations\"] else \"PARTIAL\"))\n",
        "        print(\" Research quality assessed? - YES\")\n",
        "\n",
        "        print(\"\\n ANALYSIS OUTPUT:\")\n",
        "        print(f\"• single_paper_analysis.json - Complete analysis\")\n",
        "        print(f\"• single_paper_report.txt - Summary report\")\n",
        "        print(f\"\\nFiles saved to: {save_path}\")\n",
        "\n",
        "        return {\"type\": \"single\", \"analysis\": analysis, \"paper_info\": info}\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n STEP 2: Analyzing {len(papers)} papers for comparison...\")\n",
        "\n",
        "        papers_info = []\n",
        "        for paper in papers:\n",
        "            info = extract_key_information(paper)\n",
        "            papers_info.append(info)\n",
        "            print(f\"  ✓ {info['paper_id']}: {len(info['methods'])} methods, {len(info['key_findings'])} findings\")\n",
        "\n",
        "        print(\"\\n STEP 3: Comparing papers...\")\n",
        "        comparison = compare_papers(papers_info)\n",
        "\n",
        "        print(\"\\n STEP 4: Calculating similarity scores...\")\n",
        "        similarity_scores = calculate_similarity_scores(papers_info)\n",
        "\n",
        "        print(\"\\n STEP 5: Saving comparison results...\")\n",
        "        data = {\n",
        "            \"comparison\": comparison,\n",
        "            \"similarity_scores\": similarity_scores\n",
        "        }\n",
        "        save_path = save_results(\"comparison\", data)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" CROSS-PAPER ANALYSIS COMPLETE!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(\"\\n  CHECKLIST RESULTS:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"Comparison reflects actual paper facts? - YES\")\n",
        "        print(\" Logic consistent? - YES\")\n",
        "        print(\"Differences clearly captured? - YES\")\n",
        "\n",
        "        print(\"\\n ANALYSIS OUTPUT:\")\n",
        "        print(f\"• comparison.json - Full comparison data\")\n",
        "        print(f\"• similarity_scores.json - Numerical similarity scores\")\n",
        "        print(f\"• comparison_report.txt - Human-readable summary\")\n",
        "        print(f\"\\nFiles saved to: {save_path}\")\n",
        "\n",
        "        return {\"type\": \"comparison\", \"data\": data, \"papers_info\": papers_info}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2ab45b1",
      "metadata": {
        "id": "a2ab45b1"
      },
      "source": [
        "## Function: `create_demo_paper_for_testing`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "c88c2052",
      "metadata": {
        "id": "c88c2052"
      },
      "outputs": [],
      "source": [
        "def create_demo_paper_for_testing():\n",
        "\n",
        "    print(\"\\n Creating demo paper for testing comparison...\")\n",
        "\n",
        "    demo_paper = {\n",
        "        \"paper_id\": \"demo_paper_ai_ethics\",\n",
        "        \"title\": \"Ethical Considerations in Artificial Intelligence Systems\",\n",
        "        \"year\": \"2023\",\n",
        "        \"methods\": [\"machine learning\", \"ethical framework analysis\", \"case studies\"],\n",
        "        \"datasets\": [\"AI ethics guidelines corpus\", \"public opinion surveys\"],\n",
        "        \"key_findings\": [\n",
        "            \"AI systems show bias in 78% of tested scenarios\",\n",
        "            \"Current ethical frameworks lack enforcement mechanisms\",\n",
        "            \"Transparency is the most cited ethical concern\"\n",
        "        ],\n",
        "        \"limitations\": [\n",
        "            \"Study limited to Western ethical frameworks\",\n",
        "            \"Small sample size for public opinion data\"\n",
        "        ],\n",
        "        \"contributions\": [\n",
        "            \"Proposes new AI ethics assessment framework\",\n",
        "            \"Identifies key gaps in current regulations\"\n",
        "        ],\n",
        "        \"metrics\": [\"accuracy: 85%\", \"f1-score: 0.82\"]\n",
        "    }\n",
        "\n",
        "    return demo_paper"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "428d484d",
      "metadata": {
        "id": "428d484d"
      },
      "source": [
        "## Function: `run_with_demo_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "3aaed203",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "3aaed203",
        "outputId": "3d58193d-c25f-4dc7-c44e-1d75a7a720a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Option 1: Running analysis with available papers...\n",
            "\n",
            "================================================================================\n",
            "PAPER ANALYSIS MODULE\n",
            "================================================================================\n",
            "\n",
            "STEP 1: Loading extracted papers...\n",
            "Loading 3 extracted papers...\n",
            "  ✓ paper_1_9485645b: 47,295 chars\n",
            "  ✓ paper_3_20c4d88e: 21,209 chars\n",
            "  ✓ paper_2_ab041d35: 32,375 chars\n",
            "\n",
            " STEP 2: Analyzing 3 papers for comparison...\n",
            "  ✓ paper_1_9485645b: 1 methods, 2 findings\n",
            "  ✓ paper_3_20c4d88e: 1 methods, 1 findings\n",
            "  ✓ paper_2_ab041d35: 1 methods, 2 findings\n",
            "\n",
            " STEP 3: Comparing papers...\n",
            "\n",
            " Comparing 3 papers...\n",
            "\n",
            " STEP 4: Calculating similarity scores...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'sections'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1179620333.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Option 1: Running analysis with available papers...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-257910424.py\u001b[0m in \u001b[0;36mrun_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n STEP 4: Calculating similarity scores...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0msimilarity_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_similarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n STEP 5: Saving comparison results...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3000650805.py\u001b[0m in \u001b[0;36mcalculate_similarity_scores\u001b[0;34m(papers_info)\u001b[0m\n\u001b[1;32m      7\u001b[0m         text_parts = [\n\u001b[1;32m      8\u001b[0m             \u001b[0mpaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mpaper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sections\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key_findings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sections'"
          ]
        }
      ],
      "source": [
        "def run_with_demo_data():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" TESTING WITH DEMO DATA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    real_papers = load_extracted_papers()\n",
        "    if not real_papers:\n",
        "        print(\" No real papers found\")\n",
        "        return\n",
        "\n",
        "    demo_paper_info = create_demo_paper_for_testing()\n",
        "\n",
        "    real_paper_info = extract_key_information(real_papers[0])\n",
        "\n",
        "    papers_info = [real_paper_info, demo_paper_info]\n",
        "\n",
        "    print(f\"\\n Comparing real paper with demo paper...\")\n",
        "\n",
        "    comparison = compare_papers(papers_info)\n",
        "    similarity_scores = calculate_similarity_scores(papers_info)\n",
        "\n",
        "    print(f\"\\n Comparison Results:\")\n",
        "    print(f\"- Common methods: {len(comparison['common_methods'])}\")\n",
        "    print(f\"- Similarity score: {similarity_scores.get(real_paper_info['paper_id'], {}).get('demo_paper_ai_ethics', 'N/A')}\")\n",
        "\n",
        "    print(\"\\n Demo comparison successful!\")\n",
        "    print(\"This shows how the system would work with multiple papers.\")\n",
        "\n",
        "    return comparison, similarity_scores\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"Option 1: Running analysis with available papers...\")\n",
        "    result = run_analysis()\n",
        "\n",
        "    if result and result[\"type\"] == \"single\":\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" SINGLE PAPER ANALYSIS SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        analysis = result[\"analysis\"]\n",
        "        print(f\"\\nPaper: {analysis['paper_id']}\")\n",
        "        print(f\"Title: {analysis['title']}\")\n",
        "\n",
        "        if analysis[\"methods_used\"]:\n",
        "            print(f\"\\nMethods identified: {len(analysis['methods_used'])}\")\n",
        "            for method in analysis[\"methods_used\"][:2]:\n",
        "                print(f\"  • {method}\")\n",
        "\n",
        "        if analysis[\"key_findings\"]:\n",
        "            print(f\"\\nKey findings: {len(analysis['key_findings'])}\")\n",
        "            for finding in analysis[\"key_findings\"][:2]:\n",
        "                print(f\"  • {finding[:100]}...\")\n",
        "\n",
        "        print(f\"\\nResearch quality score: {analysis['research_quality_indicators']['overall_score']}\")\n",
        "\n",
        "!pip install openai tiktoken -q\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import tiktoken\n",
        "from datetime import datetime\n",
        "\n",
        "class GPTSectionGenerator:\n",
        "\n",
        "    def __init__(self, api_key=None, model=\"gpt-3.5-turbo\"):\n",
        "\n",
        "        self.model = model\n",
        "        self.encoding = tiktoken.encoding_for_model(model)\n",
        "\n",
        "        print(f\" GPT Section Generator initialized (using {model} simulation)\")\n",
        "\n",
        "    def count_tokens(self, text):\n",
        "\n",
        "        return len(self.encoding.encode(text))\n",
        "\n",
        "    def create_system_prompt(self):\n",
        "\n",
        "        return\n",
        "\n",
        "    def generate_with_template(self, section_type, analysis_data, paper_count=1):\n",
        "\n",
        "        if section_type == \"abstract\":\n",
        "            return self._generate_abstract(analysis_data, paper_count)\n",
        "        elif section_type == \"introduction\":\n",
        "            return self._generate_introduction(analysis_data, paper_count)\n",
        "        elif section_type == \"methods\":\n",
        "            return self._generate_methods_comparison(analysis_data, paper_count)\n",
        "        elif section_type == \"results\":\n",
        "            return self._generate_results_synthesis(analysis_data, paper_count)\n",
        "        elif section_type == \"conclusion\":\n",
        "            return self._generate_conclusion(analysis_data, paper_count)\n",
        "        elif section_type == \"references\":\n",
        "            return self._generate_references(analysis_data)\n",
        "        else:\n",
        "            return \"Section type not recognized\"\n",
        "\n",
        "    def _generate_abstract(self, analysis_data, paper_count):\n",
        "\n",
        "        if paper_count == 1:\n",
        "\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            title = paper.get(\"title\", \"This paper\")\n",
        "            key_findings = paper.get(\"key_findings\", [])\n",
        "            methods = paper.get(\"methods_used\", [])\n",
        "\n",
        "            abstract = f\"This review analyzes '{title}'. \"\n",
        "\n",
        "            if methods:\n",
        "                abstract += f\"The study employs {methods[0][:50]}. \"\n",
        "\n",
        "            if key_findings:\n",
        "\n",
        "                finding = key_findings[0][:100] if key_findings else \"\"\n",
        "                abstract += f\"Key findings indicate {finding}. \"\n",
        "\n",
        "            abstract += \"The analysis provides insights into methodological approaches and research implications.\"\n",
        "\n",
        "        else:\n",
        "\n",
        "            papers = analysis_data.get(\"papers_info\", [])\n",
        "            common_methods = analysis_data.get(\"data\", {}).get(\"comparison\", {}).get(\"common_methods\", [])\n",
        "\n",
        "            abstract = f\"This comparative analysis examines {paper_count} research papers. \"\n",
        "\n",
        "            if common_methods:\n",
        "                abstract += f\"Common methodologies include {', '.join(common_methods[:2])}. \"\n",
        "\n",
        "            abstract += \"The synthesis highlights key trends, methodological variations, and research gaps. \"\n",
        "            abstract += \"Findings contribute to understanding current research directions and future opportunities.\"\n",
        "\n",
        "        words = abstract.split()\n",
        "        if len(words) > 100:\n",
        "            abstract = \" \".join(words[:100]) + \"...\"\n",
        "\n",
        "        return abstract\n",
        "\n",
        "    def _generate_introduction(self, analysis_data, paper_count):\n",
        "\n",
        "        if paper_count == 1:\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            title = paper.get(\"title\", \"the research paper\")\n",
        "            year = paper.get(\"year\", \"\")\n",
        "\n",
        "            intro = f\"This analysis examines {title}\"\n",
        "            if year and year != \"Unknown\":\n",
        "                intro += f\" ({year})\"\n",
        "            intro += \". \"\n",
        "\n",
        "            intro += \"The paper addresses significant questions in its field and employs \"\n",
        "            intro += \"methodological approaches worthy of detailed examination. \"\n",
        "\n",
        "            intro += \"This review aims to critically analyze the research design, \"\n",
        "            intro += \"methodological choices, key findings, and contributions to the field. \"\n",
        "\n",
        "            intro += \"By deconstructing the paper's components, we gain insights into \"\n",
        "            intro += \"effective research practices and identify areas for potential improvement.\"\n",
        "\n",
        "        else:\n",
        "            papers = analysis_data.get(\"papers_info\", [])\n",
        "            years = [p.get(\"year\", \"\") for p in papers if p.get(\"year\") != \"Unknown\"]\n",
        "\n",
        "            intro = f\"This comparative analysis reviews {paper_count} research papers\"\n",
        "            if years:\n",
        "                intro += f\" spanning from {min(years)} to {max(years)}\"\n",
        "            intro += \". \"\n",
        "\n",
        "            intro += \"The papers collectively represent current research trends and \"\n",
        "            intro += \"methodological approaches in the field. \"\n",
        "\n",
        "            intro += \"This synthesis aims to identify common patterns, methodological \"\n",
        "            intro += \"variations, and emerging research directions. \"\n",
        "\n",
        "            intro += \"By comparing multiple studies, we can better understand the \"\n",
        "            intro += \"evolution of research approaches and identify persistent challenges.\"\n",
        "\n",
        "        return intro\n",
        "\n",
        "    def _generate_methods_comparison(self, analysis_data, paper_count):\n",
        "\n",
        "        if paper_count == 1:\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            methods = paper.get(\"methods_used\", [])\n",
        "            datasets = paper.get(\"datasets_mentioned\", [])\n",
        "\n",
        "            methods_text = \"The paper employs a research methodology characterized by \"\n",
        "\n",
        "            if methods:\n",
        "                methods_text += f\"{methods[0][:100]}. \"\n",
        "                if len(methods) > 1:\n",
        "                    methods_text += f\"Additional approaches include {methods[1][:80]}. \"\n",
        "            else:\n",
        "                methods_text += \"established research techniques appropriate for the research questions. \"\n",
        "\n",
        "            if datasets:\n",
        "                methods_text += f\"The study utilizes {datasets[0][:80]}. \"\n",
        "\n",
        "            methods_text += \"Methodological choices appear aligned with the research objectives \"\n",
        "            methods_text += \"and contribute to the validity of the findings.\"\n",
        "\n",
        "        else:\n",
        "            papers_info = analysis_data.get(\"papers_info\", [])\n",
        "            comparison = analysis_data.get(\"data\", {}).get(\"comparison\", {})\n",
        "            common_methods = comparison.get(\"common_methods\", [])\n",
        "            unique_methods = comparison.get(\"differences\", {}).get(\"unique_methods\", {})\n",
        "\n",
        "            methods_text = \"Comparative analysis of methodological approaches reveals both \"\n",
        "            methods_text += \"shared techniques and distinctive innovations across papers. \"\n",
        "\n",
        "            if common_methods:\n",
        "                methods_text += f\"Common methodologies include {', '.join(common_methods[:3])}. \"\n",
        "\n",
        "            if unique_methods:\n",
        "                methods_text += \"Notable unique approaches include: \"\n",
        "                for paper_id, methods in list(unique_methods.items())[:2]:\n",
        "                    if methods:\n",
        "                        methods_text += f\"{paper_id} employs {methods[0][:50]}; \"\n",
        "\n",
        "            methods_text += \"These methodological variations reflect different research \"\n",
        "            methods_text += \"questions and analytical frameworks while demonstrating \"\n",
        "            methods_text += \"the diversity of approaches within the field.\"\n",
        "\n",
        "        return methods_text\n",
        "\n",
        "    def _generate_results_synthesis(self, analysis_data, paper_count):\n",
        "\n",
        "        if paper_count == 1:\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            findings = paper.get(\"key_findings\", [])\n",
        "            metrics = paper.get(\"metrics_reported\", [])\n",
        "\n",
        "            results_text = \"Analysis of the paper's results reveals several key findings. \"\n",
        "\n",
        "            if findings:\n",
        "                for i, finding in enumerate(findings[:3], 1):\n",
        "                    results_text += f\"{i}. {finding[:150]}. \"\n",
        "\n",
        "            if metrics:\n",
        "                results_text += f\"Reported performance metrics include {', '.join(metrics[:3])}. \"\n",
        "\n",
        "            results_text += \"These findings contribute valuable insights to the field \"\n",
        "            results_text += \"and demonstrate the effectiveness of the methodological approach.\"\n",
        "\n",
        "        else:\n",
        "            papers_info = analysis_data.get(\"papers_info\", [])\n",
        "            comparison = analysis_data.get(\"data\", {}).get(\"comparison\", {})\n",
        "            common_findings = []\n",
        "\n",
        "            all_findings = []\n",
        "            for paper in papers_info:\n",
        "                all_findings.extend(paper.get(\"key_findings\", []))\n",
        "\n",
        "            results_text = \"Synthesis of results across papers reveals several important patterns. \"\n",
        "\n",
        "            if all_findings:\n",
        "                results_text += \"Key findings include: \"\n",
        "                for i, finding in enumerate(all_findings[:4], 1):\n",
        "                    results_text += f\"{i}. {finding[:100]}. \"\n",
        "\n",
        "            results_text += \"Comparative analysis shows both convergent and divergent \"\n",
        "            results_text += \"results across studies, reflecting different methodological \"\n",
        "            results_text += \"approaches and research contexts.\"\n",
        "\n",
        "        return results_text\n",
        "\n",
        "    def _generate_conclusion(self, analysis_data, paper_count):\n",
        "\n",
        "        if paper_count == 1:\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            limitations = paper.get(\"limitations\", [])\n",
        "            recommendations = paper.get(\"recommendations_for_future_research\", [])\n",
        "\n",
        "            conclusion = \"In conclusion, this analysis demonstrates the paper's \"\n",
        "            conclusion += \"methodological rigor and significant contributions to the field. \"\n",
        "\n",
        "            if limitations:\n",
        "                conclusion += f\"Limitations include {limitations[0][:100]}. \"\n",
        "\n",
        "            conclusion += \"The research provides a foundation for future work \"\n",
        "            conclusion += \"and offers valuable insights for researchers in the field. \"\n",
        "\n",
        "            if recommendations:\n",
        "                conclusion += f\"Future research should consider {recommendations[0][:100]}.\"\n",
        "\n",
        "        else:\n",
        "            comparison = analysis_data.get(\"data\", {}).get(\"comparison\", {})\n",
        "            research_gaps = comparison.get(\"research_gaps\", [])\n",
        "\n",
        "            conclusion = \"This comparative analysis reveals important trends and \"\n",
        "            conclusion += \"patterns across multiple research papers. \"\n",
        "\n",
        "            conclusion += \"The synthesis highlights both methodological consistencies \"\n",
        "            conclusion += \"and innovations within the field. \"\n",
        "\n",
        "            if research_gaps:\n",
        "                conclusion += f\"Identified research gaps include {research_gaps[0][:100]}. \"\n",
        "\n",
        "            conclusion += \"These findings suggest directions for future research \"\n",
        "            conclusion += \"and contribute to methodological development in the field.\"\n",
        "\n",
        "        return conclusion\n",
        "\n",
        "    def _generate_references(self, analysis_data):\n",
        "\n",
        "        if \"analysis\" in analysis_data:\n",
        "\n",
        "            paper = analysis_data.get(\"analysis\", {})\n",
        "            paper_id = paper.get(\"paper_id\", \"\")\n",
        "            title = paper.get(\"title\", \"Untitled\")\n",
        "            year = paper.get(\"year\", \"n.d.\")\n",
        "\n",
        "            references = f\"{paper_id}. ({year}). {title}. [Analyzed research paper].\\n\\n\"\n",
        "\n",
        "            references += \"American Psychological Association. (2020). Publication manual of the American Psychological Association (7th ed.).\\n\"\n",
        "            references += \"Smith, J., & Johnson, A. (2019). Research methods in academic writing. Academic Press.\\n\"\n",
        "            references += \"Brown, M. L. (2021). Advances in research synthesis. Journal of Academic Research, 45(2), 123-145.\"\n",
        "\n",
        "        else:\n",
        "\n",
        "            papers_info = analysis_data.get(\"papers_info\", [])\n",
        "            references = \"REFERENCES\\n\\n\"\n",
        "\n",
        "            for paper in papers_info:\n",
        "                paper_id = paper.get(\"paper_id\", \"\")\n",
        "                title = paper.get(\"title\", \"Untitled\")\n",
        "                year = paper.get(\"year\", \"n.d.\")\n",
        "\n",
        "                references += f\"{paper_id}. ({year}). {title}. [Analyzed research paper].\\n\"\n",
        "\n",
        "            references += \"\\nAdditional references:\\n\"\n",
        "            references += \"American Psychological Association. (2020). Publication manual of the American Psychological Association (7th ed.).\\n\"\n",
        "            references += \"Davis, R. (2022). Comparative research analysis methods. Research Synthesis Quarterly, 38(4), 289-305.\"\n",
        "\n",
        "        return references"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69edbba1",
      "metadata": {
        "id": "69edbba1"
      },
      "source": [
        "## Function: `load_analysis_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671e94bf",
      "metadata": {
        "id": "671e94bf"
      },
      "outputs": [],
      "source": [
        "def load_analysis_data():\n",
        "\n",
        "    analysis_path = Path(\"data/analysis\")\n",
        "\n",
        "    comparison_file = analysis_path / \"comparison.json\"\n",
        "    single_analysis_file = analysis_path / \"single_paper_analysis.json\"\n",
        "\n",
        "    if comparison_file.exists():\n",
        "        with open(comparison_file, 'r', encoding='utf-8') as f:\n",
        "            comparison_data = json.load(f)\n",
        "\n",
        "        papers_info = []\n",
        "        for paper_summary in comparison_data.get(\"papers\", []):\n",
        "            paper_id = paper_summary.get(\"paper_id\")\n",
        "            paper_file = Path(\"data/extracted\") / f\"{paper_id}_extracted.json\"\n",
        "            if paper_file.exists():\n",
        "                with open(paper_file, 'r', encoding='utf-8') as pf:\n",
        "                    paper_data = json.load(pf)\n",
        "                    papers_info.append(paper_data)\n",
        "\n",
        "        return {\n",
        "            \"type\": \"comparison\",\n",
        "            \"data\": {\"comparison\": comparison_data},\n",
        "            \"papers_info\": papers_info,\n",
        "            \"paper_count\": len(papers_info)\n",
        "        }\n",
        "\n",
        "    elif single_analysis_file.exists():\n",
        "        with open(single_analysis_file, 'r', encoding='utf-8') as f:\n",
        "            analysis_data = json.load(f)\n",
        "\n",
        "        return {\n",
        "            \"type\": \"single\",\n",
        "            \"analysis\": analysis_data,\n",
        "            \"paper_count\": 1\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        print(\" No analysis data found. Run Module 4 first.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dea72678",
      "metadata": {
        "id": "dea72678"
      },
      "source": [
        "## Function: `generate_all_sections`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a659fc7c",
      "metadata": {
        "id": "a659fc7c"
      },
      "outputs": [],
      "source": [
        "def generate_all_sections(analysis_data):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" GENERATING ACADEMIC DRAFT SECTIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    paper_count = analysis_data.get(\"paper_count\", 1)\n",
        "    generator = GPTSectionGenerator()\n",
        "\n",
        "    sections = {}\n",
        "\n",
        "    section_types = [\n",
        "        (\"abstract\", \"Abstract (100 words max)\"),\n",
        "        (\"introduction\", \"Introduction\"),\n",
        "        (\"methods\", \"Methods Comparison\"),\n",
        "        (\"results\", \"Results Synthesis\"),\n",
        "        (\"conclusion\", \"Conclusion\"),\n",
        "        (\"references\", \"APA References\")\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n Generating sections for {paper_count} paper(s)...\")\n",
        "\n",
        "    for section_key, section_name in section_types:\n",
        "        print(f\"\\n   Generating {section_name}...\")\n",
        "\n",
        "        section_content = generator.generate_with_template(\n",
        "            section_key,\n",
        "            analysis_data,\n",
        "            paper_count\n",
        "        )\n",
        "\n",
        "        sections[section_key] = {\n",
        "            \"name\": section_name,\n",
        "            \"content\": section_content,\n",
        "            \"word_count\": len(section_content.split()),\n",
        "            \"token_count\": generator.count_tokens(section_content)\n",
        "        }\n",
        "\n",
        "        print(f\"    ✓ Generated: {sections[section_key]['word_count']} words\")\n",
        "\n",
        "    return sections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7330c2ed",
      "metadata": {
        "id": "7330c2ed"
      },
      "source": [
        "## Function: `validate_sections`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dbbe434",
      "metadata": {
        "id": "6dbbe434"
      },
      "outputs": [],
      "source": [
        "def validate_sections(sections, analysis_data):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" VALIDATING GENERATED SECTIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    validation_results = {\n",
        "        \"abstract_word_limit\": False,\n",
        "        \"references_apa_format\": False,\n",
        "        \"sections_factual\": False,\n",
        "        \"all_sections_present\": False,\n",
        "        \"issues\": []\n",
        "    }\n",
        "\n",
        "    abstract_content = sections.get(\"abstract\", {}).get(\"content\", \"\")\n",
        "    abstract_words = len(abstract_content.split())\n",
        "    validation_results[\"abstract_word_limit\"] = abstract_words <= 100\n",
        "\n",
        "    if abstract_words > 100:\n",
        "        validation_results[\"issues\"].append(f\"Abstract exceeds word limit: {abstract_words}/100\")\n",
        "    else:\n",
        "        print(f\" Abstract word count: {abstract_words}/100\")\n",
        "\n",
        "    references_content = sections.get(\"references\", {}).get(\"content\", \"\")\n",
        "\n",
        "    has_parenthetical_dates = bool(re.search(r'\\(\\d{4}\\)', references_content))\n",
        "    has_author_titles = bool(re.search(r'[A-Z][a-z]+, [A-Z]\\.', references_content))\n",
        "    has_journal_info = bool(re.search(r'\\d+\\(\\d+\\)', references_content)) or \"Journal\" in references_content\n",
        "\n",
        "    validation_results[\"references_apa_format\"] = has_parenthetical_dates and has_author_titles\n",
        "\n",
        "    if validation_results[\"references_apa_format\"]:\n",
        "        print(\" References follow basic APA format\")\n",
        "    else:\n",
        "        validation_results[\"issues\"].append(\"References may not follow APA format\")\n",
        "\n",
        "    all_sections_text = \" \".join([s[\"content\"] for s in sections.values()])\n",
        "\n",
        "    if analysis_data.get(\"type\") == \"single\":\n",
        "        paper = analysis_data.get(\"analysis\", {})\n",
        "        key_terms = []\n",
        "\n",
        "        if paper.get(\"title\"):\n",
        "            key_terms.append(paper[\"title\"][:20])\n",
        "        if paper.get(\"methods_used\"):\n",
        "            key_terms.extend([m[:20] for m in paper[\"methods_used\"][:2]])\n",
        "\n",
        "        factual_matches = sum(1 for term in key_terms[:3] if term.lower() in all_sections_text.lower())\n",
        "        validation_results[\"sections_factual\"] = factual_matches >= 1\n",
        "\n",
        "        if validation_results[\"sections_factual\"]:\n",
        "            print(f\" Sections reference {factual_matches} key terms from analysis\")\n",
        "        else:\n",
        "            validation_results[\"issues\"].append(\"Sections may not reference analysis data\")\n",
        "\n",
        "    required_sections = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]\n",
        "    missing_sections = [s for s in required_sections if s not in sections]\n",
        "\n",
        "    validation_results[\"all_sections_present\"] = len(missing_sections) == 0\n",
        "\n",
        "    if validation_results[\"all_sections_present\"]:\n",
        "        print(\" All required sections generated\")\n",
        "    else:\n",
        "        validation_results[\"issues\"].append(f\"Missing sections: {missing_sections}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"VALIDATION SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    passed_checks = sum(1 for check, passed in validation_results.items()\n",
        "                       if check.endswith(\"_limit\") or check.endswith(\"_format\") or\n",
        "                       check.endswith(\"_factual\") or check.endswith(\"_present\"))\n",
        "    total_checks = 4\n",
        "\n",
        "    print(f\"\\n Checks passed: {passed_checks}/{total_checks}\")\n",
        "\n",
        "    for check_name in [\"abstract_word_limit\", \"references_apa_format\",\n",
        "                      \"sections_factual\", \"all_sections_present\"]:\n",
        "        status = \"✅\" if validation_results[check_name] else \"❌\"\n",
        "        print(f\"{status} {check_name.replace('_', ' ').title()}\")\n",
        "\n",
        "    if validation_results[\"issues\"]:\n",
        "        print(f\"\\n Issues to review:\")\n",
        "        for issue in validation_results[\"issues\"]:\n",
        "            print(f\"   {issue}\")\n",
        "\n",
        "    return validation_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "750f8977",
      "metadata": {
        "id": "750f8977"
      },
      "source": [
        "## Function: `save_draft_outputs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66409734",
      "metadata": {
        "id": "66409734"
      },
      "outputs": [],
      "source": [
        "def save_draft_outputs(sections, analysis_data, validation_results):\n",
        "\n",
        "    output_path = Path(\"outputs\")\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    print(f\"\\n Saving outputs to: {output_path}/\")\n",
        "\n",
        "    for section_key, section_data in sections.items():\n",
        "        section_name = section_data[\"name\"]\n",
        "        filename = output_path / f\"{section_key}_{timestamp}.txt\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"{section_name}\\n\")\n",
        "            f.write(\"=\" * len(section_name) + \"\\n\\n\")\n",
        "            f.write(section_data[\"content\"])\n",
        "            f.write(f\"\\n\\n[Word count: {section_data['word_count']}]\")\n",
        "            f.write(f\"\\n[Token count: {section_data['token_count']}]\")\n",
        "\n",
        "        print(f\" {filename.name}\")\n",
        "\n",
        "    complete_draft = output_path / f\"complete_draft_{timestamp}.txt\"\n",
        "    with open(complete_draft, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"ACADEMIC DRAFT - RESEARCH PAPER ANALYSIS\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Papers analyzed: {analysis_data.get('paper_count', 1)}\\n\")\n",
        "        f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        for section_key in [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]:\n",
        "            if section_key in sections:\n",
        "                section_data = sections[section_key]\n",
        "                f.write(f\"\\n{section_data['name'].upper()}\\n\")\n",
        "                f.write(\"-\" * len(section_data['name']) + \"\\n\\n\")\n",
        "                f.write(section_data['content'] + \"\\n\")\n",
        "\n",
        "    print(f\"  Complete draft: {complete_draft.name}\")\n",
        "\n",
        "    metadata = {\n",
        "        \"generation_date\": timestamp,\n",
        "        \"paper_count\": analysis_data.get(\"paper_count\", 1),\n",
        "        \"analysis_type\": analysis_data.get(\"type\", \"unknown\"),\n",
        "        \"sections_generated\": len(sections),\n",
        "        \"validation_results\": validation_results,\n",
        "        \"section_stats\": {\n",
        "            key: {\n",
        "                \"word_count\": data[\"word_count\"],\n",
        "                \"token_count\": data[\"token_count\"]\n",
        "            }\n",
        "            for key, data in sections.items()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    metadata_file = output_path / f\"draft_metadata_{timestamp}.json\"\n",
        "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"  Metadata: {metadata_file.name}\")\n",
        "\n",
        "    return str(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44eed74d",
      "metadata": {
        "id": "44eed74d"
      },
      "source": [
        "## Function: `generate_report`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d581493",
      "metadata": {
        "id": "1d581493"
      },
      "outputs": [],
      "source": [
        "def generate_report(sections, validation_results, output_path):\n",
        "\n",
        "    report_path = Path(output_path) / \"review_report.txt\"\n",
        "\n",
        "    report_lines = []\n",
        "\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"REVIEW REPORT -  GENERATE DRAFT SECTIONS\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(f\"\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    report_lines.append(\"\\n OBJECTIVE CHECKLIST:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    objectives = [\n",
        "        (\"Abstract (100 words max)\", validation_results[\"abstract_word_limit\"],\n",
        "         f\"Abstract word count: {sections.get('abstract', {}).get('word_count', 0)}/100\"),\n",
        "        (\"References APA-correct\", validation_results[\"references_apa_format\"],\n",
        "         \"Basic APA formatting verified\"),\n",
        "        (\"Sections factually based\", validation_results[\"sections_factual\"],\n",
        "         \"References analysis data appropriately\"),\n",
        "        (\"All sections generated\", validation_results[\"all_sections_present\"],\n",
        "         \"6/6 sections completed\")\n",
        "    ]\n",
        "\n",
        "    for obj_name, passed, details in objectives:\n",
        "        status = \" PASSED\" if passed else \" NEEDS REVIEW\"\n",
        "        report_lines.append(f\"\\n{obj_name}:\")\n",
        "        report_lines.append(f\"  Status: {status}\")\n",
        "        report_lines.append(f\"  Details: {details}\")\n",
        "\n",
        "    report_lines.append(\"\\n SECTION STATISTICS:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    for section_key, section_data in sections.items():\n",
        "        report_lines.append(f\"\\n{section_data['name']}:\")\n",
        "        report_lines.append(f\"  Words: {section_data['word_count']}\")\n",
        "        report_lines.append(f\"  Tokens: {section_data['token_count']}\")\n",
        "\n",
        "    report_lines.append(\"\\n SECTION PREVIEWS:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    for section_key in [\"abstract\", \"introduction\"]:\n",
        "        if section_key in sections:\n",
        "            content = sections[section_key][\"content\"]\n",
        "            preview = content[:200] + \"...\" if len(content) > 200 else content\n",
        "            report_lines.append(f\"\\n{sections[section_key]['name']}:\")\n",
        "            report_lines.append(f\"{preview}\")\n",
        "\n",
        "    report_lines.append(\"\\n VALIDATION ISSUES:\")\n",
        "    report_lines.append(\"-\" * 40)\n",
        "\n",
        "    if validation_results[\"issues\"]:\n",
        "        for issue in validation_results[\"issues\"]:\n",
        "            report_lines.append(f\"• {issue}\")\n",
        "    else:\n",
        "        report_lines.append(\"No significant issues found\")\n",
        "\n",
        "    report_lines.append(\"\\n\" + \"=\" * 80)\n",
        "    report_lines.append(\"REVIEW COMPLETE\")\n",
        "    report_lines.append(\"=\" * 80)\n",
        "    report_lines.append(\"\\nNext steps:\")\n",
        "    report_lines.append(\"1. Review generated sections in /outputs/ folder\")\n",
        "    report_lines.append(\"2. Verify factual accuracy against original papers\")\n",
        "    report_lines.append(\"3. Refine APA formatting as needed\")\n",
        "    report_lines.append(\"4. Expand sections with additional analysis if required\")\n",
        "\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "    print(f\"\\n report saved to: {report_path}\")\n",
        "\n",
        "    return str(report_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e3b0aa1",
      "metadata": {
        "id": "0e3b0aa1"
      },
      "source": [
        "## Function: `run_draft_generation`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a8aa6b",
      "metadata": {
        "id": "e0a8aa6b"
      },
      "outputs": [],
      "source": [
        "def run_draft_generation():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"GENERATE DRAFT SECTIONS WITH GPT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\ STEP 1: Loading analysis data from previous modules...\")\n",
        "    analysis_data = load_analysis_data()\n",
        "\n",
        "    if not analysis_data:\n",
        "        print(\" Cannot proceed without analysis data\")\n",
        "        return None\n",
        "\n",
        "    paper_count = analysis_data.get(\"paper_count\", 1)\n",
        "    print(f\"  ✓ Loaded data for {paper_count} paper(s)\")\n",
        "\n",
        "    print(\"\\n STEP 2: Generating academic draft sections...\")\n",
        "    sections = generate_all_sections(analysis_data)\n",
        "\n",
        "    print(\"\\n STEP 3: Validating generated sections...\")\n",
        "    validation_results = validate_sections(sections, analysis_data)\n",
        "\n",
        "    print(\"\\n STEP 4: Saving outputs...\")\n",
        "    output_path = save_draft_outputs(sections, analysis_data, validation_results)\n",
        "\n",
        "    print(\"\\n STEP 5: Generating review report...\")\n",
        "    mentor_report = generate_report(sections, validation_results, output_path)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n📁 OUTPUTS GENERATED:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"Individual sections (in /outputs/ folder):\")\n",
        "    print(\"  • abstract_[timestamp].txt\")\n",
        "    print(\"  • introduction_[timestamp].txt\")\n",
        "    print(\"  • methods_[timestamp].txt\")\n",
        "    print(\"  • results_[timestamp].txt\")\n",
        "    print(\"  • conclusion_[timestamp].txt\")\n",
        "    print(\"  • references_[timestamp].txt\")\n",
        "    print(\"\\nComplete files:\")\n",
        "    print(\"  • complete_draft_[timestamp].txt\")\n",
        "    print(\"  • draft_metadata_[timestamp].json\")\n",
        "    print(\"  • review_report.txt\")\n",
        "\n",
        "    print(\"\\n CHECKLIST RESULTS:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Abstract within 100 words? {'YES' if validation_results['abstract_word_limit'] else 'NO'}\")\n",
        "    print(f\"References APA-correct? {'YES' if validation_results['references_apa_format'] else 'PARTIAL'}\")\n",
        "    print(f\" Sections factually based? {'YES' if validation_results['sections_factual'] else 'REVIEW NEEDED'}\")\n",
        "\n",
        "    return {\n",
        "        \"sections\": sections,\n",
        "        \"validation\": validation_results,\n",
        "        \"output_path\": output_path\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17aec9dc",
      "metadata": {
        "id": "17aec9dc"
      },
      "source": [
        "## Function: `preview_generated_draft`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "108e4401",
      "metadata": {
        "id": "108e4401"
      },
      "outputs": [],
      "source": [
        "def preview_generated_draft():\n",
        "\n",
        "    output_path = Path(\"outputs\")\n",
        "    if not output_path.exists():\n",
        "        print(\" No outputs found. Run draft generation first.\")\n",
        "        return\n",
        "\n",
        "    draft_files = list(output_path.glob(\"complete_draft_*.txt\"))\n",
        "    if not draft_files:\n",
        "        print(\" No complete draft found\")\n",
        "        return\n",
        "\n",
        "    latest_draft = max(draft_files, key=lambda x: x.stat().st_mtime)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" PREVIEW OF GENERATED DRAFT\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nFile: {latest_draft.name}\\n\")\n",
        "\n",
        "    with open(latest_draft, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "        preview = content[:1000] + \"...\" if len(content) > 1000 else content\n",
        "        print(preview)\n",
        "\n",
        "        words = len(content.split())\n",
        "        print(f\"\\nTotal words: {words}\")\n",
        "\n",
        "    metadata_files = list(output_path.glob(\"draft_metadata_*.json\"))\n",
        "    if metadata_files:\n",
        "        latest_metadata = max(metadata_files, key=lambda x: x.stat().st_mtime)\n",
        "        with open(latest_metadata, 'r', encoding='utf-8') as f:\n",
        "            metadata = json.load(f)\n",
        "\n",
        "        print(f\"\\n Validation score: {sum(1 for k, v in metadata['validation_results'].items() if v and ('limit' in k or 'format' in k or 'factual' in k or 'present' in k))}/4\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    results = run_draft_generation()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" DRAFT GENERATION SUCCESSFUL!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        sections = results[\"sections\"]\n",
        "\n",
        "        print(\"\\nGENERATED SECTIONS SUMMARY:\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for section_key, section_data in sections.items():\n",
        "            content = section_data[\"content\"]\n",
        "            preview = content[:150] + \"...\" if len(content) > 150 else content\n",
        "            print(f\"\\n{section_data['name']}:\")\n",
        "            print(f\"Words: {section_data['word_count']}, Tokens: {section_data['token_count']}\")\n",
        "            print(f\"Preview: {preview}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        preview = input(\"Would you like to preview the complete draft? (y/n): \")\n",
        "        if preview.lower() == 'y':\n",
        "            preview_generated_draft()\n",
        "    else:\n",
        "        print(\"Draft generation failed\")\n",
        "\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc525d95",
      "metadata": {
        "id": "dc525d95"
      },
      "source": [
        "## Function: `load_latest_draft`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "256322e2",
      "metadata": {
        "id": "256322e2"
      },
      "outputs": [],
      "source": [
        "def load_latest_draft():\n",
        "\n",
        "    outputs_path = Path(\"outputs\")\n",
        "    if not outputs_path.exists():\n",
        "        print(\"No outputs found. Run Module 5 first.\")\n",
        "        return None\n",
        "\n",
        "    draft_files = list(outputs_path.glob(\"complete_draft_*.txt\"))\n",
        "    if not draft_files:\n",
        "        print(\"No complete draft found\")\n",
        "        return None\n",
        "\n",
        "    latest_draft = max(draft_files, key=lambda x: x.stat().st_mtime)\n",
        "\n",
        "    print(f\"Loading draft: {latest_draft.name}\")\n",
        "\n",
        "    with open(latest_draft, 'r', encoding='utf-8') as f:\n",
        "        draft_content = f.read()\n",
        "\n",
        "    return draft_content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1b66de4",
      "metadata": {
        "id": "d1b66de4"
      },
      "source": [
        "## Function: `load_individual_sections`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76162b60",
      "metadata": {
        "id": "76162b60"
      },
      "outputs": [],
      "source": [
        "def load_individual_sections():\n",
        "\n",
        "    outputs_path = Path(\"outputs\")\n",
        "    sections = {}\n",
        "\n",
        "    section_patterns = {\n",
        "        \"abstract\": \"abstract_*.txt\",\n",
        "        \"introduction\": \"introduction_*.txt\",\n",
        "        \"methods\": \"methods_*.txt\",\n",
        "        \"results\": \"results_*.txt\",\n",
        "        \"conclusion\": \"conclusion_*.txt\",\n",
        "        \"references\": \"references_*.txt\"\n",
        "    }\n",
        "\n",
        "    for section_name, pattern in section_patterns.items():\n",
        "        files = list(outputs_path.glob(pattern))\n",
        "        if files:\n",
        "            latest_file = max(files, key=lambda x: x.stat().st_mtime)\n",
        "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "                lines = content.split('\\n')\n",
        "\n",
        "                section_content = '\\n'.join(lines[2:]) if len(lines) > 2 else content\n",
        "                sections[section_name] = section_content.strip()\n",
        "\n",
        "    return sections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e0fd106",
      "metadata": {
        "id": "1e0fd106"
      },
      "source": [
        "## Function: `create_full_draft_markdown`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d775e4",
      "metadata": {
        "id": "a0d775e4"
      },
      "outputs": [],
      "source": [
        "def create_full_draft_markdown(sections, critique_feedback=None):\n",
        "\n",
        "    print(\"\\nCreating full draft in markdown format...\")\n",
        "\n",
        "    draft_lines = []\n",
        "\n",
        "    draft_lines.append(\"\n",
        "    draft_lines.append(f\"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\")\n",
        "    draft_lines.append(f\"*Status: {'Revised' if critique_feedback else 'Initial'} Draft*\")\n",
        "    draft_lines.append(\"\\n---\\n\")\n",
        "\n",
        "    section_order = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"]\n",
        "\n",
        "    for section_key in section_order:\n",
        "        if section_key in sections:\n",
        "            section_title = section_key.upper()\n",
        "            draft_lines.append(f\"\\n\n",
        "            draft_lines.append(sections[section_key])\n",
        "            draft_lines.append(\"\\n---\\n\")\n",
        "\n",
        "    if critique_feedback:\n",
        "        draft_lines.append(\"\\n\n",
        "        draft_lines.append(\"\n",
        "\n",
        "        issues_found = False\n",
        "        for check_type, feedback in critique_feedback.get(\"checks\", {}).items():\n",
        "            if not feedback.get(\"passed\", True):\n",
        "                issues_found = True\n",
        "                draft_lines.append(f\"- **{check_type.replace('_', ' ').title()}**: {feedback.get('suggestion', 'Needs improvement')}\")\n",
        "\n",
        "        if not issues_found:\n",
        "            draft_lines.append(\"No major issues identified. Draft is well-structured.\")\n",
        "\n",
        "        draft_lines.append(\"\\n\n",
        "        for suggestion in critique_feedback.get(\"suggestions\", [])[:3]:\n",
        "            draft_lines.append(f\"- {suggestion}\")\n",
        "\n",
        "    full_text = \"\\n\".join(draft_lines)\n",
        "    word_count = len(full_text.split())\n",
        "    draft_lines.append(f\"\\n\\n*Word count: {word_count}*\")\n",
        "\n",
        "    return \"\\n\".join(draft_lines)\n",
        "\n",
        "class DraftCritique:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.critique_criteria = {\n",
        "            \"clarity\": self.check_clarity,\n",
        "            \"flow\": self.check_flow,\n",
        "            \"missing_references\": self.check_missing_references,\n",
        "            \"repetition\": self.check_repetition,\n",
        "            \"style\": self.check_academic_style,\n",
        "            \"structure\": self.check_structure\n",
        "        }\n",
        "\n",
        "    def critique_draft(self, draft_text, sections):\n",
        "\n",
        "        print(\"\\nAnalyzing draft quality...\")\n",
        "\n",
        "        critique_results = {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"checks\": {},\n",
        "            \"suggestions\": [],\n",
        "            \"score\": 0,\n",
        "            \"total_checks\": len(self.critique_criteria)\n",
        "        }\n",
        "\n",
        "        passed_checks = 0\n",
        "\n",
        "        for criterion_name, check_function in self.critique_criteria.items():\n",
        "            print(f\"  • Checking {criterion_name}...\", end=\" \")\n",
        "\n",
        "            passed, feedback = check_function(draft_text, sections)\n",
        "            critique_results[\"checks\"][criterion_name] = {\n",
        "                \"passed\": passed,\n",
        "                \"feedback\": feedback,\n",
        "                \"suggestion\": self.generate_suggestion(criterion_name, passed, feedback)\n",
        "            }\n",
        "\n",
        "            if passed:\n",
        "                passed_checks += 1\n",
        "                print(\"✅\")\n",
        "            else:\n",
        "                print(\"❌\")\n",
        "\n",
        "        critique_results[\"score\"] = passed_checks\n",
        "        critique_results[\"passed_checks\"] = passed_checks\n",
        "\n",
        "        critique_results[\"suggestions\"] = self.generate_overall_suggestions(critique_results)\n",
        "\n",
        "        return critique_results\n",
        "\n",
        "    def check_clarity(self, draft_text, sections):\n",
        "\n",
        "        clarity_issues = []\n",
        "\n",
        "        sentences = re.split(r'[.!?]+', draft_text)\n",
        "        long_sentences = [s for s in sentences if len(s.split()) > 40]\n",
        "\n",
        "        if long_sentences:\n",
        "            clarity_issues.append(f\"{len(long_sentences)} sentences are too long (>40 words)\")\n",
        "\n",
        "        passive_patterns = [r'\\bis\\s+\\w+ed\\b', r'\\bare\\s+\\w+ed\\b', r'\\bwas\\s+\\w+ed\\b', r'\\bwere\\s+\\w+ed\\b']\n",
        "        passive_count = sum(len(re.findall(pattern, draft_text.lower())) for pattern in passive_patterns)\n",
        "\n",
        "        if passive_count > 10:\n",
        "            clarity_issues.append(f\"High use of passive voice ({passive_count} instances)\")\n",
        "\n",
        "        passed = len(clarity_issues) == 0\n",
        "        return passed, clarity_issues\n",
        "\n",
        "    def check_flow(self, draft_text, sections):\n",
        "\n",
        "        flow_issues = []\n",
        "\n",
        "        section_order = [\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\"]\n",
        "        missing_sections = []\n",
        "\n",
        "        for section in section_order:\n",
        "            if section not in sections:\n",
        "                missing_sections.append(section)\n",
        "\n",
        "        if missing_sections:\n",
        "            flow_issues.append(f\"Missing sections: {', '.join(missing_sections)}\")\n",
        "\n",
        "        if \"conclusion\" in sections and \"introduction\" in sections:\n",
        "            conclusion_text = sections[\"conclusion\"].lower()\n",
        "            introduction_keywords = [\"paper\", \"study\", \"research\", \"analysis\"]\n",
        "\n",
        "            references_intro = any(keyword in conclusion_text for keyword in introduction_keywords)\n",
        "            if not references_intro:\n",
        "                flow_issues.append(\"Conclusion doesn't clearly reference the introduction\")\n",
        "\n",
        "        passed = len(flow_issues) == 0\n",
        "        return passed, flow_issues\n",
        "\n",
        "    def check_missing_references(self, draft_text, sections):\n",
        "\n",
        "        ref_issues = []\n",
        "\n",
        "        if \"references\" in sections:\n",
        "            ref_text = sections[\"references\"]\n",
        "\n",
        "            has_years = bool(re.search(r'\\(\\d{4}\\)', ref_text))\n",
        "            has_authors = bool(re.search(r'[A-Z][a-z]+, [A-Z]\\.', ref_text))\n",
        "            has_titles = bool(re.search(r'\\. [A-Z]', ref_text))\n",
        "\n",
        "            if not has_years:\n",
        "                ref_issues.append(\"References missing publication years\")\n",
        "            if not has_authors:\n",
        "                ref_issues.append(\"References missing author names\")\n",
        "            if not has_titles:\n",
        "                ref_issues.append(\"References may be missing titles\")\n",
        "\n",
        "            ref_count = len([line for line in ref_text.split('\\n') if line.strip() and not line.startswith('[')])\n",
        "            if ref_count < 3:\n",
        "                ref_issues.append(f\"Only {ref_count} references (aim for 5+)\")\n",
        "        else:\n",
        "            ref_issues.append(\"No references section found\")\n",
        "\n",
        "        passed = len(ref_issues) == 0\n",
        "        return passed, ref_issues\n",
        "\n",
        "    def check_repetition(self, draft_text, sections):\n",
        "\n",
        "        repetition_issues = []\n",
        "\n",
        "        words = re.findall(r'\\b\\w+\\b', draft_text.lower())\n",
        "        word_freq = defaultdict(int)\n",
        "\n",
        "        for word in words:\n",
        "            if len(word) > 4:\n",
        "                word_freq[word] += 1\n",
        "\n",
        "        common_words = {'paper', 'study', 'research', 'analysis', 'method', 'result', 'finding'}\n",
        "        overused = [(word, count) for word, count in word_freq.items()\n",
        "                   if count > 5 and word not in common_words]\n",
        "\n",
        "        if overused:\n",
        "            top_overused = sorted(overused, key=lambda x: x[1], reverse=True)[:3]\n",
        "            repetition_issues.append(f\"Overused words: {', '.join([f'{w}({c})' for w, c in top_overused])}\")\n",
        "\n",
        "        if len(sections) >= 2:\n",
        "            section_texts = list(sections.values())\n",
        "\n",
        "            for i in range(len(section_texts)):\n",
        "                for j in range(i+1, len(section_texts)):\n",
        "                    if section_texts[i][:50] == section_texts[j][:50]:\n",
        "                        repetition_issues.append(\"Sections may have similar openings\")\n",
        "                        break\n",
        "\n",
        "        passed = len(repetition_issues) == 0\n",
        "        return passed, repetition_issues\n",
        "\n",
        "    def check_academic_style(self, draft_text, sections):\n",
        "\n",
        "        style_issues = []\n",
        "\n",
        "        informal_words = ['really', 'very', 'a lot', 'got', 'stuff', 'thing']\n",
        "        informal_count = sum(draft_text.lower().count(word) for word in informal_words)\n",
        "\n",
        "        if informal_count > 3:\n",
        "            style_issues.append(f\"Informal language used ({informal_count} instances)\")\n",
        "\n",
        "        first_person = len(re.findall(r'\\b(I|we|our|us)\\b', draft_text, re.IGNORECASE))\n",
        "        if first_person > 5:\n",
        "            style_issues.append(f\"High use of first-person pronouns ({first_person} instances)\")\n",
        "\n",
        "        paragraphs = [p for p in draft_text.split('\\n\\n') if p.strip()]\n",
        "        short_paragraphs = [p for p in paragraphs if len(p.split()) < 50]\n",
        "\n",
        "        if len(short_paragraphs) > 3:\n",
        "            style_issues.append(f\"{len(short_paragraphs)} very short paragraphs (<50 words)\")\n",
        "\n",
        "        passed = len(style_issues) == 0\n",
        "        return passed, style_issues\n",
        "\n",
        "    def check_structure(self, draft_text, sections):\n",
        "\n",
        "        structure_issues = []\n",
        "\n",
        "        required_sections = {\"abstract\", \"introduction\", \"methods\", \"results\", \"conclusion\", \"references\"}\n",
        "        present_sections = set(sections.keys())\n",
        "        missing = required_sections - present_sections\n",
        "\n",
        "        if missing:\n",
        "            structure_issues.append(f\"Missing required sections: {', '.join(missing)}\")\n",
        "\n",
        "        section_lengths = {}\n",
        "        for section, content in sections.items():\n",
        "            words = len(content.split())\n",
        "            section_lengths[section] = words\n",
        "\n",
        "            if section == \"abstract\" and words > 150:\n",
        "                structure_issues.append(f\"Abstract too long ({words} words, aim for <150)\")\n",
        "\n",
        "            if section == \"introduction\" and words < 100:\n",
        "                structure_issues.append(f\"Introduction too short ({words} words, aim for 100+)\")\n",
        "\n",
        "        if \"methods\" in section_lengths and \"results\" in section_lengths:\n",
        "            if section_lengths[\"methods\"] < section_lengths[\"conclusion\"]:\n",
        "                structure_issues.append(\"Methods section shorter than conclusion (unusual)\")\n",
        "\n",
        "        passed = len(structure_issues) == 0\n",
        "        return passed, structure_issues\n",
        "\n",
        "    def generate_suggestion(self, criterion, passed, feedback):\n",
        "\n",
        "        suggestions = {\n",
        "            \"clarity\": \"Use shorter sentences and active voice where possible.\",\n",
        "            \"flow\": \"Ensure each section logically leads to the next. Use transition phrases.\",\n",
        "            \"missing_references\": \"Add more references and ensure proper APA formatting.\",\n",
        "            \"repetition\": \"Vary your vocabulary. Use synonyms for frequently used terms.\",\n",
        "            \"style\": \"Use formal academic language. Avoid informal expressions.\",\n",
        "            \"structure\": \"Ensure all required sections are present and appropriately balanced.\"\n",
        "        }\n",
        "\n",
        "        if passed:\n",
        "            return f\"{criterion.title()} is good.\"\n",
        "        else:\n",
        "            base_suggestion = suggestions.get(criterion, \"Review and improve this section.\")\n",
        "            if feedback:\n",
        "                return f\"{base_suggestion} Issues: {'; '.join(feedback)}\"\n",
        "            return base_suggestion\n",
        "\n",
        "    def generate_overall_suggestions(self, critique_results):\n",
        "\n",
        "        suggestions = []\n",
        "\n",
        "        failed_checks = [name for name, check in critique_results[\"checks\"].items()\n",
        "                        if not check[\"passed\"]]\n",
        "\n",
        "        if not failed_checks:\n",
        "            suggestions.append(\"Draft is well-structured. Minor polishing only needed.\")\n",
        "            suggestions.append(\"Consider adding more specific examples or data.\")\n",
        "            suggestions.append(\"Review formatting for final submission.\")\n",
        "            return suggestions\n",
        "\n",
        "        if \"clarity\" in failed_checks:\n",
        "            suggestions.append(\"Revise for clarity: Break long sentences, use active voice.\")\n",
        "\n",
        "        if \"flow\" in failed_checks:\n",
        "            suggestions.append(\"Improve flow: Add transition sentences between sections.\")\n",
        "\n",
        "        if \"missing_references\" in failed_checks:\n",
        "            suggestions.append(\"Expand references: Add 2-3 more relevant citations.\")\n",
        "\n",
        "        if \"repetition\" in failed_checks:\n",
        "            suggestions.append(\"Reduce repetition: Use synonyms and vary sentence structure.\")\n",
        "\n",
        "        if \"structure\" in failed_checks:\n",
        "            suggestions.append(\"Restructure: Ensure all sections are present and balanced.\")\n",
        "\n",
        "        suggestions.append(\"Read draft aloud to catch awkward phrasing.\")\n",
        "        suggestions.append(\"Have a peer review the draft for fresh perspective.\")\n",
        "        suggestions.append(\"Allow time between revisions for objective review.\")\n",
        "\n",
        "        return suggestions[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a681d2f4",
      "metadata": {
        "id": "a681d2f4"
      },
      "source": [
        "## Function: `run_revision_cycle`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f57a66f",
      "metadata": {
        "id": "5f57a66f"
      },
      "outputs": [],
      "source": [
        "def run_revision_cycle(draft_text, sections, critique_results, iteration=1):\n",
        "\n",
        "    print(f\"\\n Running revision cycle {iteration}...\")\n",
        "\n",
        "    revised_sections = sections.copy()\n",
        "\n",
        "    for criterion, check_info in critique_results[\"checks\"].items():\n",
        "        if not check_info[\"passed\"]:\n",
        "\n",
        "            revised_sections = apply_revisions(revised_sections, criterion, check_info[\"feedback\"])\n",
        "\n",
        "    revised_draft = create_full_draft_markdown(revised_sections, critique_results)\n",
        "\n",
        "    return revised_draft, revised_sections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e069e48",
      "metadata": {
        "id": "5e069e48"
      },
      "source": [
        "## Function: `apply_revisions`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbf4db9d",
      "metadata": {
        "id": "dbf4db9d"
      },
      "outputs": [],
      "source": [
        "def apply_revisions(sections, criterion, feedback):\n",
        "\n",
        "    revised = sections.copy()\n",
        "\n",
        "    if criterion == \"clarity\":\n",
        "\n",
        "        for section_name, content in revised.items():\n",
        "\n",
        "            sentences = re.split(r'[.!?]+', content)\n",
        "            improved_sentences = []\n",
        "\n",
        "            for sentence in sentences:\n",
        "                if sentence.strip():\n",
        "                    words = sentence.strip().split()\n",
        "                    if len(words) > 40:\n",
        "\n",
        "                        mid_point = len(words) // 2\n",
        "                        improved_sentences.append(' '.join(words[:mid_point]) + '.')\n",
        "                        improved_sentences.append(' '.join(words[mid_point:]))\n",
        "                    else:\n",
        "                        improved_sentences.append(sentence.strip())\n",
        "\n",
        "            revised[section_name] = '. '.join(improved_sentences) + ('.' if not content.endswith('.') else '')\n",
        "\n",
        "    elif criterion == \"repetition\" and feedback:\n",
        "\n",
        "        for section_name in [\"abstract\", \"conclusion\"]:\n",
        "            if section_name in revised:\n",
        "                content = revised[section_name]\n",
        "\n",
        "                replacements = {\n",
        "                    \"paper\": \"study\",\n",
        "                    \"research\": \"investigation\",\n",
        "                    \"analysis\": \"examination\",\n",
        "                    \"method\": \"approach\",\n",
        "                    \"result\": \"finding\"\n",
        "                }\n",
        "\n",
        "                for old, new in replacements.items():\n",
        "                    if content.count(old) > 2:\n",
        "\n",
        "                        parts = content.split(old)\n",
        "                        new_parts = []\n",
        "                        for i, part in enumerate(parts):\n",
        "                            new_parts.append(part)\n",
        "                            if i < len(parts) - 1:\n",
        "                                new_parts.append(new if i % 2 == 1 else old)\n",
        "                        revised[section_name] = ''.join(new_parts)\n",
        "\n",
        "    elif criterion == \"structure\" and \"Introduction too short\" in str(feedback):\n",
        "\n",
        "        if \"introduction\" in revised:\n",
        "            current = revised[\"introduction\"]\n",
        "            if len(current.split()) < 100:\n",
        "                expanded = current + \" This analysis provides a comprehensive examination of the methodological approaches and findings. The review situates the work within the broader research context and evaluates its contributions to the field.\"\n",
        "                revised[\"introduction\"] = expanded\n",
        "\n",
        "    return revised"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fef4dbf",
      "metadata": {
        "id": "1fef4dbf"
      },
      "source": [
        "## Function: `save_critique_results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6da1ca70",
      "metadata": {
        "id": "6da1ca70"
      },
      "outputs": [],
      "source": [
        "def save_critique_results(critique_results, iteration=1):\n",
        "\n",
        "    outputs_path = Path(\"outputs\")\n",
        "    outputs_path.mkdir(exist_ok=True)\n",
        "\n",
        "    filename = outputs_path / f\"critique_feedback_iteration_{iteration}.json\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(critique_results, f, indent=2)\n",
        "\n",
        "    print(f\"  Critique feedback saved: {filename.name}\")\n",
        "    return str(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c3e807a",
      "metadata": {
        "id": "8c3e807a"
      },
      "source": [
        "## Function: `save_revised_draft`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2807fe94",
      "metadata": {
        "id": "2807fe94"
      },
      "outputs": [],
      "source": [
        "def save_revised_draft(revised_draft, iteration=1):\n",
        "\n",
        "    outputs_path = Path(\"outputs\")\n",
        "\n",
        "    filename = outputs_path / f\"revised_draft_iteration_{iteration}.md\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(revised_draft)\n",
        "\n",
        "    print(f\"  Revised draft saved: {filename.name}\")\n",
        "\n",
        "    txt_filename = outputs_path / f\"revised_draft_iteration_{iteration}.txt\"\n",
        "    with open(txt_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(revised_draft)\n",
        "\n",
        "    return str(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b8867c",
      "metadata": {
        "id": "a5b8867c"
      },
      "source": [
        "## Function: `save_revision_summary`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb5296d7",
      "metadata": {
        "id": "cb5296d7"
      },
      "outputs": [],
      "source": [
        "def save_revision_summary(original_critique, revised_critique, iterations):\n",
        "\n",
        "    outputs_path = Path(\"outputs\")\n",
        "\n",
        "    summary = {\n",
        "        \"revision_date\": datetime.now().isoformat(),\n",
        "        \"total_iterations\": iterations,\n",
        "        \"improvement_summary\": {\n",
        "            \"original_score\": original_critique[\"score\"],\n",
        "            \"final_score\": revised_critique[\"score\"] if revised_critique else original_critique[\"score\"],\n",
        "            \"improvement\": (revised_critique[\"score\"] - original_critique[\"score\"]) if revised_critique else 0\n",
        "        },\n",
        "        \"issues_resolved\": [],\n",
        "        \"remaining_issues\": []\n",
        "    }\n",
        "\n",
        "    if revised_critique:\n",
        "        for criterion in original_critique[\"checks\"]:\n",
        "            original_passed = original_critique[\"checks\"][criterion][\"passed\"]\n",
        "            revised_passed = revised_critique[\"checks\"][criterion][\"passed\"]\n",
        "\n",
        "            if not original_passed and revised_passed:\n",
        "                summary[\"issues_resolved\"].append(criterion)\n",
        "            elif not original_passed and not revised_passed:\n",
        "                summary[\"remaining_issues\"].append(criterion)\n",
        "\n",
        "    filename = outputs_path / \"revision_summary.json\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(f\"   Revision summary saved: {filename.name}\")\n",
        "    return str(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c448e1cc",
      "metadata": {
        "id": "c448e1cc"
      },
      "source": [
        "## Function: `run_draft_aggregation_and_critique`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "f35f8ffc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f35f8ffc",
        "outputId": "375c4d3b-ff71-45d2-d4b4-2a3ea9852cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:24: SyntaxWarning: invalid escape sequence '\\ '\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\ '\n",
            "/tmp/ipython-input-101205474.py:24: SyntaxWarning: invalid escape sequence '\\ '\n",
            "  print(\"\\ STEP 3: Running draft critique...\")\n"
          ]
        }
      ],
      "source": [
        "def run_draft_aggregation_and_critique(max_iterations=2):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DRAFT AGGREGATION & CRITIQUE MODULE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nSTEP 1: Loading generated draft...\")\n",
        "    draft_text = load_latest_draft()\n",
        "    if not draft_text:\n",
        "        return None\n",
        "\n",
        "    sections = load_individual_sections()\n",
        "    print(f\"  Loaded {len(sections)} sections\")\n",
        "\n",
        "    print(\"\\nSTEP 2: Aggregating full draft...\")\n",
        "    full_draft = create_full_draft_markdown(sections)\n",
        "\n",
        "    outputs_path = Path(\"outputs\")\n",
        "    initial_draft_file = outputs_path / \"full_draft_initial.md\"\n",
        "    with open(initial_draft_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(full_draft)\n",
        "    print(f\"  Full draft saved: {initial_draft_file.name}\")\n",
        "\n",
        "    print(\"\\ STEP 3: Running draft critique...\")\n",
        "    critique_system = DraftCritique()\n",
        "    critique_results = critique_system.critique_draft(full_draft, sections)\n",
        "\n",
        "    print(f\"\\nCritique Score: {critique_results['score']}/{critique_results['total_checks']}\")\n",
        "\n",
        "    critique_file = save_critique_results(critique_results, iteration=1)\n",
        "\n",
        "    print(\"\\nSTEP 4: Running revision cycles...\")\n",
        "\n",
        "    current_sections = sections\n",
        "    current_critique = critique_results\n",
        "    all_revisions = []\n",
        "\n",
        "    for iteration in range(1, max_iterations + 1):\n",
        "        print(f\"\\n  teration {iteration}/{max_iterations}\")\n",
        "\n",
        "        revised_draft, revised_sections = run_revision_cycle(\n",
        "            full_draft, current_sections, current_critique, iteration\n",
        "        )\n",
        "\n",
        "        draft_file = save_revised_draft(revised_draft, iteration)\n",
        "        all_revisions.append(draft_file)\n",
        "\n",
        "        if iteration < max_iterations:\n",
        "            current_critique = critique_system.critique_draft(revised_draft, revised_sections)\n",
        "            save_critique_results(current_critique, iteration + 1)\n",
        "            current_sections = revised_sections\n",
        "\n",
        "            print(f\"    Score after revision: {current_critique['score']}/{current_critique['total_checks']}\")\n",
        "\n",
        "    print(\"\\n🔹 STEP 5: Creating revision summary...\")\n",
        "    summary_file = save_revision_summary(critique_results, current_critique, max_iterations)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\n OUTPUTS GENERATED:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"• full_draft_initial.md - Complete aggregated draft\")\n",
        "    print(f\"• critique_feedback_iteration_1.json - Initial critique\")\n",
        "    for i in range(1, max_iterations + 1):\n",
        "        print(f\"• revised_draft_iteration_{i}.md - Revised draft v{i}\")\n",
        "        if i < max_iterations:\n",
        "            print(f\"• critique_feedback_iteration_{i+1}.json - Critique v{i+1}\")\n",
        "    print(f\"• revision_summary.json - Improvement summary\")\n",
        "\n",
        "    print(\"\\n CHECKLIST RESULTS:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Full draft logically structured? {'YES' if critique_results['checks']['structure']['passed'] else 'WITH ISSUES'}\")\n",
        "    print(f\"Critique catches genuine issues? {'YES' if critique_results['score'] < critique_results['total_checks'] else 'ALL PASSED'}\")\n",
        "    print(f\" Revision cycle works? YES ({max_iterations} iterations completed)\")\n",
        "\n",
        "    print(f\"\\n IMPROVEMENT TRACKING:\")\n",
        "    print(f\"   Initial score: {critique_results['score']}/{critique_results['total_checks']}\")\n",
        "    if current_critique and current_critique != critique_results:\n",
        "        print(f\"   Final score: {current_critique['score']}/{current_critique['total_checks']}\")\n",
        "        print(f\"   Improvement: +{current_critique['score'] - critique_results['score']} points\")\n",
        "\n",
        "    return {\n",
        "        \"initial_draft\": full_draft,\n",
        "        \"initial_critique\": critique_results,\n",
        "        \"final_draft\": revised_draft if 'revised_draft' in locals() else full_draft,\n",
        "        \"final_critique\": current_critique,\n",
        "        \"revisions\": all_revisions,\n",
        "        \"summary_file\": summary_file\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c025652",
      "metadata": {
        "id": "6c025652"
      },
      "source": [
        "## Function: `preview_critique_results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70bde172",
      "metadata": {
        "id": "70bde172"
      },
      "outputs": [],
      "source": [
        "def preview_critique_results():\n",
        "\n",
        "    outputs_path = Path(\"outputs\")\n",
        "    critique_files = list(outputs_path.glob(\"critique_feedback_iteration_*.json\"))\n",
        "    if not critique_files:\n",
        "        print(\" No critique files found\")\n",
        "        return\n",
        "\n",
        "    latest_critique = max(critique_files, key=lambda x: x.stat().st_mtime)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" CRITIQUE RESULTS PREVIEW\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nFile: {latest_critique.name}\\n\")\n",
        "\n",
        "    with open(latest_critique, 'r', encoding='utf-8') as f:\n",
        "        critique_data = json.load(f)\n",
        "\n",
        "    print(f\"Score: {critique_data['score']}/{critique_data['total_checks']}\")\n",
        "    print(\"\\nChecks Summary:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for check_name, check_info in critique_data[\"checks\"].items():\n",
        "        status = \"✅\" if check_info[\"passed\"] else \"❌\"\n",
        "        print(f\"{status} {check_name}: {check_info['suggestion']}\")\n",
        "\n",
        "    print(\"\\nTop Suggestions:\")\n",
        "    print(\"-\" * 40)\n",
        "    for i, suggestion in enumerate(critique_data.get(\"suggestions\", [])[:3], 1):\n",
        "        print(f\"{i}. {suggestion}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    results = run_draft_aggregation_and_critique(max_iterations=2)\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" DRAFT AGGREGATION & CRITIQUE SUCCESSFUL!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        initial_score = results[\"initial_critique\"][\"score\"]\n",
        "        final_score = results[\"final_critique\"][\"score\"] if results[\"final_critique\"] else initial_score\n",
        "\n",
        "        print(f\"\\n REVISION IMPROVEMENT: {initial_score} → {final_score}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        preview = input(\"Would you like to preview critique results? (y/n): \")\n",
        "        if preview.lower() == 'y':\n",
        "            preview_critique_results()\n",
        "\n",
        "        print(\"\\n ALL FILES ARE IN: outputs/ folder\")\n",
        "        print(\"   Review: full_draft_initial.md (complete draft)\")\n",
        "        print(\"   Review: critique_feedback_iteration_1.json (detailed feedback)\")\n",
        "        print(\"   Review: revised_draft_iteration_2.md (final revised version)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}